
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) miniconda/4.9.2

 

08/13/2021 17:09:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/13/2021 17:09:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=None,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-opensubtitles_all/runs/Aug13_17-09-35_gpu25.int.triton.aalto.fi,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
output_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-opensubtitles_all,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=bert-base-finnish-cased-v1-finetuned-opensubtitles_all,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-opensubtitles_all,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/13/2021 17:09:36 - WARNING - datasets.builder - Using custom data configuration default-ab7c8abf87111696
08/13/2021 17:09:36 - INFO - datasets.builder - Generating dataset text (/home/moisioa3/.cache/huggingface/datasets/text/default-ab7c8abf87111696/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)
Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/moisioa3/.cache/huggingface/datasets/text/default-ab7c8abf87111696/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 7115.02it/s]
08/13/2021 17:09:36 - INFO - datasets.utils.download_manager - Downloading took 0.0 min
08/13/2021 17:09:40 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 131.62it/s]
08/13/2021 17:09:40 - INFO - datasets.utils.info_utils - Unable to verify checksums.
08/13/2021 17:09:40 - INFO - datasets.builder - Generating split train
0 tables [00:00, ? tables/s]1 tables [00:00,  1.68 tables/s]2 tables [00:00,  2.77 tables/s]3 tables [00:00,  3.60 tables/s]4 tables [00:01,  4.14 tables/s]5 tables [00:01,  4.54 tables/s]6 tables [00:01,  4.94 tables/s]7 tables [00:01,  5.33 tables/s]8 tables [00:01,  5.55 tables/s]9 tables [00:01,  5.65 tables/s]10 tables [00:02,  5.81 tables/s]11 tables [00:02,  5.91 tables/s]12 tables [00:02,  5.84 tables/s]13 tables [00:02,  5.85 tables/s]14 tables [00:02,  5.90 tables/s]15 tables [00:03,  5.88 tables/s]16 tables [00:03,  5.92 tables/s]17 tables [00:03,  6.07 tables/s]18 tables [00:03,  5.99 tables/s]19 tables [00:03,  6.12 tables/s]20 tables [00:03,  5.94 tables/s]21 tables [00:04,  5.93 tables/s]22 tables [00:04,  5.99 tables/s]23 tables [00:04,  5.91 tables/s]24 tables [00:04,  6.03 tables/s]25 tables [00:04,  6.11 tables/s]26 tables [00:04,  6.06 tables/s]27 tables [00:04,  6.04 tables/s]28 tables [00:05,  6.02 tables/s]29 tables [00:05,  5.85 tables/s]30 tables [00:05,  5.96 tables/s]31 tables [00:05,  6.06 tables/s]32 tables [00:05,  5.96 tables/s]33 tables [00:05,  6.05 tables/s]34 tables [00:06,  6.02 tables/s]35 tables [00:06,  5.89 tables/s]36 tables [00:06,  5.99 tables/s]37 tables [00:06,  5.86 tables/s]38 tables [00:06,  5.95 tables/s]39 tables [00:06,  6.08 tables/s]40 tables [00:07,  6.22 tables/s]41 tables [00:07,  6.22 tables/s]42 tables [00:07,  6.26 tables/s]43 tables [00:07,  6.16 tables/s]44 tables [00:07,  6.16 tables/s]45 tables [00:07,  6.14 tables/s]46 tables [00:08,  6.26 tables/s]47 tables [00:08,  6.31 tables/s]48 tables [00:08,  6.16 tables/s]49 tables [00:08,  6.22 tables/s]50 tables [00:08,  6.26 tables/s]51 tables [00:08,  6.32 tables/s]52 tables [00:09,  6.10 tables/s]53 tables [00:09,  6.10 tables/s]54 tables [00:09,  6.17 tables/s]55 tables [00:09,  6.24 tables/s]56 tables [00:09,  6.25 tables/s]57 tables [00:09,  6.30 tables/s]58 tables [00:10,  6.26 tables/s]59 tables [00:10,  6.21 tables/s]60 tables [00:10,  6.31 tables/s]61 tables [00:10,  6.41 tables/s]62 tables [00:10,  6.52 tables/s]63 tables [00:10,  6.58 tables/s]64 tables [00:10,  6.30 tables/s]65 tables [00:11,  6.37 tables/s]66 tables [00:11,  6.37 tables/s]67 tables [00:11,  6.06 tables/s]68 tables [00:11,  6.07 tables/s]69 tables [00:11,  6.21 tables/s]70 tables [00:11,  6.17 tables/s]71 tables [00:12,  6.18 tables/s]72 tables [00:12,  6.18 tables/s]73 tables [00:12,  6.11 tables/s]74 tables [00:12,  6.24 tables/s]75 tables [00:12,  6.32 tables/s]76 tables [00:12,  6.29 tables/s]77 tables [00:13,  6.14 tables/s]78 tables [00:13,  6.21 tables/s]79 tables [00:13,  6.20 tables/s]80 tables [00:13,  6.24 tables/s]81 tables [00:13,  6.34 tables/s]82 tables [00:13,  6.30 tables/s]83 tables [00:14,  6.37 tables/s]84 tables [00:14,  6.37 tables/s]85 tables [00:14,  6.29 tables/s]86 tables [00:14,  6.25 tables/s]87 tables [00:14,  6.22 tables/s]88 tables [00:14,  6.04 tables/s]89 tables [00:15,  6.20 tables/s]90 tables [00:15,  6.33 tables/s]91 tables [00:15,  6.44 tables/s]92 tables [00:15,  6.37 tables/s]93 tables [00:15,  6.23 tables/s]94 tables [00:15,  6.10 tables/s]95 tables [00:15,  6.12 tables/s]96 tables [00:16,  6.05 tables/s]97 tables [00:16,  5.92 tables/s]98 tables [00:16,  6.07 tables/s]99 tables [00:16,  6.25 tables/s]100 tables [00:16,  6.25 tables/s]101 tables [00:16,  6.26 tables/s]102 tables [00:17,  6.11 tables/s]103 tables [00:17,  6.01 tables/s]104 tables [00:17,  5.99 tables/s]105 tables [00:17,  6.15 tables/s]106 tables [00:17,  6.06 tables/s]107 tables [00:17,  6.13 tables/s]108 tables [00:18,  6.12 tables/s]109 tables [00:18,  6.10 tables/s]110 tables [00:18,  6.11 tables/s]111 tables [00:18,  6.12 tables/s]112 tables [00:18,  6.02 tables/s]113 tables [00:18,  6.15 tables/s]114 tables [00:19,  6.12 tables/s]                                  08/13/2021 17:09:59 - INFO - datasets.builder - Generating split validation
0 tables [00:00, ? tables/s]                            08/13/2021 17:09:59 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset text downloaded and prepared to /home/moisioa3/.cache/huggingface/datasets/text/default-ab7c8abf87111696/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  8.90it/s]100%|██████████| 2/2 [00:00<00:00, 17.39it/s]
[INFO|configuration_utils.py:545] 2021-08-13 17:10:00,524 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-13 17:10:00,525 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|configuration_utils.py:545] 2021-08-13 17:10:01,440 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-13 17:10:01,441 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|tokenization_utils_base.py:1730] 2021-08-13 17:10:04,158 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt from cache at /home/moisioa3/.cache/huggingface/transformers/69c0c339871654aa7305fe47345f0b713e6973a476eb1cf5f200d557b6bad765.ee591817c6a7d736b63494878a337beccf9497af463ab8eb01d19bf5f7169026
[INFO|tokenization_utils_base.py:1730] 2021-08-13 17:10:04,159 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json from cache at /home/moisioa3/.cache/huggingface/transformers/3583dbf83678cb60c5faaf0a07aa0d452fc4ec09aac87b8680027bf79b1a6270.e49785bf2de92e06a4d89026870d6979723c8e64cfc9311596ca5b9a3b56289e
[INFO|tokenization_utils_base.py:1730] 2021-08-13 17:10:04,159 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-08-13 17:10:04,159 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-08-13 17:10:04,160 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer_config.json from cache at /home/moisioa3/.cache/huggingface/transformers/978f5aef1d382479fb5ee87b700be69b8cc9ac5f26184a5835fc6e0c03571860.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|configuration_utils.py:545] 2021-08-13 17:10:04,620 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-13 17:10:04,621 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|modeling_utils.py:1271] 2021-08-13 17:10:05,175 >> loading weights file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/pytorch_model.bin from cache at /home/moisioa3/.cache/huggingface/transformers/276bf5f0d95b31fc0ed72ef6e2e1b771f2265351a4d322667fd8c73d8473d3fc.3d524bdc756dfbb2ba6c3c3a18e4e2afcc84034db29556b337605e9f8c39c2c2
[WARNING|modeling_utils.py:1501] 2021-08-13 17:10:08,950 >> Some weights of the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1518] 2021-08-13 17:10:08,951 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Running tokenizer on every text in dataset:   0%|          | 0/42009 [00:00<?, ?ba/s]08/13/2021 17:10:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-ab7c8abf87111696/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-65b7086b3d318038.arrow
Running tokenizer on every text in dataset:   0%|          | 1/42009 [00:00<1:56:40,  6.00ba/s]Running tokenizer on every text in dataset:   0%|          | 4/42009 [00:00<57:24, 12.19ba/s]  Running tokenizer on every text in dataset:   0%|          | 7/42009 [00:00<40:01, 17.49ba/s]Running tokenizer on every text in dataset:   0%|          | 10/42009 [00:00<33:27, 20.92ba/s]Running tokenizer on every text in dataset:   0%|          | 13/42009 [00:00<30:10, 23.20ba/s]Running tokenizer on every text in dataset:   0%|          | 16/42009 [00:00<32:10, 21.76ba/s]Running tokenizer on every text in dataset:   0%|          | 19/42009 [00:00<29:21, 23.84ba/s]Running tokenizer on every text in dataset:   0%|          | 22/42009 [00:01<27:27, 25.48ba/s]Running tokenizer on every text in dataset:   0%|          | 25/42009 [00:01<29:57, 23.36ba/s]Running tokenizer on every text in dataset:   0%|          | 28/42009 [00:01<28:16, 24.75ba/s]Running tokenizer on every text in dataset:   0%|          | 31/42009 [00:01<26:58, 25.94ba/s]Running tokenizer on every text in dataset:   0%|          | 34/42009 [00:01<29:26, 23.76ba/s]Running tokenizer on every text in dataset:   0%|          | 37/42009 [00:01<28:06, 24.89ba/s]Running tokenizer on every text in dataset:   0%|          | 40/42009 [00:01<28:00, 24.97ba/s]Running tokenizer on every text in dataset:   0%|          | 43/42009 [00:01<30:45, 22.74ba/s]Running tokenizer on every text in dataset:   0%|          | 46/42009 [00:02<30:01, 23.29ba/s]Running tokenizer on every text in dataset:   0%|          | 49/42009 [00:02<28:49, 24.26ba/s]Running tokenizer on every text in dataset:   0%|          | 52/42009 [00:02<31:33, 22.16ba/s]Running tokenizer on every text in dataset:   0%|          | 55/42009 [00:02<29:43, 23.52ba/s]Running tokenizer on every text in dataset:   0%|          | 58/42009 [00:02<28:24, 24.62ba/s]Running tokenizer on every text in dataset:   0%|          | 61/42009 [00:02<31:09, 22.44ba/s]Running tokenizer on every text in dataset:   0%|          | 64/42009 [00:02<29:35, 23.63ba/s]Running tokenizer on every text in dataset:   0%|          | 67/42009 [00:02<27:54, 25.05ba/s]Running tokenizer on every text in dataset:   0%|          | 70/42009 [00:03<27:07, 25.78ba/s]Running tokenizer on every text in dataset:   0%|          | 73/42009 [00:03<31:19, 22.31ba/s]Running tokenizer on every text in dataset:   0%|          | 76/42009 [00:03<29:39, 23.57ba/s]Running tokenizer on every text in dataset:   0%|          | 79/42009 [00:03<28:17, 24.70ba/s]Running tokenizer on every text in dataset:   0%|          | 82/42009 [00:03<30:08, 23.19ba/s]Running tokenizer on every text in dataset:   0%|          | 85/42009 [00:03<28:25, 24.59ba/s]Running tokenizer on every text in dataset:   0%|          | 88/42009 [00:03<27:43, 25.20ba/s]Running tokenizer on every text in dataset:   0%|          | 91/42009 [00:03<30:34, 22.85ba/s]Running tokenizer on every text in dataset:   0%|          | 94/42009 [00:04<29:06, 23.99ba/s]Running tokenizer on every text in dataset:   0%|          | 97/42009 [00:04<28:07, 24.83ba/s]Running tokenizer on every text in dataset:   0%|          | 100/42009 [00:04<30:45, 22.71ba/s]Running tokenizer on every text in dataset:   0%|          | 103/42009 [00:04<29:24, 23.74ba/s]Running tokenizer on every text in dataset:   0%|          | 106/42009 [00:04<28:04, 24.87ba/s]Running tokenizer on every text in dataset:   0%|          | 109/42009 [00:04<30:37, 22.80ba/s]Running tokenizer on every text in dataset:   0%|          | 112/42009 [00:04<29:35, 23.60ba/s]Running tokenizer on every text in dataset:   0%|          | 115/42009 [00:04<28:24, 24.57ba/s]Running tokenizer on every text in dataset:   0%|          | 118/42009 [00:05<30:59, 22.53ba/s]Running tokenizer on every text in dataset:   0%|          | 121/42009 [00:05<29:13, 23.89ba/s]Running tokenizer on every text in dataset:   0%|          | 124/42009 [00:05<28:15, 24.70ba/s]Running tokenizer on every text in dataset:   0%|          | 127/42009 [00:05<31:50, 21.92ba/s]Running tokenizer on every text in dataset:   0%|          | 130/42009 [00:05<29:55, 23.33ba/s]Running tokenizer on every text in dataset:   0%|          | 133/42009 [00:05<28:32, 24.45ba/s]Running tokenizer on every text in dataset:   0%|          | 136/42009 [00:05<27:48, 25.10ba/s]Running tokenizer on every text in dataset:   0%|          | 139/42009 [00:05<30:31, 22.86ba/s]Running tokenizer on every text in dataset:   0%|          | 142/42009 [00:06<29:11, 23.90ba/s]Running tokenizer on every text in dataset:   0%|          | 145/42009 [00:06<28:28, 24.50ba/s]Running tokenizer on every text in dataset:   0%|          | 148/42009 [00:06<32:55, 21.19ba/s]Running tokenizer on every text in dataset:   0%|          | 151/42009 [00:06<32:00, 21.79ba/s]Running tokenizer on every text in dataset:   0%|          | 154/42009 [00:06<31:23, 22.23ba/s]Running tokenizer on every text in dataset:   0%|          | 157/42009 [00:06<33:42, 20.69ba/s]Running tokenizer on every text in dataset:   0%|          | 160/42009 [00:06<30:57, 22.53ba/s]Running tokenizer on every text in dataset:   0%|          | 163/42009 [00:07<29:26, 23.69ba/s]Running tokenizer on every text in dataset:   0%|          | 166/42009 [00:07<31:54, 21.85ba/s]Running tokenizer on every text in dataset:   0%|          | 169/42009 [00:07<30:15, 23.05ba/s]Running tokenizer on every text in dataset:   0%|          | 172/42009 [00:07<29:02, 24.01ba/s]Running tokenizer on every text in dataset:   0%|          | 175/42009 [00:07<31:10, 22.36ba/s]Running tokenizer on every text in dataset:   0%|          | 178/42009 [00:07<29:44, 23.44ba/s]Running tokenizer on every text in dataset:   0%|          | 181/42009 [00:07<27:53, 24.99ba/s]Running tokenizer on every text in dataset:   0%|          | 184/42009 [00:07<30:32, 22.82ba/s]Running tokenizer on every text in dataset:   0%|          | 187/42009 [00:08<28:58, 24.06ba/s]Running tokenizer on every text in dataset:   0%|          | 190/42009 [00:08<27:50, 25.03ba/s]Running tokenizer on every text in dataset:   0%|          | 193/42009 [00:08<27:09, 25.66ba/s]Running tokenizer on every text in dataset:   0%|          | 196/42009 [00:08<29:57, 23.26ba/s]Running tokenizer on every text in dataset:   0%|          | 199/42009 [00:08<28:43, 24.26ba/s]Running tokenizer on every text in dataset:   0%|          | 202/42009 [00:08<27:52, 25.00ba/s]Running tokenizer on every text in dataset:   0%|          | 205/42009 [00:08<30:48, 22.61ba/s]Running tokenizer on every text in dataset:   0%|          | 208/42009 [00:08<29:20, 23.75ba/s]Running tokenizer on every text in dataset:   1%|          | 211/42009 [00:09<28:05, 24.80ba/s]Running tokenizer on every text in dataset:   1%|          | 214/42009 [00:09<30:26, 22.88ba/s]Running tokenizer on every text in dataset:   1%|          | 217/42009 [00:09<28:54, 24.09ba/s]Running tokenizer on every text in dataset:   1%|          | 220/42009 [00:09<27:47, 25.07ba/s]Running tokenizer on every text in dataset:   1%|          | 223/42009 [00:09<30:13, 23.05ba/s]Running tokenizer on every text in dataset:   1%|          | 226/42009 [00:09<28:52, 24.11ba/s]Running tokenizer on every text in dataset:   1%|          | 229/42009 [00:09<28:16, 24.63ba/s]Running tokenizer on every text in dataset:   1%|          | 232/42009 [00:09<31:06, 22.39ba/s]Running tokenizer on every text in dataset:   1%|          | 235/42009 [00:10<29:32, 23.57ba/s]Running tokenizer on every text in dataset:   1%|          | 238/42009 [00:10<28:12, 24.68ba/s]Running tokenizer on every text in dataset:   1%|          | 241/42009 [00:10<30:43, 22.66ba/s]Running tokenizer on every text in dataset:   1%|          | 244/42009 [00:10<29:15, 23.79ba/s]Running tokenizer on every text in dataset:   1%|          | 247/42009 [00:10<27:48, 25.02ba/s]Running tokenizer on every text in dataset:   1%|          | 250/42009 [00:10<27:25, 25.38ba/s]Running tokenizer on every text in dataset:   1%|          | 253/42009 [00:10<31:52, 21.84ba/s]Running tokenizer on every text in dataset:   1%|          | 256/42009 [00:10<30:23, 22.89ba/s]Running tokenizer on every text in dataset:   1%|          | 259/42009 [00:11<29:01, 23.98ba/s]Running tokenizer on every text in dataset:   1%|          | 262/42009 [00:11<31:19, 22.21ba/s]Running tokenizer on every text in dataset:   1%|          | 265/42009 [00:11<29:48, 23.35ba/s]Running tokenizer on every text in dataset:   1%|          | 268/42009 [00:11<28:58, 24.01ba/s]Running tokenizer on every text in dataset:   1%|          | 271/42009 [00:11<31:53, 21.81ba/s]Running tokenizer on every text in dataset:   1%|          | 274/42009 [00:11<29:46, 23.36ba/s]Running tokenizer on every text in dataset:   1%|          | 277/42009 [00:11<28:57, 24.02ba/s]Running tokenizer on every text in dataset:   1%|          | 280/42009 [00:12<32:10, 21.61ba/s]Running tokenizer on every text in dataset:   1%|          | 283/42009 [00:12<29:41, 23.42ba/s]Running tokenizer on every text in dataset:   1%|          | 286/42009 [00:12<28:22, 24.50ba/s]Running tokenizer on every text in dataset:   1%|          | 289/42009 [00:12<30:59, 22.43ba/s]Running tokenizer on every text in dataset:   1%|          | 292/42009 [00:12<30:57, 22.45ba/s]Running tokenizer on every text in dataset:   1%|          | 295/42009 [00:12<29:08, 23.85ba/s]Running tokenizer on every text in dataset:   1%|          | 298/42009 [00:12<31:01, 22.41ba/s]Running tokenizer on every text in dataset:   1%|          | 301/42009 [00:12<29:07, 23.86ba/s]Running tokenizer on every text in dataset:   1%|          | 304/42009 [00:12<27:40, 25.11ba/s]Running tokenizer on every text in dataset:   1%|          | 307/42009 [00:13<26:48, 25.92ba/s]Running tokenizer on every text in dataset:   1%|          | 310/42009 [00:13<29:32, 23.52ba/s]Running tokenizer on every text in dataset:   1%|          | 313/42009 [00:13<28:24, 24.46ba/s]Running tokenizer on every text in dataset:   1%|          | 316/42009 [00:13<27:17, 25.46ba/s]Running tokenizer on every text in dataset:   1%|          | 319/42009 [00:13<30:03, 23.11ba/s]Running tokenizer on every text in dataset:   1%|          | 322/42009 [00:13<28:21, 24.50ba/s]Running tokenizer on every text in dataset:   1%|          | 325/42009 [00:13<27:07, 25.62ba/s]Running tokenizer on every text in dataset:   1%|          | 328/42009 [00:14<30:01, 23.14ba/s]Running tokenizer on every text in dataset:   1%|          | 331/42009 [00:14<28:34, 24.32ba/s]Running tokenizer on every text in dataset:   1%|          | 334/42009 [00:14<27:12, 25.53ba/s]Running tokenizer on every text in dataset:   1%|          | 337/42009 [00:14<30:18, 22.92ba/s]Running tokenizer on every text in dataset:   1%|          | 340/42009 [00:14<28:29, 24.37ba/s]Running tokenizer on every text in dataset:   1%|          | 343/42009 [00:14<27:10, 25.55ba/s]Running tokenizer on every text in dataset:   1%|          | 346/42009 [00:14<30:09, 23.02ba/s]Running tokenizer on every text in dataset:   1%|          | 349/42009 [00:14<28:49, 24.09ba/s]Running tokenizer on every text in dataset:   1%|          | 352/42009 [00:14<27:38, 25.12ba/s]Running tokenizer on every text in dataset:   1%|          | 355/42009 [00:15<30:24, 22.82ba/s]Running tokenizer on every text in dataset:   1%|          | 358/42009 [00:15<29:14, 23.74ba/s]Running tokenizer on every text in dataset:   1%|          | 361/42009 [00:15<28:07, 24.68ba/s]Running tokenizer on every text in dataset:   1%|          | 364/42009 [00:15<27:09, 25.56ba/s]Running tokenizer on every text in dataset:   1%|          | 367/42009 [00:15<29:41, 23.38ba/s]Running tokenizer on every text in dataset:   1%|          | 371/42009 [00:15<27:13, 25.49ba/s]Running tokenizer on every text in dataset:   1%|          | 374/42009 [00:15<29:22, 23.63ba/s]Running tokenizer on every text in dataset:   1%|          | 377/42009 [00:16<28:28, 24.37ba/s]Running tokenizer on every text in dataset:   1%|          | 380/42009 [00:16<27:33, 25.18ba/s]Running tokenizer on every text in dataset:   1%|          | 383/42009 [00:16<27:22, 25.34ba/s]Running tokenizer on every text in dataset:   1%|          | 386/42009 [00:16<30:31, 22.73ba/s]Running tokenizer on every text in dataset:   1%|          | 389/42009 [00:16<28:36, 24.25ba/s]Running tokenizer on every text in dataset:   1%|          | 392/42009 [00:16<27:42, 25.04ba/s]Running tokenizer on every text in dataset:   1%|          | 395/42009 [00:16<30:00, 23.11ba/s]Running tokenizer on every text in dataset:   1%|          | 398/42009 [00:16<28:22, 24.44ba/s]Running tokenizer on every text in dataset:   1%|          | 401/42009 [00:16<27:05, 25.60ba/s]Running tokenizer on every text in dataset:   1%|          | 404/42009 [00:17<29:43, 23.33ba/s]Running tokenizer on every text in dataset:   1%|          | 407/42009 [00:17<28:31, 24.30ba/s]Running tokenizer on every text in dataset:   1%|          | 410/42009 [00:17<28:00, 24.75ba/s]Running tokenizer on every text in dataset:   1%|          | 413/42009 [00:17<31:22, 22.09ba/s]Running tokenizer on every text in dataset:   1%|          | 416/42009 [00:17<30:02, 23.07ba/s]Running tokenizer on every text in dataset:   1%|          | 419/42009 [00:17<29:03, 23.85ba/s]Running tokenizer on every text in dataset:   1%|          | 422/42009 [00:17<32:18, 21.45ba/s]Running tokenizer on every text in dataset:   1%|          | 425/42009 [00:18<29:47, 23.27ba/s]Running tokenizer on every text in dataset:   1%|          | 428/42009 [00:18<28:34, 24.25ba/s]Running tokenizer on every text in dataset:   1%|          | 431/42009 [00:18<31:07, 22.27ba/s]Running tokenizer on every text in dataset:   1%|          | 434/42009 [00:18<29:36, 23.40ba/s]Running tokenizer on every text in dataset:   1%|          | 437/42009 [00:18<28:07, 24.63ba/s]Running tokenizer on every text in dataset:   1%|          | 440/42009 [00:18<27:20, 25.34ba/s]Running tokenizer on every text in dataset:   1%|          | 443/42009 [00:18<30:06, 23.01ba/s]Running tokenizer on every text in dataset:   1%|          | 446/42009 [00:18<29:05, 23.82ba/s]Running tokenizer on every text in dataset:   1%|          | 449/42009 [00:19<27:54, 24.82ba/s]Running tokenizer on every text in dataset:   1%|          | 452/42009 [00:19<30:43, 22.54ba/s]Running tokenizer on every text in dataset:   1%|          | 455/42009 [00:19<29:18, 23.64ba/s]Running tokenizer on every text in dataset:   1%|          | 458/42009 [00:19<27:40, 25.02ba/s]Running tokenizer on every text in dataset:   1%|          | 461/42009 [00:19<30:25, 22.76ba/s]Running tokenizer on every text in dataset:   1%|          | 464/42009 [00:19<29:12, 23.70ba/s]Running tokenizer on every text in dataset:   1%|          | 467/42009 [00:19<29:16, 23.64ba/s]Running tokenizer on every text in dataset:   1%|          | 470/42009 [00:19<31:45, 21.80ba/s]Running tokenizer on every text in dataset:   1%|          | 473/42009 [00:20<29:48, 23.23ba/s]Running tokenizer on every text in dataset:   1%|          | 476/42009 [00:20<29:00, 23.87ba/s]Running tokenizer on every text in dataset:   1%|          | 479/42009 [00:20<31:33, 21.93ba/s]Running tokenizer on every text in dataset:   1%|          | 482/42009 [00:20<29:24, 23.53ba/s]Running tokenizer on every text in dataset:   1%|          | 485/42009 [00:20<27:58, 24.73ba/s]Running tokenizer on every text in dataset:   1%|          | 488/42009 [00:20<30:19, 22.82ba/s]Running tokenizer on every text in dataset:   1%|          | 491/42009 [00:20<28:39, 24.14ba/s]Running tokenizer on every text in dataset:   1%|          | 494/42009 [00:20<27:36, 25.06ba/s]Running tokenizer on every text in dataset:   1%|          | 497/42009 [00:21<26:38, 25.97ba/s]Running tokenizer on every text in dataset:   1%|          | 500/42009 [00:21<29:19, 23.59ba/s]Running tokenizer on every text in dataset:   1%|          | 503/42009 [00:21<28:02, 24.68ba/s]Running tokenizer on every text in dataset:   1%|          | 506/42009 [00:21<27:11, 25.44ba/s]Running tokenizer on every text in dataset:   1%|          | 509/42009 [00:21<30:27, 22.70ba/s]Running tokenizer on every text in dataset:   1%|          | 512/42009 [00:21<28:42, 24.09ba/s]Running tokenizer on every text in dataset:   1%|          | 515/42009 [00:21<27:56, 24.75ba/s]Running tokenizer on every text in dataset:   1%|          | 518/42009 [00:21<30:38, 22.57ba/s]Running tokenizer on every text in dataset:   1%|          | 521/42009 [00:22<28:34, 24.20ba/s]Running tokenizer on every text in dataset:   1%|          | 524/42009 [00:22<27:06, 25.50ba/s]Running tokenizer on every text in dataset:   1%|▏         | 527/42009 [00:22<30:20, 22.78ba/s]Running tokenizer on every text in dataset:   1%|▏         | 530/42009 [00:22<28:40, 24.11ba/s]Running tokenizer on every text in dataset:   1%|▏         | 533/42009 [00:22<27:38, 25.00ba/s]Running tokenizer on every text in dataset:   1%|▏         | 536/42009 [00:22<30:28, 22.68ba/s]Running tokenizer on every text in dataset:   1%|▏         | 539/42009 [00:22<28:29, 24.25ba/s]Running tokenizer on every text in dataset:   1%|▏         | 542/42009 [00:22<27:11, 25.42ba/s]Running tokenizer on every text in dataset:   1%|▏         | 545/42009 [00:23<29:39, 23.30ba/s]Running tokenizer on every text in dataset:   1%|▏         | 548/42009 [00:23<27:53, 24.78ba/s]Running tokenizer on every text in dataset:   1%|▏         | 551/42009 [00:23<27:00, 25.58ba/s]Running tokenizer on every text in dataset:   1%|▏         | 554/42009 [00:23<26:51, 25.72ba/s]Running tokenizer on every text in dataset:   1%|▏         | 557/42009 [00:23<29:37, 23.32ba/s]Running tokenizer on every text in dataset:   1%|▏         | 560/42009 [00:23<27:48, 24.85ba/s]Running tokenizer on every text in dataset:   1%|▏         | 563/42009 [00:23<26:53, 25.69ba/s]Running tokenizer on every text in dataset:   1%|▏         | 566/42009 [00:23<30:00, 23.02ba/s]Running tokenizer on every text in dataset:   1%|▏         | 569/42009 [00:24<28:35, 24.15ba/s]Running tokenizer on every text in dataset:   1%|▏         | 572/42009 [00:24<27:54, 24.74ba/s]Running tokenizer on every text in dataset:   1%|▏         | 575/42009 [00:24<30:19, 22.77ba/s]Running tokenizer on every text in dataset:   1%|▏         | 578/42009 [00:24<29:25, 23.47ba/s]Running tokenizer on every text in dataset:   1%|▏         | 581/42009 [00:24<28:33, 24.18ba/s]Running tokenizer on every text in dataset:   1%|▏         | 584/42009 [00:24<30:38, 22.53ba/s]Running tokenizer on every text in dataset:   1%|▏         | 587/42009 [00:24<29:33, 23.36ba/s]Running tokenizer on every text in dataset:   1%|▏         | 590/42009 [00:24<29:00, 23.80ba/s]Running tokenizer on every text in dataset:   1%|▏         | 593/42009 [00:25<31:39, 21.80ba/s]Running tokenizer on every text in dataset:   1%|▏         | 596/42009 [00:25<29:51, 23.12ba/s]Running tokenizer on every text in dataset:   1%|▏         | 599/42009 [00:25<28:44, 24.02ba/s]Running tokenizer on every text in dataset:   1%|▏         | 602/42009 [00:25<30:17, 22.78ba/s]Running tokenizer on every text in dataset:   1%|▏         | 606/42009 [00:25<27:32, 25.06ba/s]Running tokenizer on every text in dataset:   1%|▏         | 609/42009 [00:25<26:47, 25.75ba/s]Running tokenizer on every text in dataset:   1%|▏         | 612/42009 [00:25<30:15, 22.81ba/s]Running tokenizer on every text in dataset:   1%|▏         | 616/42009 [00:26<27:44, 24.87ba/s]Running tokenizer on every text in dataset:   1%|▏         | 619/42009 [00:26<26:45, 25.78ba/s]Running tokenizer on every text in dataset:   1%|▏         | 622/42009 [00:26<29:18, 23.54ba/s]Running tokenizer on every text in dataset:   1%|▏         | 625/42009 [00:26<27:35, 25.00ba/s]Running tokenizer on every text in dataset:   1%|▏         | 628/42009 [00:26<26:24, 26.11ba/s]Running tokenizer on every text in dataset:   2%|▏         | 631/42009 [00:26<29:08, 23.67ba/s]Running tokenizer on every text in dataset:   2%|▏         | 634/42009 [00:26<27:20, 25.23ba/s]Running tokenizer on every text in dataset:   2%|▏         | 637/42009 [00:26<26:48, 25.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 640/42009 [00:27<29:58, 23.01ba/s]Running tokenizer on every text in dataset:   2%|▏         | 643/42009 [00:27<28:29, 24.20ba/s]Running tokenizer on every text in dataset:   2%|▏         | 646/42009 [00:27<27:39, 24.92ba/s]Running tokenizer on every text in dataset:   2%|▏         | 649/42009 [00:27<26:42, 25.81ba/s]Running tokenizer on every text in dataset:   2%|▏         | 652/42009 [00:27<29:29, 23.37ba/s]Running tokenizer on every text in dataset:   2%|▏         | 655/42009 [00:27<28:38, 24.07ba/s]Running tokenizer on every text in dataset:   2%|▏         | 658/42009 [00:27<27:58, 24.64ba/s]Running tokenizer on every text in dataset:   2%|▏         | 661/42009 [00:27<30:59, 22.24ba/s]Running tokenizer on every text in dataset:   2%|▏         | 664/42009 [00:27<29:16, 23.54ba/s]Running tokenizer on every text in dataset:   2%|▏         | 667/42009 [00:28<28:22, 24.28ba/s]Running tokenizer on every text in dataset:   2%|▏         | 670/42009 [00:28<30:58, 22.25ba/s]Running tokenizer on every text in dataset:   2%|▏         | 673/42009 [00:28<29:36, 23.27ba/s]Running tokenizer on every text in dataset:   2%|▏         | 676/42009 [00:28<28:21, 24.29ba/s]Running tokenizer on every text in dataset:   2%|▏         | 679/42009 [00:28<30:49, 22.35ba/s]Running tokenizer on every text in dataset:   2%|▏         | 682/42009 [00:28<29:04, 23.69ba/s]Running tokenizer on every text in dataset:   2%|▏         | 685/42009 [00:28<27:50, 24.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 688/42009 [00:29<30:30, 22.58ba/s]Running tokenizer on every text in dataset:   2%|▏         | 691/42009 [00:29<29:41, 23.20ba/s]Running tokenizer on every text in dataset:   2%|▏         | 694/42009 [00:29<28:35, 24.08ba/s]Running tokenizer on every text in dataset:   2%|▏         | 697/42009 [00:29<31:25, 21.91ba/s]Running tokenizer on every text in dataset:   2%|▏         | 700/42009 [00:29<29:54, 23.02ba/s]Running tokenizer on every text in dataset:   2%|▏         | 703/42009 [00:29<28:52, 23.84ba/s]Running tokenizer on every text in dataset:   2%|▏         | 706/42009 [00:29<28:05, 24.50ba/s]Running tokenizer on every text in dataset:   2%|▏         | 709/42009 [00:29<30:12, 22.78ba/s]Running tokenizer on every text in dataset:   2%|▏         | 712/42009 [00:30<29:26, 23.38ba/s]Running tokenizer on every text in dataset:   2%|▏         | 715/42009 [00:30<29:36, 23.25ba/s]Running tokenizer on every text in dataset:   2%|▏         | 718/42009 [00:30<33:52, 20.31ba/s]Running tokenizer on every text in dataset:   2%|▏         | 721/42009 [00:30<31:12, 22.05ba/s]Running tokenizer on every text in dataset:   2%|▏         | 724/42009 [00:30<29:27, 23.36ba/s]Running tokenizer on every text in dataset:   2%|▏         | 727/42009 [00:30<31:51, 21.59ba/s]Running tokenizer on every text in dataset:   2%|▏         | 730/42009 [00:30<30:14, 22.74ba/s]Running tokenizer on every text in dataset:   2%|▏         | 733/42009 [00:30<28:47, 23.89ba/s]Running tokenizer on every text in dataset:   2%|▏         | 736/42009 [00:31<31:50, 21.61ba/s]Running tokenizer on every text in dataset:   2%|▏         | 739/42009 [00:31<30:04, 22.87ba/s]Running tokenizer on every text in dataset:   2%|▏         | 742/42009 [00:31<28:35, 24.06ba/s]Running tokenizer on every text in dataset:   2%|▏         | 745/42009 [00:31<30:53, 22.26ba/s]Running tokenizer on every text in dataset:   2%|▏         | 748/42009 [00:31<28:54, 23.79ba/s]Running tokenizer on every text in dataset:   2%|▏         | 751/42009 [00:31<28:23, 24.22ba/s]Running tokenizer on every text in dataset:   2%|▏         | 754/42009 [00:31<30:57, 22.21ba/s]Running tokenizer on every text in dataset:   2%|▏         | 757/42009 [00:32<29:20, 23.43ba/s]Running tokenizer on every text in dataset:   2%|▏         | 760/42009 [00:32<28:28, 24.14ba/s]Running tokenizer on every text in dataset:   2%|▏         | 763/42009 [00:32<27:33, 24.94ba/s]Running tokenizer on every text in dataset:   2%|▏         | 766/42009 [00:32<30:15, 22.71ba/s]Running tokenizer on every text in dataset:   2%|▏         | 769/42009 [00:32<29:08, 23.58ba/s]Running tokenizer on every text in dataset:   2%|▏         | 772/42009 [00:32<28:39, 23.98ba/s]Running tokenizer on every text in dataset:   2%|▏         | 775/42009 [00:32<32:16, 21.29ba/s]Running tokenizer on every text in dataset:   2%|▏         | 778/42009 [00:32<30:12, 22.75ba/s]Running tokenizer on every text in dataset:   2%|▏         | 781/42009 [00:33<28:34, 24.05ba/s]Running tokenizer on every text in dataset:   2%|▏         | 784/42009 [00:33<30:36, 22.45ba/s]Running tokenizer on every text in dataset:   2%|▏         | 787/42009 [00:33<29:35, 23.21ba/s]Running tokenizer on every text in dataset:   2%|▏         | 790/42009 [00:33<28:44, 23.90ba/s]Running tokenizer on every text in dataset:   2%|▏         | 793/42009 [00:33<30:29, 22.53ba/s]Running tokenizer on every text in dataset:   2%|▏         | 796/42009 [00:33<29:00, 23.68ba/s]Running tokenizer on every text in dataset:   2%|▏         | 799/42009 [00:33<27:55, 24.60ba/s]Running tokenizer on every text in dataset:   2%|▏         | 802/42009 [00:33<30:12, 22.74ba/s]Running tokenizer on every text in dataset:   2%|▏         | 805/42009 [00:34<28:33, 24.04ba/s]Running tokenizer on every text in dataset:   2%|▏         | 808/42009 [00:34<27:25, 25.04ba/s]Running tokenizer on every text in dataset:   2%|▏         | 811/42009 [00:34<29:53, 22.97ba/s]Running tokenizer on every text in dataset:   2%|▏         | 814/42009 [00:34<27:54, 24.60ba/s]Running tokenizer on every text in dataset:   2%|▏         | 818/42009 [00:34<25:44, 26.67ba/s]Running tokenizer on every text in dataset:   2%|▏         | 821/42009 [00:34<29:13, 23.48ba/s]Running tokenizer on every text in dataset:   2%|▏         | 824/42009 [00:34<28:13, 24.33ba/s]Running tokenizer on every text in dataset:   2%|▏         | 827/42009 [00:34<27:09, 25.27ba/s]Running tokenizer on every text in dataset:   2%|▏         | 830/42009 [00:35<30:00, 22.88ba/s]Running tokenizer on every text in dataset:   2%|▏         | 833/42009 [00:35<28:42, 23.90ba/s]Running tokenizer on every text in dataset:   2%|▏         | 836/42009 [00:35<27:34, 24.88ba/s]Running tokenizer on every text in dataset:   2%|▏         | 839/42009 [00:35<27:23, 25.05ba/s]Running tokenizer on every text in dataset:   2%|▏         | 842/42009 [00:35<30:57, 22.17ba/s]Running tokenizer on every text in dataset:   2%|▏         | 845/42009 [00:35<29:12, 23.49ba/s]Running tokenizer on every text in dataset:   2%|▏         | 848/42009 [00:35<28:17, 24.25ba/s]Running tokenizer on every text in dataset:   2%|▏         | 851/42009 [00:36<31:32, 21.74ba/s]Running tokenizer on every text in dataset:   2%|▏         | 854/42009 [00:36<29:54, 22.93ba/s]Running tokenizer on every text in dataset:   2%|▏         | 857/42009 [00:36<28:42, 23.89ba/s]Running tokenizer on every text in dataset:   2%|▏         | 860/42009 [00:36<30:44, 22.31ba/s]Running tokenizer on every text in dataset:   2%|▏         | 863/42009 [00:36<29:40, 23.11ba/s]Running tokenizer on every text in dataset:   2%|▏         | 866/42009 [00:36<28:43, 23.87ba/s]Running tokenizer on every text in dataset:   2%|▏         | 869/42009 [00:36<30:55, 22.18ba/s]Running tokenizer on every text in dataset:   2%|▏         | 872/42009 [00:36<29:00, 23.64ba/s]Running tokenizer on every text in dataset:   2%|▏         | 875/42009 [00:37<27:26, 24.98ba/s]Running tokenizer on every text in dataset:   2%|▏         | 878/42009 [00:37<30:33, 22.43ba/s]Running tokenizer on every text in dataset:   2%|▏         | 881/42009 [00:37<28:43, 23.86ba/s]Running tokenizer on every text in dataset:   2%|▏         | 884/42009 [00:37<27:35, 24.84ba/s]Running tokenizer on every text in dataset:   2%|▏         | 887/42009 [00:37<30:15, 22.65ba/s]Running tokenizer on every text in dataset:   2%|▏         | 890/42009 [00:37<28:42, 23.87ba/s]Running tokenizer on every text in dataset:   2%|▏         | 893/42009 [00:37<28:26, 24.10ba/s]Running tokenizer on every text in dataset:   2%|▏         | 896/42009 [00:37<28:31, 24.02ba/s]Running tokenizer on every text in dataset:   2%|▏         | 899/42009 [00:38<31:01, 22.08ba/s]Running tokenizer on every text in dataset:   2%|▏         | 902/42009 [00:38<29:23, 23.31ba/s]Running tokenizer on every text in dataset:   2%|▏         | 905/42009 [00:38<28:22, 24.15ba/s]Running tokenizer on every text in dataset:   2%|▏         | 908/42009 [00:38<30:58, 22.11ba/s]Running tokenizer on every text in dataset:   2%|▏         | 911/42009 [00:38<29:21, 23.34ba/s]Running tokenizer on every text in dataset:   2%|▏         | 914/42009 [00:38<28:20, 24.17ba/s]Running tokenizer on every text in dataset:   2%|▏         | 917/42009 [00:38<30:48, 22.23ba/s]Running tokenizer on every text in dataset:   2%|▏         | 921/42009 [00:38<27:45, 24.67ba/s]Running tokenizer on every text in dataset:   2%|▏         | 924/42009 [00:39<26:55, 25.43ba/s]Running tokenizer on every text in dataset:   2%|▏         | 927/42009 [00:39<29:59, 22.82ba/s]Running tokenizer on every text in dataset:   2%|▏         | 930/42009 [00:39<29:05, 23.54ba/s]Running tokenizer on every text in dataset:   2%|▏         | 933/42009 [00:39<29:51, 22.93ba/s]Running tokenizer on every text in dataset:   2%|▏         | 936/42009 [00:39<31:37, 21.65ba/s]Running tokenizer on every text in dataset:   2%|▏         | 939/42009 [00:39<29:24, 23.28ba/s]Running tokenizer on every text in dataset:   2%|▏         | 942/42009 [00:39<27:51, 24.57ba/s]Running tokenizer on every text in dataset:   2%|▏         | 945/42009 [00:40<30:30, 22.43ba/s]Running tokenizer on every text in dataset:   2%|▏         | 948/42009 [00:40<29:08, 23.48ba/s]Running tokenizer on every text in dataset:   2%|▏         | 951/42009 [00:40<28:31, 23.99ba/s]Running tokenizer on every text in dataset:   2%|▏         | 954/42009 [00:40<31:19, 21.85ba/s]Running tokenizer on every text in dataset:   2%|▏         | 957/42009 [00:40<30:06, 22.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 960/42009 [00:40<28:13, 24.24ba/s]Running tokenizer on every text in dataset:   2%|▏         | 963/42009 [00:40<30:42, 22.27ba/s]Running tokenizer on every text in dataset:   2%|▏         | 966/42009 [00:40<29:19, 23.32ba/s]Running tokenizer on every text in dataset:   2%|▏         | 969/42009 [00:41<28:47, 23.76ba/s]Running tokenizer on every text in dataset:   2%|▏         | 972/42009 [00:41<28:02, 24.40ba/s]Running tokenizer on every text in dataset:   2%|▏         | 975/42009 [00:41<31:15, 21.88ba/s]Running tokenizer on every text in dataset:   2%|▏         | 978/42009 [00:41<29:18, 23.33ba/s]Running tokenizer on every text in dataset:   2%|▏         | 981/42009 [00:41<27:54, 24.50ba/s]Running tokenizer on every text in dataset:   2%|▏         | 984/42009 [00:41<30:39, 22.30ba/s]Running tokenizer on every text in dataset:   2%|▏         | 987/42009 [00:41<29:24, 23.25ba/s]Running tokenizer on every text in dataset:   2%|▏         | 990/42009 [00:41<27:52, 24.53ba/s]Running tokenizer on every text in dataset:   2%|▏         | 993/42009 [00:42<30:07, 22.70ba/s]Running tokenizer on every text in dataset:   2%|▏         | 996/42009 [00:42<29:19, 23.31ba/s]Running tokenizer on every text in dataset:   2%|▏         | 999/42009 [00:42<28:36, 23.89ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1002/42009 [00:42<30:48, 22.19ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1005/42009 [00:42<28:53, 23.65ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1008/42009 [00:42<27:44, 24.63ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1011/42009 [00:42<29:53, 22.85ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1014/42009 [00:42<28:22, 24.08ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1017/42009 [00:43<27:05, 25.22ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1020/42009 [00:43<29:28, 23.17ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1023/42009 [00:43<29:02, 23.52ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1026/42009 [00:43<28:37, 23.87ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1029/42009 [00:43<28:14, 24.19ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1032/42009 [00:43<30:50, 22.15ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1035/42009 [00:43<29:09, 23.42ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1038/42009 [00:43<28:10, 24.23ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1041/42009 [00:44<30:33, 22.34ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1044/42009 [00:44<28:46, 23.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1047/42009 [00:44<28:46, 23.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 1050/42009 [00:44<30:49, 22.15ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1053/42009 [00:44<29:03, 23.49ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1056/42009 [00:44<28:02, 24.34ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1059/42009 [00:44<31:01, 21.99ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1062/42009 [00:45<29:21, 23.24ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1065/42009 [00:45<28:17, 24.12ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1068/42009 [00:45<31:25, 21.71ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1071/42009 [00:45<30:15, 22.54ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1074/42009 [00:45<28:30, 23.93ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1077/42009 [00:45<30:48, 22.14ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1080/42009 [00:45<29:16, 23.31ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1083/42009 [00:45<28:25, 24.00ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1086/42009 [00:46<27:46, 24.56ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1089/42009 [00:46<30:03, 22.69ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1092/42009 [00:46<28:36, 23.84ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1095/42009 [00:46<27:34, 24.73ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1098/42009 [00:46<29:58, 22.75ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1101/42009 [00:46<28:03, 24.29ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1104/42009 [00:46<26:42, 25.53ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1107/42009 [00:46<29:32, 23.07ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1110/42009 [00:47<28:59, 23.51ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1113/42009 [00:47<28:30, 23.92ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1116/42009 [00:47<30:52, 22.07ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1119/42009 [00:47<29:48, 22.87ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1122/42009 [00:47<28:19, 24.06ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1125/42009 [00:47<30:37, 22.25ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1128/42009 [00:47<28:31, 23.89ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1131/42009 [00:47<27:07, 25.11ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1134/42009 [00:48<30:45, 22.15ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1137/42009 [00:48<29:17, 23.26ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1140/42009 [00:48<27:32, 24.73ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1143/42009 [00:48<26:20, 25.85ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1146/42009 [00:48<29:36, 23.00ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1149/42009 [00:48<28:02, 24.28ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1152/42009 [00:48<26:37, 25.57ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1155/42009 [00:48<29:13, 23.30ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1158/42009 [00:49<28:36, 23.80ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1161/42009 [00:49<27:58, 24.33ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1164/42009 [00:49<31:09, 21.85ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1167/42009 [00:49<29:38, 22.97ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1170/42009 [00:49<28:10, 24.16ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1173/42009 [00:49<29:44, 22.89ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1176/42009 [00:49<28:37, 23.78ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1179/42009 [00:49<27:07, 25.09ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1182/42009 [00:50<30:11, 22.54ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1185/42009 [00:50<29:05, 23.39ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1188/42009 [00:50<28:03, 24.25ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1191/42009 [00:50<30:25, 22.36ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1194/42009 [00:50<29:23, 23.14ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1197/42009 [00:50<28:26, 23.92ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1200/42009 [00:50<27:20, 24.88ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1203/42009 [00:51<29:40, 22.91ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1206/42009 [00:51<28:00, 24.28ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1209/42009 [00:51<28:50, 23.58ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1212/42009 [00:51<32:48, 20.72ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1215/42009 [00:51<30:40, 22.16ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1218/42009 [00:51<29:18, 23.19ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1221/42009 [00:51<31:30, 21.58ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1224/42009 [00:51<29:30, 23.04ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1227/42009 [00:52<28:04, 24.21ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1230/42009 [00:52<30:25, 22.33ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1233/42009 [00:52<28:39, 23.71ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1236/42009 [00:52<27:57, 24.31ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1239/42009 [00:52<30:36, 22.20ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1242/42009 [00:52<29:11, 23.28ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1245/42009 [00:52<28:57, 23.46ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1248/42009 [00:53<30:57, 21.94ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1251/42009 [00:53<30:09, 22.53ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1254/42009 [00:53<29:36, 22.94ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1257/42009 [00:53<28:06, 24.16ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1260/42009 [00:53<30:40, 22.13ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1263/42009 [00:53<30:13, 22.47ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1266/42009 [00:53<28:52, 23.52ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1269/42009 [00:53<31:08, 21.80ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1272/42009 [00:54<29:27, 23.04ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1275/42009 [00:54<28:36, 23.74ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1278/42009 [00:54<30:44, 22.08ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1281/42009 [00:54<28:44, 23.62ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1284/42009 [00:54<27:38, 24.55ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1287/42009 [00:54<30:20, 22.37ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1290/42009 [00:54<28:06, 24.14ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1293/42009 [00:54<27:08, 25.01ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1296/42009 [00:55<29:23, 23.09ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1299/42009 [00:55<27:28, 24.70ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1302/42009 [00:55<26:39, 25.45ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1305/42009 [00:55<28:39, 23.68ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1308/42009 [00:55<27:24, 24.75ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1311/42009 [00:55<28:18, 23.97ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1314/42009 [00:55<26:47, 25.32ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1317/42009 [00:55<28:37, 23.69ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1321/42009 [00:56<26:14, 25.85ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1324/42009 [00:56<28:15, 23.99ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1327/42009 [00:56<27:52, 24.32ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1330/42009 [00:56<27:11, 24.93ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1333/42009 [00:56<27:04, 25.04ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1336/42009 [00:56<29:53, 22.67ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1339/42009 [00:56<28:03, 24.16ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1342/42009 [00:56<26:56, 25.16ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1345/42009 [00:57<29:31, 22.96ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1348/42009 [00:57<28:10, 24.06ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1351/42009 [00:57<26:54, 25.18ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1354/42009 [00:57<29:55, 22.65ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1357/42009 [00:57<28:46, 23.55ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1360/42009 [00:57<27:27, 24.67ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1363/42009 [00:57<29:54, 22.65ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1366/42009 [00:57<28:15, 23.98ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1369/42009 [00:58<27:23, 24.73ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1372/42009 [00:58<29:42, 22.79ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1375/42009 [00:58<28:29, 23.77ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1378/42009 [00:58<27:36, 24.53ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1381/42009 [00:58<30:19, 22.33ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1384/42009 [00:58<28:29, 23.76ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1388/42009 [00:58<26:34, 25.48ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1391/42009 [00:59<28:58, 23.36ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1394/42009 [00:59<27:59, 24.19ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1397/42009 [00:59<26:51, 25.19ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1400/42009 [00:59<28:54, 23.41ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1404/42009 [00:59<26:50, 25.21ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1407/42009 [00:59<26:22, 25.66ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1410/42009 [00:59<29:02, 23.30ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1413/42009 [00:59<27:46, 24.36ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1416/42009 [01:00<26:50, 25.20ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1419/42009 [01:00<29:45, 22.74ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1422/42009 [01:00<29:01, 23.31ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1425/42009 [01:00<27:53, 24.25ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1428/42009 [01:00<26:53, 25.15ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1431/42009 [01:00<29:14, 23.13ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1434/42009 [01:00<28:13, 23.96ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1437/42009 [01:00<27:17, 24.77ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1440/42009 [01:01<30:08, 22.43ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1443/42009 [01:01<29:00, 23.31ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1446/42009 [01:01<28:01, 24.13ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1449/42009 [01:01<30:37, 22.07ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1453/42009 [01:01<27:22, 24.69ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1456/42009 [01:01<26:02, 25.96ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1459/42009 [01:01<29:19, 23.05ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1462/42009 [01:01<28:15, 23.91ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1465/42009 [01:02<27:25, 24.64ba/s]Running tokenizer on every text in dataset:   3%|▎         | 1468/42009 [01:02<31:03, 21.75ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1471/42009 [01:02<30:06, 22.44ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1474/42009 [01:02<28:52, 23.39ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1477/42009 [01:02<31:20, 21.55ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1480/42009 [01:02<29:42, 22.73ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1483/42009 [01:02<28:20, 23.83ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1486/42009 [01:03<30:18, 22.29ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1489/42009 [01:03<28:58, 23.30ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1492/42009 [01:03<27:19, 24.72ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1495/42009 [01:03<29:26, 22.94ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1498/42009 [01:03<27:36, 24.46ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1501/42009 [01:03<26:08, 25.83ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1504/42009 [01:03<25:59, 25.98ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1507/42009 [01:03<28:52, 23.38ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1510/42009 [01:03<27:21, 24.68ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1513/42009 [01:04<26:38, 25.33ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1516/42009 [01:04<29:09, 23.15ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1519/42009 [01:04<27:54, 24.18ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1522/42009 [01:04<27:16, 24.74ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1525/42009 [01:04<29:25, 22.94ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1528/42009 [01:04<27:44, 24.32ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1531/42009 [01:04<26:43, 25.25ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1534/42009 [01:04<29:12, 23.09ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1537/42009 [01:05<27:46, 24.28ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1540/42009 [01:05<26:57, 25.02ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1543/42009 [01:05<29:38, 22.76ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1546/42009 [01:05<28:13, 23.89ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1549/42009 [01:05<27:05, 24.89ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1552/42009 [01:05<29:37, 22.76ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1555/42009 [01:05<28:57, 23.28ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1558/42009 [01:05<27:58, 24.09ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1561/42009 [01:06<26:47, 25.16ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1564/42009 [01:06<29:43, 22.68ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1567/42009 [01:06<28:28, 23.67ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1570/42009 [01:06<27:29, 24.52ba/s]Running tokenizer on every text in dataset:   4%|▎         | 1573/42009 [01:06<30:12, 22.31ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1576/42009 [01:06<28:57, 23.27ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1579/42009 [01:06<27:55, 24.13ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1582/42009 [01:07<30:23, 22.17ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1585/42009 [01:07<30:00, 22.46ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1588/42009 [01:07<28:31, 23.62ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1591/42009 [01:07<30:15, 22.26ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1594/42009 [01:07<28:20, 23.76ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1597/42009 [01:07<27:01, 24.92ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1600/42009 [01:07<30:15, 22.26ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1603/42009 [01:07<29:29, 22.83ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1606/42009 [01:08<28:33, 23.58ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1609/42009 [01:08<31:10, 21.60ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1612/42009 [01:08<29:19, 22.96ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1615/42009 [01:08<28:11, 23.88ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1618/42009 [01:08<27:24, 24.57ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1621/42009 [01:08<29:47, 22.59ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1624/42009 [01:08<28:57, 23.24ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1627/42009 [01:08<28:14, 23.83ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1630/42009 [01:09<30:57, 21.74ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1633/42009 [01:09<29:08, 23.09ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1636/42009 [01:09<27:52, 24.14ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1639/42009 [01:09<29:57, 22.46ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1642/42009 [01:09<28:18, 23.77ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1645/42009 [01:09<27:26, 24.51ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1648/42009 [01:09<29:43, 22.63ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1651/42009 [01:09<28:25, 23.66ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1654/42009 [01:10<27:30, 24.45ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1657/42009 [01:10<29:35, 22.73ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1660/42009 [01:10<28:11, 23.86ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1663/42009 [01:10<27:06, 24.80ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1666/42009 [01:10<29:06, 23.10ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1669/42009 [01:10<27:42, 24.26ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1672/42009 [01:10<27:44, 24.23ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1675/42009 [01:10<27:06, 24.80ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1678/42009 [01:11<29:29, 22.80ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1681/42009 [01:11<28:28, 23.60ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1684/42009 [01:11<27:27, 24.47ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1687/42009 [01:11<29:51, 22.51ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1690/42009 [01:11<28:10, 23.84ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1693/42009 [01:11<28:02, 23.96ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1696/42009 [01:11<30:15, 22.21ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1699/42009 [01:12<28:18, 23.74ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1702/42009 [01:12<27:52, 24.09ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1705/42009 [01:12<31:57, 21.02ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1708/42009 [01:12<30:12, 22.24ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1711/42009 [01:12<28:50, 23.29ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1714/42009 [01:12<30:03, 22.35ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1717/42009 [01:12<28:23, 23.65ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1720/42009 [01:12<27:58, 24.00ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1723/42009 [01:13<30:02, 22.36ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1726/42009 [01:13<28:06, 23.89ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1729/42009 [01:13<26:55, 24.93ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1732/42009 [01:13<25:47, 26.02ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1735/42009 [01:13<28:25, 23.62ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1738/42009 [01:13<27:12, 24.68ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1741/42009 [01:13<26:46, 25.06ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1744/42009 [01:13<29:55, 22.43ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1747/42009 [01:14<28:27, 23.58ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1750/42009 [01:14<26:47, 25.05ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1753/42009 [01:14<29:07, 23.04ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1756/42009 [01:14<27:40, 24.25ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1759/42009 [01:14<27:27, 24.44ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1762/42009 [01:14<30:03, 22.32ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1765/42009 [01:14<28:56, 23.18ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1768/42009 [01:14<28:15, 23.73ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1771/42009 [01:15<30:25, 22.04ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1774/42009 [01:15<28:31, 23.51ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1777/42009 [01:15<28:09, 23.82ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1780/42009 [01:15<31:00, 21.62ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1783/42009 [01:15<28:59, 23.13ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1786/42009 [01:15<27:36, 24.28ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1789/42009 [01:15<27:52, 24.04ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1792/42009 [01:16<30:06, 22.26ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1795/42009 [01:16<28:13, 23.74ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1799/42009 [01:16<29:02, 23.08ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1802/42009 [01:16<27:54, 24.01ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1805/42009 [01:16<27:00, 24.80ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1808/42009 [01:16<26:10, 25.60ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1811/42009 [01:16<28:56, 23.15ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1814/42009 [01:16<27:55, 23.99ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1817/42009 [01:17<27:06, 24.72ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1820/42009 [01:17<29:36, 22.63ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1823/42009 [01:17<29:07, 22.99ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1826/42009 [01:17<28:30, 23.49ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1829/42009 [01:17<31:06, 21.52ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1832/42009 [01:17<28:39, 23.36ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1835/42009 [01:17<27:19, 24.51ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1838/42009 [01:17<29:01, 23.06ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1841/42009 [01:18<27:11, 24.63ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1845/42009 [01:18<25:09, 26.61ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1848/42009 [01:18<27:55, 23.97ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1851/42009 [01:18<27:13, 24.59ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1854/42009 [01:18<26:36, 25.15ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1857/42009 [01:18<29:08, 22.96ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1860/42009 [01:18<27:36, 24.24ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1863/42009 [01:18<26:29, 25.26ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1866/42009 [01:19<30:43, 21.78ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1869/42009 [01:19<28:35, 23.40ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1872/42009 [01:19<27:17, 24.52ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1875/42009 [01:19<29:50, 22.42ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1878/42009 [01:19<28:21, 23.58ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1881/42009 [01:19<26:54, 24.86ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1884/42009 [01:19<26:34, 25.17ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1887/42009 [01:19<29:14, 22.87ba/s]Running tokenizer on every text in dataset:   4%|▍         | 1890/42009 [01:20<28:16, 23.65ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1893/42009 [01:20<27:00, 24.76ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1896/42009 [01:20<29:42, 22.50ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1899/42009 [01:20<28:19, 23.60ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1902/42009 [01:20<27:13, 24.55ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1905/42009 [01:20<28:50, 23.17ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1909/42009 [01:20<25:24, 26.30ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1913/42009 [01:21<25:07, 26.61ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1916/42009 [01:21<24:23, 27.40ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1919/42009 [01:21<24:11, 27.62ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1922/42009 [01:21<23:54, 27.95ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1925/42009 [01:21<27:11, 24.56ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1928/42009 [01:21<26:29, 25.21ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1931/42009 [01:21<25:57, 25.73ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1934/42009 [01:21<28:52, 23.13ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1937/42009 [01:21<27:38, 24.16ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1940/42009 [01:22<26:57, 24.77ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1943/42009 [01:22<29:25, 22.70ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1946/42009 [01:22<28:04, 23.78ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1949/42009 [01:22<27:32, 24.24ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1952/42009 [01:22<30:09, 22.13ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1955/42009 [01:22<27:59, 23.85ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1958/42009 [01:22<26:16, 25.41ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1961/42009 [01:22<28:25, 23.48ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1964/42009 [01:23<26:50, 24.86ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1967/42009 [01:23<26:26, 25.24ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1970/42009 [01:23<29:35, 22.56ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1973/42009 [01:23<28:05, 23.76ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1976/42009 [01:23<27:16, 24.46ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1979/42009 [01:23<26:39, 25.03ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1982/42009 [01:23<28:41, 23.25ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1985/42009 [01:23<27:15, 24.48ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1988/42009 [01:24<28:00, 23.81ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1991/42009 [01:24<33:47, 19.74ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1994/42009 [01:24<31:12, 21.37ba/s]Running tokenizer on every text in dataset:   5%|▍         | 1997/42009 [01:24<30:51, 21.61ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2000/42009 [01:24<32:18, 20.63ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2003/42009 [01:24<29:47, 22.38ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2006/42009 [01:24<28:01, 23.80ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2009/42009 [01:25<30:20, 21.97ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2012/42009 [01:25<28:46, 23.17ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2015/42009 [01:25<27:54, 23.88ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2018/42009 [01:25<30:14, 22.04ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2021/42009 [01:25<28:48, 23.14ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2024/42009 [01:25<27:55, 23.86ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2027/42009 [01:25<31:47, 20.96ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2030/42009 [01:26<29:40, 22.45ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2033/42009 [01:26<27:51, 23.92ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2036/42009 [01:26<26:47, 24.87ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2039/42009 [01:26<29:58, 22.23ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2042/42009 [01:26<28:32, 23.34ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2045/42009 [01:26<27:44, 24.01ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2048/42009 [01:26<30:27, 21.86ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2051/42009 [01:26<30:02, 22.17ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2054/42009 [01:27<28:59, 22.98ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2057/42009 [01:27<30:49, 21.60ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2060/42009 [01:27<28:55, 23.02ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2064/42009 [01:27<26:20, 25.27ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2067/42009 [01:27<28:33, 23.31ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2070/42009 [01:27<27:35, 24.13ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2073/42009 [01:27<26:27, 25.16ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2076/42009 [01:27<28:48, 23.11ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2079/42009 [01:28<27:27, 24.24ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2082/42009 [01:28<26:52, 24.76ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2085/42009 [01:28<28:25, 23.40ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2088/42009 [01:28<27:32, 24.16ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2091/42009 [01:28<26:53, 24.74ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2094/42009 [01:28<29:25, 22.60ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2097/42009 [01:28<28:06, 23.66ba/s]Running tokenizer on every text in dataset:   5%|▍         | 2100/42009 [01:28<27:07, 24.53ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2103/42009 [01:29<29:47, 22.33ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2106/42009 [01:29<29:28, 22.56ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2109/42009 [01:29<27:24, 24.26ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2112/42009 [01:29<26:25, 25.16ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2115/42009 [01:29<28:52, 23.03ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2118/42009 [01:29<28:43, 23.14ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2121/42009 [01:29<28:03, 23.70ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2124/42009 [01:30<33:43, 19.71ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2127/42009 [01:30<34:39, 19.18ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2130/42009 [01:30<32:13, 20.62ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2133/42009 [01:30<32:41, 20.33ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2136/42009 [01:30<29:43, 22.35ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2139/42009 [01:30<27:41, 24.00ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2142/42009 [01:30<29:39, 22.40ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2145/42009 [01:30<27:46, 23.91ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2148/42009 [01:31<26:31, 25.05ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2151/42009 [01:31<29:10, 22.77ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2154/42009 [01:31<27:16, 24.35ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2157/42009 [01:31<25:45, 25.79ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2160/42009 [01:31<28:49, 23.04ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2163/42009 [01:31<27:51, 23.84ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2166/42009 [01:31<26:14, 25.31ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2169/42009 [01:31<25:44, 25.80ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2172/42009 [01:32<28:36, 23.21ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2175/42009 [01:32<27:37, 24.04ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2178/42009 [01:32<26:37, 24.93ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2181/42009 [01:32<29:16, 22.68ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2184/42009 [01:32<28:07, 23.61ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2187/42009 [01:32<26:51, 24.72ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2190/42009 [01:32<29:04, 22.82ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2193/42009 [01:32<27:53, 23.80ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2196/42009 [01:33<27:09, 24.43ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2199/42009 [01:33<30:17, 21.90ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2202/42009 [01:33<29:07, 22.78ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2205/42009 [01:33<28:17, 23.45ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2208/42009 [01:33<30:44, 21.57ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2211/42009 [01:33<29:11, 22.73ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2214/42009 [01:33<28:12, 23.51ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2217/42009 [01:34<31:32, 21.03ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2220/42009 [01:34<29:20, 22.60ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2223/42009 [01:34<27:59, 23.68ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2226/42009 [01:34<26:38, 24.89ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2229/42009 [01:34<29:27, 22.51ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2232/42009 [01:34<28:32, 23.23ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2235/42009 [01:34<27:54, 23.75ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2238/42009 [01:34<30:12, 21.95ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2241/42009 [01:35<27:56, 23.72ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2244/42009 [01:35<26:49, 24.70ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2247/42009 [01:35<29:13, 22.67ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2250/42009 [01:35<27:46, 23.86ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2253/42009 [01:35<27:15, 24.31ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2256/42009 [01:35<29:24, 22.52ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2259/42009 [01:35<27:43, 23.89ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2262/42009 [01:35<27:08, 24.41ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2265/42009 [01:36<29:29, 22.46ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2268/42009 [01:36<27:47, 23.83ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2271/42009 [01:36<26:35, 24.91ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2274/42009 [01:36<28:49, 22.97ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2278/42009 [01:36<26:16, 25.20ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2281/42009 [01:36<25:27, 26.01ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2284/42009 [01:36<27:46, 23.84ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2287/42009 [01:36<26:48, 24.69ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2290/42009 [01:37<27:31, 24.05ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2293/42009 [01:37<30:08, 21.96ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2296/42009 [01:37<28:22, 23.33ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2299/42009 [01:37<27:22, 24.17ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2302/42009 [01:37<26:27, 25.01ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2305/42009 [01:37<28:52, 22.91ba/s]Running tokenizer on every text in dataset:   5%|▌         | 2308/42009 [01:37<27:54, 23.71ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2311/42009 [01:38<28:18, 23.37ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2314/42009 [01:38<29:39, 22.30ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2317/42009 [01:38<28:00, 23.62ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2320/42009 [01:38<26:49, 24.65ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2323/42009 [01:38<28:57, 22.85ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2326/42009 [01:38<27:33, 24.01ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2329/42009 [01:38<26:59, 24.50ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2332/42009 [01:38<29:29, 22.43ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2335/42009 [01:39<27:45, 23.82ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2338/42009 [01:39<26:28, 24.97ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2341/42009 [01:39<28:44, 23.01ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2344/42009 [01:39<27:06, 24.39ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2347/42009 [01:39<26:30, 24.94ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2350/42009 [01:39<30:10, 21.91ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2353/42009 [01:39<29:06, 22.70ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2356/42009 [01:39<27:20, 24.18ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2359/42009 [01:40<25:48, 25.60ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2362/42009 [01:40<27:53, 23.70ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2366/42009 [01:40<25:52, 25.53ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2369/42009 [01:40<28:16, 23.37ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2372/42009 [01:40<27:39, 23.88ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2375/42009 [01:40<27:39, 23.88ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2378/42009 [01:40<26:45, 24.69ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2381/42009 [01:40<29:06, 22.69ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2384/42009 [01:41<28:00, 23.59ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2387/42009 [01:41<27:00, 24.45ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2390/42009 [01:41<29:06, 22.68ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2393/42009 [01:41<27:23, 24.10ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2396/42009 [01:41<26:22, 25.03ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2399/42009 [01:41<28:28, 23.19ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2402/42009 [01:41<27:12, 24.26ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2405/42009 [01:41<25:58, 25.41ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2408/42009 [01:42<28:06, 23.48ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2411/42009 [01:42<26:36, 24.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2414/42009 [01:42<26:06, 25.27ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2417/42009 [01:42<28:45, 22.95ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2420/42009 [01:42<26:48, 24.62ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2424/42009 [01:42<24:28, 26.96ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2427/42009 [01:42<27:08, 24.31ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2430/42009 [01:42<26:27, 24.93ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2433/42009 [01:43<25:46, 25.60ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2436/42009 [01:43<28:27, 23.17ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2439/42009 [01:43<27:03, 24.38ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2442/42009 [01:43<25:50, 25.51ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2445/42009 [01:43<28:13, 23.37ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2448/42009 [01:43<26:46, 24.62ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2451/42009 [01:43<25:21, 25.99ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2454/42009 [01:43<24:49, 26.55ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2457/42009 [01:44<27:43, 23.78ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2460/42009 [01:44<27:07, 24.30ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2463/42009 [01:44<26:26, 24.92ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2466/42009 [01:44<30:00, 21.97ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2469/42009 [01:44<28:38, 23.01ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2472/42009 [01:44<27:17, 24.15ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2475/42009 [01:44<30:15, 21.77ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2478/42009 [01:44<28:17, 23.29ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2481/42009 [01:45<26:58, 24.42ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2484/42009 [01:45<29:22, 22.42ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2487/42009 [01:45<28:05, 23.44ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2490/42009 [01:45<26:44, 24.63ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2493/42009 [01:45<29:10, 22.57ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2496/42009 [01:45<27:35, 23.87ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2499/42009 [01:45<26:32, 24.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2502/42009 [01:45<29:04, 22.64ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2505/42009 [01:46<27:30, 23.94ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2508/42009 [01:46<26:08, 25.18ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2511/42009 [01:46<25:04, 26.26ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2514/42009 [01:46<27:20, 24.07ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2517/42009 [01:46<26:11, 25.13ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2520/42009 [01:46<25:15, 26.05ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2523/42009 [01:46<28:23, 23.17ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2526/42009 [01:46<27:03, 24.32ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2529/42009 [01:47<26:13, 25.10ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2532/42009 [01:47<28:45, 22.88ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2535/42009 [01:47<27:18, 24.08ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2538/42009 [01:47<25:43, 25.58ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2541/42009 [01:47<29:10, 22.54ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2544/42009 [01:47<27:37, 23.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2548/42009 [01:47<25:37, 25.66ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2551/42009 [01:47<28:23, 23.16ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2554/42009 [01:48<27:26, 23.97ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2557/42009 [01:48<26:23, 24.91ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2560/42009 [01:48<29:14, 22.49ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2563/42009 [01:48<28:07, 23.37ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2566/42009 [01:48<26:38, 24.67ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2569/42009 [01:48<29:32, 22.26ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2572/42009 [01:48<27:41, 23.73ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2575/42009 [01:48<26:33, 24.75ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2578/42009 [01:49<29:50, 22.02ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2581/42009 [01:49<28:03, 23.42ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2584/42009 [01:49<26:34, 24.72ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2587/42009 [01:49<25:50, 25.42ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2590/42009 [01:49<28:21, 23.17ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2593/42009 [01:49<27:29, 23.90ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2596/42009 [01:49<26:19, 24.96ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2599/42009 [01:50<28:55, 22.70ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2602/42009 [01:50<27:02, 24.29ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2605/42009 [01:50<25:47, 25.47ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2608/42009 [01:50<29:05, 22.58ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2611/42009 [01:50<28:00, 23.45ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2614/42009 [01:50<27:14, 24.10ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2617/42009 [01:50<30:08, 21.79ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2620/42009 [01:50<28:29, 23.04ba/s]Running tokenizer on every text in dataset:   6%|▌         | 2623/42009 [01:51<27:31, 23.85ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2626/42009 [01:51<29:37, 22.16ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2629/42009 [01:51<28:22, 23.13ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2632/42009 [01:51<27:43, 23.68ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2635/42009 [01:51<29:45, 22.05ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2638/42009 [01:51<28:03, 23.38ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2641/42009 [01:51<27:15, 24.06ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2644/42009 [01:51<26:15, 24.98ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2647/42009 [01:52<28:58, 22.65ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2650/42009 [01:52<27:41, 23.69ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2653/42009 [01:52<26:34, 24.69ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2656/42009 [01:52<29:10, 22.48ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2659/42009 [01:52<27:44, 23.64ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2662/42009 [01:52<26:35, 24.65ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2665/42009 [01:52<28:47, 22.77ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2668/42009 [01:52<27:10, 24.12ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2671/42009 [01:53<25:53, 25.32ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2674/42009 [01:53<28:11, 23.25ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2677/42009 [01:53<27:17, 24.02ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2680/42009 [01:53<26:44, 24.51ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2683/42009 [01:53<29:36, 22.14ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2686/42009 [01:53<28:39, 22.86ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2689/42009 [01:53<28:30, 22.99ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2692/42009 [01:54<30:44, 21.31ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2695/42009 [01:54<28:57, 22.63ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2698/42009 [01:54<27:17, 24.00ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2701/42009 [01:54<26:13, 24.98ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2704/42009 [01:54<28:53, 22.68ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2707/42009 [01:54<27:04, 24.20ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2710/42009 [01:54<25:47, 25.40ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2713/42009 [01:54<29:39, 22.09ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2717/42009 [01:55<26:20, 24.87ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2720/42009 [01:55<25:33, 25.62ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2723/42009 [01:55<28:03, 23.34ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2726/42009 [01:55<27:09, 24.10ba/s]Running tokenizer on every text in dataset:   6%|▋         | 2729/42009 [01:55<26:18, 24.88ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2732/42009 [01:55<28:38, 22.86ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2735/42009 [01:55<26:55, 24.31ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2738/42009 [01:55<25:45, 25.42ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2741/42009 [01:56<28:07, 23.27ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2744/42009 [01:56<26:26, 24.75ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2747/42009 [01:56<25:16, 25.89ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2750/42009 [01:56<28:09, 23.24ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2753/42009 [01:56<27:14, 24.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2756/42009 [01:56<26:03, 25.11ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2759/42009 [01:56<27:54, 23.44ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2762/42009 [01:56<26:21, 24.82ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2765/42009 [01:56<26:12, 24.96ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2768/42009 [01:57<29:08, 22.44ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2771/42009 [01:57<27:42, 23.60ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2774/42009 [01:57<26:35, 24.59ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2777/42009 [01:57<25:37, 25.51ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2780/42009 [01:57<27:42, 23.60ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2783/42009 [01:57<26:19, 24.84ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2786/42009 [01:57<25:04, 26.06ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2789/42009 [01:57<26:48, 24.38ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2792/42009 [01:58<25:27, 25.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2795/42009 [01:58<24:44, 26.41ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2798/42009 [01:58<27:08, 24.09ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2801/42009 [01:58<25:58, 25.17ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2804/42009 [01:58<25:33, 25.56ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2807/42009 [01:58<27:54, 23.41ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2810/42009 [01:58<26:59, 24.20ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2813/42009 [01:58<26:42, 24.45ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2816/42009 [01:59<29:36, 22.07ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2819/42009 [01:59<28:10, 23.18ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2822/42009 [01:59<27:08, 24.07ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2825/42009 [01:59<29:23, 22.22ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2828/42009 [01:59<27:40, 23.60ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2831/42009 [01:59<26:31, 24.62ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2834/42009 [01:59<25:47, 25.32ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2837/42009 [01:59<28:11, 23.16ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2840/42009 [02:00<26:47, 24.36ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2843/42009 [02:00<25:57, 25.15ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2846/42009 [02:00<28:36, 22.81ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2849/42009 [02:00<26:58, 24.19ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2852/42009 [02:00<25:58, 25.12ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2855/42009 [02:00<28:19, 23.04ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2858/42009 [02:00<26:42, 24.44ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2861/42009 [02:00<25:21, 25.74ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2864/42009 [02:01<28:00, 23.29ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2867/42009 [02:01<26:32, 24.57ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2870/42009 [02:01<25:36, 25.47ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2873/42009 [02:01<28:09, 23.16ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2876/42009 [02:01<26:53, 24.25ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2879/42009 [02:01<26:03, 25.03ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2882/42009 [02:01<29:37, 22.01ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2885/42009 [02:01<28:01, 23.26ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2888/42009 [02:02<26:53, 24.24ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2891/42009 [02:02<26:23, 24.70ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2894/42009 [02:02<29:09, 22.36ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2897/42009 [02:02<27:29, 23.71ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2900/42009 [02:02<26:26, 24.65ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2903/42009 [02:02<28:58, 22.49ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2906/42009 [02:02<27:16, 23.89ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2910/42009 [02:02<24:58, 26.08ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2913/42009 [02:03<27:09, 24.00ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2916/42009 [02:03<25:49, 25.23ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2919/42009 [02:03<25:24, 25.64ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2922/42009 [02:03<28:09, 23.14ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2925/42009 [02:03<26:59, 24.13ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2928/42009 [02:03<26:14, 24.82ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2931/42009 [02:03<28:42, 22.68ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2934/42009 [02:04<27:07, 24.00ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2938/42009 [02:04<25:10, 25.86ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2941/42009 [02:04<28:02, 23.23ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2944/42009 [02:04<26:18, 24.75ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2947/42009 [02:04<25:13, 25.82ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2950/42009 [02:04<27:38, 23.55ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2953/42009 [02:04<26:22, 24.69ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2956/42009 [02:04<25:14, 25.79ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2959/42009 [02:05<27:59, 23.25ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2962/42009 [02:05<27:02, 24.07ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2965/42009 [02:05<26:28, 24.57ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2968/42009 [02:05<28:38, 22.71ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2971/42009 [02:05<26:48, 24.27ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2975/42009 [02:05<24:57, 26.07ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2978/42009 [02:05<27:03, 24.04ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2981/42009 [02:05<26:06, 24.92ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2984/42009 [02:06<25:19, 25.68ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2987/42009 [02:06<28:37, 22.72ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2990/42009 [02:06<28:28, 22.84ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2993/42009 [02:06<27:02, 24.04ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2996/42009 [02:06<29:47, 21.83ba/s]Running tokenizer on every text in dataset:   7%|▋         | 2999/42009 [02:06<27:52, 23.33ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3002/42009 [02:06<26:28, 24.55ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3005/42009 [02:06<25:48, 25.18ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3008/42009 [02:07<28:12, 23.05ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3011/42009 [02:07<26:58, 24.09ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3014/42009 [02:07<26:37, 24.41ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3017/42009 [02:07<29:14, 22.22ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3020/42009 [02:07<28:10, 23.06ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3023/42009 [02:07<27:06, 23.97ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3026/42009 [02:07<28:55, 22.46ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3029/42009 [02:07<26:59, 24.07ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3032/42009 [02:08<25:38, 25.33ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3035/42009 [02:08<28:17, 22.96ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3038/42009 [02:08<26:25, 24.57ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3041/42009 [02:08<25:14, 25.73ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3044/42009 [02:08<28:02, 23.16ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3047/42009 [02:08<26:09, 24.83ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3050/42009 [02:08<24:58, 26.01ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3053/42009 [02:08<27:45, 23.39ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3056/42009 [02:09<26:39, 24.35ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3059/42009 [02:09<26:04, 24.90ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3062/42009 [02:09<25:17, 25.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3065/42009 [02:09<27:51, 23.30ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3068/42009 [02:09<26:16, 24.70ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3071/42009 [02:09<25:08, 25.81ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3074/42009 [02:09<27:57, 23.21ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3077/42009 [02:09<27:06, 23.94ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3080/42009 [02:10<26:15, 24.70ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3083/42009 [02:10<28:23, 22.85ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3086/42009 [02:10<26:28, 24.50ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3089/42009 [02:10<25:24, 25.52ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3092/42009 [02:10<28:14, 22.96ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3095/42009 [02:10<27:23, 23.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3098/42009 [02:10<26:40, 24.31ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3101/42009 [02:10<29:19, 22.11ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3104/42009 [02:11<27:32, 23.54ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3107/42009 [02:11<25:58, 24.97ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3110/42009 [02:11<27:39, 23.44ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3114/42009 [02:11<25:16, 25.65ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3117/42009 [02:11<24:32, 26.42ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3120/42009 [02:11<27:16, 23.76ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3123/42009 [02:11<25:59, 24.93ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3126/42009 [02:11<25:46, 25.15ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3129/42009 [02:12<28:55, 22.40ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3132/42009 [02:12<27:48, 23.30ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3135/42009 [02:12<26:42, 24.27ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3138/42009 [02:12<25:56, 24.97ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3141/42009 [02:12<28:25, 22.79ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3144/42009 [02:12<26:32, 24.40ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3147/42009 [02:12<25:18, 25.60ba/s]Running tokenizer on every text in dataset:   7%|▋         | 3150/42009 [02:12<27:59, 23.14ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3153/42009 [02:13<26:24, 24.53ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3156/42009 [02:13<26:04, 24.83ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3159/42009 [02:13<28:29, 22.73ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3162/42009 [02:13<28:18, 22.87ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3165/42009 [02:13<28:02, 23.09ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3168/42009 [02:13<29:58, 21.59ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3171/42009 [02:13<28:15, 22.91ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3174/42009 [02:13<26:51, 24.09ba/s]Running tokenizer on every text in dataset:   8%|▊         | 3177/42009 [02:14<28:47, 22.48ba/s]slurmstepd: error: *** JOB 62203878 ON gpu25 CANCELLED AT 2021-08-13T17:12:23 ***
Running tokenizer on every text in dataset:   8%|▊         | 3180/42009 [02:14<29:06, 22.23ba/s]Job has already finished for job 62203878
