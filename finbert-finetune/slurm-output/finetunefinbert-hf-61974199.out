
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) anaconda/2021-03-tf2

 

2021-07-30 11:45:42.822248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
07/30/2021 11:45:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
07/30/2021 11:45:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=None,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/log/runs/Jul30_11-45-46_gpu32.int.triton.aalto.fi,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
output_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/log,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=log,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/scratch/work/moisioa3/conv_lm/finbert-finetune/log,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
07/30/2021 11:45:47 - WARNING - datasets.builder - Using custom data configuration default-8e0620f55d53b34e
07/30/2021 11:45:47 - INFO - datasets.builder - Overwrite dataset info from restored data version.
07/30/2021 11:45:47 - INFO - datasets.info - Loading Dataset info from /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5
07/30/2021 11:45:47 - WARNING - datasets.builder - Reusing dataset text (/home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)
07/30/2021 11:45:47 - INFO - datasets.info - Loading Dataset info from /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 246.93it/s]
[INFO|configuration_utils.py:545] 2021-07-30 11:45:47,818 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-07-30 11:45:47,819 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|configuration_utils.py:545] 2021-07-30 11:45:48,710 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-07-30 11:45:48,711 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|tokenization_utils_base.py:1730] 2021-07-30 11:45:51,387 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt from cache at /home/moisioa3/.cache/huggingface/transformers/69c0c339871654aa7305fe47345f0b713e6973a476eb1cf5f200d557b6bad765.ee591817c6a7d736b63494878a337beccf9497af463ab8eb01d19bf5f7169026
[INFO|tokenization_utils_base.py:1730] 2021-07-30 11:45:51,388 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json from cache at /home/moisioa3/.cache/huggingface/transformers/3583dbf83678cb60c5faaf0a07aa0d452fc4ec09aac87b8680027bf79b1a6270.e49785bf2de92e06a4d89026870d6979723c8e64cfc9311596ca5b9a3b56289e
[INFO|tokenization_utils_base.py:1730] 2021-07-30 11:45:51,388 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-07-30 11:45:51,388 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-07-30 11:45:51,388 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer_config.json from cache at /home/moisioa3/.cache/huggingface/transformers/978f5aef1d382479fb5ee87b700be69b8cc9ac5f26184a5835fc6e0c03571860.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|configuration_utils.py:545] 2021-07-30 11:45:51,831 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-07-30 11:45:51,833 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|modeling_utils.py:1271] 2021-07-30 11:45:52,334 >> loading weights file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/pytorch_model.bin from cache at /home/moisioa3/.cache/huggingface/transformers/276bf5f0d95b31fc0ed72ef6e2e1b771f2265351a4d322667fd8c73d8473d3fc.3d524bdc756dfbb2ba6c3c3a18e4e2afcc84034db29556b337605e9f8c39c2c2
[WARNING|modeling_utils.py:1501] 2021-07-30 11:45:54,361 >> Some weights of the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1518] 2021-07-30 11:45:54,361 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
07/30/2021 11:45:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-1cafc5cc695701e6.arrow
07/30/2021 11:45:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-96a9159252db4335.arrow
07/30/2021 11:45:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-b558d7ac623b7ff3.arrow
07/30/2021 11:45:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-8e0620f55d53b34e/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-b83e85d2b9ca780e.arrow
[INFO|trainer.py:521] 2021-07-30 11:45:58,873 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.
[INFO|trainer.py:1164] 2021-07-30 11:45:58,889 >> ***** Running training *****
[INFO|trainer.py:1165] 2021-07-30 11:45:58,889 >>   Num examples = 164
[INFO|trainer.py:1166] 2021-07-30 11:45:58,889 >>   Num Epochs = 3
[INFO|trainer.py:1167] 2021-07-30 11:45:58,889 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1168] 2021-07-30 11:45:58,890 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1169] 2021-07-30 11:45:58,890 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1170] 2021-07-30 11:45:58,890 >>   Total optimization steps = 63
  0%|          | 0/63 [00:00<?, ?it/s]  2%|▏         | 1/63 [00:00<00:36,  1.68it/s]  3%|▎         | 2/63 [00:00<00:32,  1.89it/s]  5%|▍         | 3/63 [00:01<00:28,  2.08it/s]  6%|▋         | 4/63 [00:01<00:26,  2.24it/s]  8%|▊         | 5/63 [00:02<00:24,  2.35it/s] 10%|▉         | 6/63 [00:02<00:23,  2.45it/s] 11%|█         | 7/63 [00:02<00:22,  2.52it/s] 13%|█▎        | 8/63 [00:03<00:21,  2.58it/s] 14%|█▍        | 9/63 [00:03<00:20,  2.62it/s] 16%|█▌        | 10/63 [00:03<00:20,  2.64it/s] 17%|█▋        | 11/63 [00:04<00:19,  2.66it/s] 19%|█▉        | 12/63 [00:04<00:19,  2.67it/s] 21%|██        | 13/63 [00:05<00:18,  2.68it/s] 22%|██▏       | 14/63 [00:05<00:18,  2.70it/s] 24%|██▍       | 15/63 [00:05<00:17,  2.69it/s] 25%|██▌       | 16/63 [00:06<00:17,  2.70it/s] 27%|██▋       | 17/63 [00:06<00:17,  2.70it/s] 29%|██▊       | 18/63 [00:06<00:16,  2.70it/s] 30%|███       | 19/63 [00:07<00:16,  2.70it/s] 32%|███▏      | 20/63 [00:07<00:15,  2.70it/s] 33%|███▎      | 21/63 [00:07<00:13,  3.12it/s] 35%|███▍      | 22/63 [00:08<00:13,  2.99it/s] 37%|███▋      | 23/63 [00:08<00:13,  2.90it/s] 38%|███▊      | 24/63 [00:08<00:13,  2.83it/s] 40%|███▉      | 25/63 [00:09<00:13,  2.79it/s] 41%|████▏     | 26/63 [00:09<00:13,  2.77it/s] 43%|████▎     | 27/63 [00:10<00:13,  2.75it/s] 44%|████▍     | 28/63 [00:10<00:12,  2.74it/s] 46%|████▌     | 29/63 [00:10<00:12,  2.72it/s] 48%|████▊     | 30/63 [00:11<00:12,  2.71it/s] 49%|████▉     | 31/63 [00:11<00:11,  2.71it/s] 51%|█████     | 32/63 [00:11<00:11,  2.71it/s] 52%|█████▏    | 33/63 [00:12<00:11,  2.72it/s] 54%|█████▍    | 34/63 [00:12<00:10,  2.70it/s] 56%|█████▌    | 35/63 [00:13<00:10,  2.71it/s] 57%|█████▋    | 36/63 [00:13<00:09,  2.71it/s] 59%|█████▊    | 37/63 [00:13<00:09,  2.71it/s] 60%|██████    | 38/63 [00:14<00:09,  2.71it/s] 62%|██████▏   | 39/63 [00:14<00:08,  2.70it/s] 63%|██████▎   | 40/63 [00:14<00:08,  2.70it/s] 65%|██████▌   | 41/63 [00:15<00:08,  2.70it/s] 67%|██████▋   | 42/63 [00:15<00:06,  3.12it/s] 68%|██████▊   | 43/63 [00:15<00:06,  2.99it/s] 70%|██████▉   | 44/63 [00:16<00:06,  2.89it/s] 71%|███████▏  | 45/63 [00:16<00:06,  2.83it/s] 73%|███████▎  | 46/63 [00:16<00:06,  2.79it/s] 75%|███████▍  | 47/63 [00:17<00:05,  2.76it/s] 76%|███████▌  | 48/63 [00:17<00:05,  2.73it/s] 78%|███████▊  | 49/63 [00:18<00:05,  2.73it/s] 79%|███████▉  | 50/63 [00:18<00:04,  2.71it/s] 81%|████████  | 51/63 [00:18<00:04,  2.71it/s] 83%|████████▎ | 52/63 [00:19<00:04,  2.71it/s] 84%|████████▍ | 53/63 [00:19<00:03,  2.70it/s] 86%|████████▌ | 54/63 [00:19<00:03,  2.70it/s] 87%|████████▋ | 55/63 [00:20<00:02,  2.69it/s] 89%|████████▉ | 56/63 [00:20<00:02,  2.69it/s] 90%|█████████ | 57/63 [00:20<00:02,  2.69it/s] 92%|█████████▏| 58/63 [00:21<00:01,  2.70it/s] 94%|█████████▎| 59/63 [00:21<00:01,  2.69it/s] 95%|█████████▌| 60/63 [00:22<00:01,  2.69it/s] 97%|█████████▋| 61/63 [00:22<00:00,  2.69it/s] 98%|█████████▊| 62/63 [00:22<00:00,  2.69it/s]100%|██████████| 63/63 [00:23<00:00,  3.11it/s][INFO|trainer.py:1360] 2021-07-30 11:46:21,962 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               100%|██████████| 63/63 [00:23<00:00,  3.11it/s]100%|██████████| 63/63 [00:23<00:00,  2.73it/s]
[INFO|trainer.py:1919] 2021-07-30 11:46:21,966 >> Saving model checkpoint to /scratch/work/moisioa3/conv_lm/finbert-finetune/log
[INFO|configuration_utils.py:379] 2021-07-30 11:46:21,968 >> Configuration saved in /scratch/work/moisioa3/conv_lm/finbert-finetune/log/config.json
[INFO|modeling_utils.py:997] 2021-07-30 11:46:22,809 >> Model weights saved in /scratch/work/moisioa3/conv_lm/finbert-finetune/log/pytorch_model.bin
[INFO|tokenization_utils_base.py:2006] 2021-07-30 11:46:22,812 >> tokenizer config file saved in /scratch/work/moisioa3/conv_lm/finbert-finetune/log/tokenizer_config.json
[INFO|tokenization_utils_base.py:2012] 2021-07-30 11:46:22,813 >> Special tokens file saved in /scratch/work/moisioa3/conv_lm/finbert-finetune/log/special_tokens_map.json
{'train_runtime': 23.0748, 'train_samples_per_second': 21.322, 'train_steps_per_second': 2.73, 'train_loss': 3.990992712596106, 'epoch': 3.0}
***** train metrics *****
  epoch                    =        3.0
  train_loss               =      3.991
  train_runtime            = 0:00:23.07
  train_samples            =        164
  train_samples_per_second =     21.322
  train_steps_per_second   =       2.73
07/30/2021 11:46:22 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:521] 2021-07-30 11:46:22,882 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.
[INFO|trainer.py:2165] 2021-07-30 11:46:22,884 >> ***** Running Evaluation *****
[INFO|trainer.py:2167] 2021-07-30 11:46:22,884 >>   Num examples = 45
[INFO|trainer.py:2170] 2021-07-30 11:46:22,884 >>   Batch size = 8
  0%|          | 0/6 [00:00<?, ?it/s] 33%|███▎      | 2/6 [00:00<00:00, 17.59it/s] 50%|█████     | 3/6 [00:00<00:00, 13.28it/s] 67%|██████▋   | 4/6 [00:00<00:00, 11.39it/s] 83%|████████▎ | 5/6 [00:00<00:00, 10.40it/s]100%|██████████| 6/6 [00:00<00:00,  9.73it/s]100%|██████████| 6/6 [00:00<00:00,  9.17it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_loss               =     3.6501
  eval_runtime            = 0:00:00.66
  eval_samples            =         45
  eval_samples_per_second =     67.565
  eval_steps_per_second   =      9.009
  perplexity              =    38.4787
