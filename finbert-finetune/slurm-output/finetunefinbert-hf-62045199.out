
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) miniconda/4.9.2

 

08/04/2021 13:47:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/04/2021 13:47:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=None,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/runs/Aug04_13-47-48_gpu27.int.triton.aalto.fi,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
output_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=bert-base-finnish-cased-v1-finetuned-web-dsp,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/04/2021 13:47:49 - WARNING - datasets.builder - Using custom data configuration default-65a6a01de02db827
08/04/2021 13:47:49 - INFO - datasets.builder - Generating dataset text (/home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)
Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 5679.49it/s]
08/04/2021 13:47:49 - INFO - datasets.utils.download_manager - Downloading took 0.0 min
08/04/2021 13:47:51 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 142.63it/s]
08/04/2021 13:47:51 - INFO - datasets.utils.info_utils - Unable to verify checksums.
08/04/2021 13:47:51 - INFO - datasets.builder - Generating split train
0 tables [00:00, ? tables/s]1 tables [00:00,  2.82 tables/s]2 tables [00:00,  4.34 tables/s]3 tables [00:00,  5.41 tables/s]4 tables [00:00,  6.21 tables/s]5 tables [00:00,  6.76 tables/s]6 tables [00:00,  7.43 tables/s]7 tables [00:01,  7.33 tables/s]8 tables [00:01,  7.85 tables/s]9 tables [00:01,  6.84 tables/s]10 tables [00:01,  7.16 tables/s]11 tables [00:01,  7.37 tables/s]12 tables [00:01,  7.99 tables/s]13 tables [00:01,  7.91 tables/s]14 tables [00:02,  7.69 tables/s]15 tables [00:02,  6.77 tables/s]16 tables [00:02,  6.69 tables/s]17 tables [00:02,  7.11 tables/s]18 tables [00:02,  7.10 tables/s]19 tables [00:02,  7.63 tables/s]20 tables [00:02,  7.47 tables/s]21 tables [00:03,  7.53 tables/s]22 tables [00:03,  6.93 tables/s]23 tables [00:03,  7.36 tables/s]24 tables [00:03,  7.21 tables/s]25 tables [00:03,  7.15 tables/s]26 tables [00:03,  7.28 tables/s]27 tables [00:03,  7.63 tables/s]28 tables [00:04,  7.03 tables/s]29 tables [00:04,  7.28 tables/s]30 tables [00:04,  7.17 tables/s]31 tables [00:04,  7.52 tables/s]32 tables [00:04,  7.79 tables/s]33 tables [00:04,  7.89 tables/s]34 tables [00:04,  7.99 tables/s]35 tables [00:04,  8.06 tables/s]36 tables [00:05,  7.66 tables/s]37 tables [00:05,  7.32 tables/s]38 tables [00:05,  7.56 tables/s]39 tables [00:05,  8.05 tables/s]40 tables [00:05,  7.39 tables/s]41 tables [00:05,  7.32 tables/s]42 tables [00:05,  7.94 tables/s]43 tables [00:05,  8.25 tables/s]44 tables [00:06,  8.48 tables/s]45 tables [00:06,  8.66 tables/s]46 tables [00:06,  8.73 tables/s]47 tables [00:06,  7.95 tables/s]48 tables [00:06,  7.87 tables/s]                                 08/04/2021 13:47:57 - INFO - datasets.builder - Generating split validation
0 tables [00:00, ? tables/s]                            08/04/2021 13:47:57 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset text downloaded and prepared to /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 41.26it/s]
[INFO|file_utils.py:1631] 2021-08-04 13:47:58,428 >> https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/moisioa3/.cache/huggingface/transformers/tmpfk7s__j6
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 218kB/s]
[INFO|file_utils.py:1635] 2021-08-04 13:47:58,878 >> storing https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json in cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|file_utils.py:1643] 2021-08-04 13:47:58,879 >> creating metadata file for /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:545] 2021-08-04 13:47:58,885 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-04 13:47:58,886 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|file_utils.py:1631] 2021-08-04 13:47:59,336 >> https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /home/moisioa3/.cache/huggingface/transformers/tmph6tjeuby
Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 16.0kB/s]
[INFO|file_utils.py:1635] 2021-08-04 13:47:59,783 >> storing https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer_config.json in cache at /home/moisioa3/.cache/huggingface/transformers/978f5aef1d382479fb5ee87b700be69b8cc9ac5f26184a5835fc6e0c03571860.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|file_utils.py:1643] 2021-08-04 13:47:59,785 >> creating metadata file for /home/moisioa3/.cache/huggingface/transformers/978f5aef1d382479fb5ee87b700be69b8cc9ac5f26184a5835fc6e0c03571860.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|configuration_utils.py:545] 2021-08-04 13:48:00,236 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-04 13:48:00,237 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|file_utils.py:1631] 2021-08-04 13:48:01,136 >> https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /home/moisioa3/.cache/huggingface/transformers/tmp7k7o9ovl
Downloading:   0%|          | 0.00/424k [00:00<?, ?B/s]Downloading:   8%|▊         | 32.8k/424k [00:00<00:01, 326kB/s]Downloading:  42%|████▏     | 180k/424k [00:00<00:00, 988kB/s] Downloading: 100%|██████████| 424k/424k [00:00<00:00, 2.00MB/s]
[INFO|file_utils.py:1635] 2021-08-04 13:48:01,799 >> storing https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt in cache at /home/moisioa3/.cache/huggingface/transformers/69c0c339871654aa7305fe47345f0b713e6973a476eb1cf5f200d557b6bad765.ee591817c6a7d736b63494878a337beccf9497af463ab8eb01d19bf5f7169026
[INFO|file_utils.py:1643] 2021-08-04 13:48:01,800 >> creating metadata file for /home/moisioa3/.cache/huggingface/transformers/69c0c339871654aa7305fe47345f0b713e6973a476eb1cf5f200d557b6bad765.ee591817c6a7d736b63494878a337beccf9497af463ab8eb01d19bf5f7169026
[INFO|file_utils.py:1631] 2021-08-04 13:48:02,264 >> https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/moisioa3/.cache/huggingface/transformers/tmphjouv029
Downloading:   0%|          | 0.00/816k [00:00<?, ?B/s]Downloading:   4%|▎         | 28.7k/816k [00:00<00:02, 285kB/s]Downloading:  22%|██▏       | 183k/816k [00:00<00:00, 1.01MB/s]Downloading:  55%|█████▌    | 453k/816k [00:00<00:00, 1.75MB/s]Downloading: 100%|██████████| 816k/816k [00:00<00:00, 2.57MB/s]
[INFO|file_utils.py:1635] 2021-08-04 13:48:03,038 >> storing https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json in cache at /home/moisioa3/.cache/huggingface/transformers/3583dbf83678cb60c5faaf0a07aa0d452fc4ec09aac87b8680027bf79b1a6270.e49785bf2de92e06a4d89026870d6979723c8e64cfc9311596ca5b9a3b56289e
[INFO|file_utils.py:1643] 2021-08-04 13:48:03,039 >> creating metadata file for /home/moisioa3/.cache/huggingface/transformers/3583dbf83678cb60c5faaf0a07aa0d452fc4ec09aac87b8680027bf79b1a6270.e49785bf2de92e06a4d89026870d6979723c8e64cfc9311596ca5b9a3b56289e
[INFO|tokenization_utils_base.py:1730] 2021-08-04 13:48:04,397 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/vocab.txt from cache at /home/moisioa3/.cache/huggingface/transformers/69c0c339871654aa7305fe47345f0b713e6973a476eb1cf5f200d557b6bad765.ee591817c6a7d736b63494878a337beccf9497af463ab8eb01d19bf5f7169026
[INFO|tokenization_utils_base.py:1730] 2021-08-04 13:48:04,397 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer.json from cache at /home/moisioa3/.cache/huggingface/transformers/3583dbf83678cb60c5faaf0a07aa0d452fc4ec09aac87b8680027bf79b1a6270.e49785bf2de92e06a4d89026870d6979723c8e64cfc9311596ca5b9a3b56289e
[INFO|tokenization_utils_base.py:1730] 2021-08-04 13:48:04,398 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-08-04 13:48:04,398 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1730] 2021-08-04 13:48:04,398 >> loading file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/tokenizer_config.json from cache at /home/moisioa3/.cache/huggingface/transformers/978f5aef1d382479fb5ee87b700be69b8cc9ac5f26184a5835fc6e0c03571860.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f
[INFO|configuration_utils.py:545] 2021-08-04 13:48:04,849 >> loading configuration file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/config.json from cache at /home/moisioa3/.cache/huggingface/transformers/e27939251243299384d3c49756d6710f25a683fa4d5e00e6f42fe6cc59202f07.1b2c5b5f39fed7ac39db55c0d2566730a96257ac7215ad6c2a8a109e2ccf1ccd
[INFO|configuration_utils.py:581] 2021-08-04 13:48:04,850 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|file_utils.py:1631] 2021-08-04 13:48:05,384 >> https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/moisioa3/.cache/huggingface/transformers/tmpg4esvz6v
Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]Downloading:   0%|          | 25.6k/501M [00:00<48:52, 171kB/s]Downloading:   0%|          | 200k/501M [00:00<11:06, 751kB/s] Downloading:   0%|          | 844k/501M [00:00<03:11, 2.62MB/s]Downloading:   0%|          | 1.78M/501M [00:00<01:46, 4.69MB/s]Downloading:   1%|          | 4.52M/501M [00:00<00:42, 11.7MB/s]Downloading:   2%|▏         | 8.25M/501M [00:00<00:25, 19.6MB/s]Downloading:   2%|▏         | 11.5M/501M [00:00<00:21, 23.2MB/s]Downloading:   3%|▎         | 16.1M/501M [00:00<00:16, 30.0MB/s]Downloading:   4%|▍         | 19.2M/501M [00:01<00:16, 30.1MB/s]Downloading:   4%|▍         | 22.3M/501M [00:01<00:15, 30.0MB/s]Downloading:   5%|▌         | 26.3M/501M [00:01<00:14, 32.7MB/s]Downloading:   6%|▌         | 30.0M/501M [00:01<00:13, 34.0MB/s]Downloading:   7%|▋         | 33.4M/501M [00:01<00:13, 33.6MB/s]Downloading:   8%|▊         | 37.8M/501M [00:01<00:12, 36.2MB/s]Downloading:   8%|▊         | 41.4M/501M [00:01<00:13, 33.7MB/s]Downloading:   9%|▉         | 45.7M/501M [00:01<00:12, 36.2MB/s]Downloading:  10%|▉         | 49.4M/501M [00:01<00:13, 33.7MB/s]Downloading:  11%|█         | 53.6M/501M [00:02<00:12, 36.1MB/s]Downloading:  11%|█▏        | 57.3M/501M [00:02<00:13, 33.7MB/s]Downloading:  12%|█▏        | 61.5M/501M [00:02<00:12, 36.1MB/s]Downloading:  13%|█▎        | 65.2M/501M [00:02<00:12, 33.7MB/s]Downloading:  14%|█▍        | 69.4M/501M [00:02<00:12, 35.8MB/s]Downloading:  15%|█▍        | 73.1M/501M [00:02<00:12, 33.9MB/s]Downloading:  15%|█▌        | 77.3M/501M [00:02<00:11, 36.1MB/s]Downloading:  16%|█▌        | 81.0M/501M [00:02<00:12, 33.7MB/s]Downloading:  17%|█▋        | 84.4M/501M [00:02<00:12, 33.9MB/s]Downloading:  18%|█▊        | 88.0M/501M [00:03<00:12, 33.9MB/s]Downloading:  18%|█▊        | 91.8M/501M [00:03<00:11, 35.1MB/s]Downloading:  19%|█▉        | 95.8M/501M [00:03<00:11, 35.7MB/s]Downloading:  20%|█▉        | 99.4M/501M [00:03<00:11, 34.2MB/s]Downloading:  21%|██        | 103M/501M [00:03<00:11, 35.2MB/s] Downloading:  21%|██▏       | 107M/501M [00:03<00:11, 33.3MB/s]Downloading:  22%|██▏       | 112M/501M [00:03<00:10, 36.7MB/s]Downloading:  23%|██▎       | 115M/501M [00:03<00:11, 34.3MB/s]Downloading:  24%|██▍       | 120M/501M [00:03<00:10, 36.2MB/s]Downloading:  25%|██▍       | 123M/501M [00:04<00:11, 34.0MB/s]Downloading:  25%|██▌       | 127M/501M [00:04<00:10, 36.1MB/s]Downloading:  26%|██▌       | 131M/501M [00:04<00:10, 33.7MB/s]Downloading:  27%|██▋       | 135M/501M [00:04<00:10, 36.1MB/s]Downloading:  28%|██▊       | 139M/501M [00:04<00:10, 33.1MB/s]Downloading:  29%|██▊       | 143M/501M [00:04<00:10, 35.5MB/s]Downloading:  29%|██▉       | 147M/501M [00:04<00:10, 33.6MB/s]Downloading:  30%|███       | 151M/501M [00:04<00:09, 35.7MB/s]Downloading:  31%|███       | 155M/501M [00:04<00:10, 34.0MB/s]Downloading:  32%|███▏      | 159M/501M [00:05<00:09, 35.7MB/s]Downloading:  33%|███▎      | 163M/501M [00:05<00:09, 34.0MB/s]Downloading:  33%|███▎      | 167M/501M [00:05<00:09, 35.5MB/s]Downloading:  34%|███▍      | 171M/501M [00:05<00:09, 33.8MB/s]Downloading:  35%|███▍      | 175M/501M [00:05<00:09, 35.7MB/s]Downloading:  36%|███▌      | 179M/501M [00:05<00:09, 33.9MB/s]Downloading:  37%|███▋      | 183M/501M [00:05<00:08, 35.9MB/s]Downloading:  37%|███▋      | 186M/501M [00:05<00:09, 34.0MB/s]Downloading:  38%|███▊      | 191M/501M [00:05<00:08, 35.7MB/s]Downloading:  39%|███▉      | 194M/501M [00:06<00:09, 34.0MB/s]Downloading:  40%|███▉      | 199M/501M [00:06<00:08, 36.0MB/s]Downloading:  40%|████      | 202M/501M [00:06<00:08, 33.5MB/s]Downloading:  41%|████▏     | 207M/501M [00:06<00:08, 35.7MB/s]Downloading:  42%|████▏     | 210M/501M [00:06<00:08, 33.7MB/s]Downloading:  43%|████▎     | 215M/501M [00:06<00:07, 35.8MB/s]Downloading:  44%|████▎     | 218M/501M [00:06<00:08, 33.8MB/s]Downloading:  44%|████▍     | 222M/501M [00:06<00:07, 35.9MB/s]Downloading:  45%|████▌     | 226M/501M [00:06<00:08, 34.0MB/s]Downloading:  46%|████▌     | 230M/501M [00:07<00:07, 35.8MB/s]Downloading:  47%|████▋     | 234M/501M [00:07<00:07, 33.9MB/s]Downloading:  48%|████▊     | 238M/501M [00:07<00:07, 35.8MB/s]Downloading:  48%|████▊     | 242M/501M [00:07<00:07, 33.9MB/s]Downloading:  49%|████▉     | 246M/501M [00:07<00:07, 35.8MB/s]Downloading:  50%|████▉     | 250M/501M [00:07<00:07, 33.7MB/s]Downloading:  51%|█████     | 254M/501M [00:07<00:06, 35.9MB/s]Downloading:  51%|█████▏    | 258M/501M [00:07<00:07, 33.9MB/s]Downloading:  52%|█████▏    | 262M/501M [00:07<00:06, 35.6MB/s]Downloading:  53%|█████▎    | 266M/501M [00:08<00:06, 33.9MB/s]Downloading:  54%|█████▍    | 269M/501M [00:08<00:07, 29.9MB/s]Downloading:  55%|█████▍    | 273M/501M [00:08<00:07, 32.3MB/s]Downloading:  55%|█████▌    | 276M/501M [00:08<00:07, 29.5MB/s]Downloading:  56%|█████▌    | 280M/501M [00:08<00:07, 30.9MB/s]Downloading:  57%|█████▋    | 284M/501M [00:08<00:06, 31.2MB/s]Downloading:  57%|█████▋    | 288M/501M [00:08<00:06, 33.7MB/s]Downloading:  58%|█████▊    | 291M/501M [00:08<00:06, 30.9MB/s]Downloading:  59%|█████▉    | 295M/501M [00:09<00:06, 31.3MB/s]Downloading:  60%|█████▉    | 298M/501M [00:09<00:06, 33.5MB/s]Downloading:  60%|██████    | 302M/501M [00:09<00:06, 32.7MB/s]Downloading:  61%|██████    | 306M/501M [00:09<00:05, 35.9MB/s]Downloading:  62%|██████▏   | 310M/501M [00:09<00:05, 34.0MB/s]Downloading:  63%|██████▎   | 313M/501M [00:09<00:05, 34.2MB/s]Downloading:  63%|██████▎   | 317M/501M [00:09<00:05, 33.8MB/s]Downloading:  64%|██████▍   | 321M/501M [00:09<00:05, 35.2MB/s]Downloading:  65%|██████▍   | 325M/501M [00:09<00:04, 36.6MB/s]Downloading:  66%|██████▌   | 329M/501M [00:10<00:05, 34.4MB/s]Downloading:  66%|██████▋   | 332M/501M [00:10<00:04, 33.8MB/s]Downloading:  67%|██████▋   | 336M/501M [00:10<00:04, 33.9MB/s]Downloading:  68%|██████▊   | 339M/501M [00:10<00:04, 35.1MB/s]Downloading:  68%|██████▊   | 343M/501M [00:10<00:04, 35.1MB/s]Downloading:  69%|██████▉   | 346M/501M [00:10<00:04, 34.4MB/s]Downloading:  70%|███████   | 351M/501M [00:10<00:04, 37.3MB/s]Downloading:  71%|███████   | 355M/501M [00:10<00:04, 34.2MB/s]Downloading:  72%|███████▏  | 359M/501M [00:10<00:03, 37.1MB/s]Downloading:  72%|███████▏  | 363M/501M [00:11<00:04, 34.3MB/s]Downloading:  73%|███████▎  | 367M/501M [00:11<00:03, 36.3MB/s]Downloading:  74%|███████▍  | 371M/501M [00:11<00:03, 33.7MB/s]Downloading:  75%|███████▍  | 375M/501M [00:11<00:03, 35.8MB/s]Downloading:  76%|███████▌  | 379M/501M [00:11<00:03, 34.2MB/s]Downloading:  76%|███████▋  | 383M/501M [00:11<00:03, 36.0MB/s]Downloading:  77%|███████▋  | 386M/501M [00:11<00:03, 33.4MB/s]Downloading:  78%|███████▊  | 391M/501M [00:11<00:03, 35.8MB/s]Downloading:  79%|███████▉  | 394M/501M [00:11<00:03, 34.0MB/s]Downloading:  80%|███████▉  | 399M/501M [00:12<00:02, 35.8MB/s]Downloading:  80%|████████  | 402M/501M [00:12<00:02, 34.3MB/s]Downloading:  81%|████████  | 406M/501M [00:12<00:02, 35.0MB/s]Downloading:  82%|████████▏ | 410M/501M [00:12<00:02, 33.0MB/s]Downloading:  83%|████████▎ | 414M/501M [00:12<00:02, 37.1MB/s]Downloading:  84%|████████▎ | 418M/501M [00:12<00:02, 34.1MB/s]Downloading:  84%|████████▍ | 422M/501M [00:12<00:02, 35.7MB/s]Downloading:  85%|████████▌ | 426M/501M [00:12<00:02, 33.9MB/s]Downloading:  86%|████████▌ | 430M/501M [00:12<00:01, 35.8MB/s]Downloading:  87%|████████▋ | 434M/501M [00:13<00:01, 34.1MB/s]Downloading:  88%|████████▊ | 438M/501M [00:13<00:01, 35.8MB/s]Downloading:  88%|████████▊ | 442M/501M [00:13<00:01, 34.1MB/s]Downloading:  89%|████████▉ | 446M/501M [00:13<00:01, 35.8MB/s]Downloading:  90%|████████▉ | 450M/501M [00:13<00:01, 34.0MB/s]Downloading:  91%|█████████ | 454M/501M [00:13<00:01, 36.1MB/s]Downloading:  91%|█████████▏| 458M/501M [00:13<00:01, 33.5MB/s]Downloading:  92%|█████████▏| 462M/501M [00:13<00:01, 35.8MB/s]Downloading:  93%|█████████▎| 466M/501M [00:13<00:01, 33.6MB/s]Downloading:  94%|█████████▎| 469M/501M [00:14<00:00, 34.8MB/s]Downloading:  94%|█████████▍| 473M/501M [00:14<00:00, 27.9MB/s]Downloading:  95%|█████████▌| 478M/501M [00:14<00:00, 32.3MB/s]Downloading:  96%|█████████▌| 481M/501M [00:14<00:00, 30.6MB/s]Downloading:  97%|█████████▋| 484M/501M [00:14<00:00, 31.5MB/s]Downloading:  98%|█████████▊| 488M/501M [00:14<00:00, 32.0MB/s]Downloading:  98%|█████████▊| 493M/501M [00:14<00:00, 35.1MB/s]Downloading:  99%|█████████▉| 496M/501M [00:14<00:00, 32.9MB/s]Downloading: 100%|█████████▉| 500M/501M [00:15<00:00, 33.7MB/s]Downloading: 100%|██████████| 501M/501M [00:15<00:00, 33.3MB/s]
[INFO|file_utils.py:1635] 2021-08-04 13:48:21,020 >> storing https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/pytorch_model.bin in cache at /home/moisioa3/.cache/huggingface/transformers/276bf5f0d95b31fc0ed72ef6e2e1b771f2265351a4d322667fd8c73d8473d3fc.3d524bdc756dfbb2ba6c3c3a18e4e2afcc84034db29556b337605e9f8c39c2c2
[INFO|file_utils.py:1643] 2021-08-04 13:48:21,024 >> creating metadata file for /home/moisioa3/.cache/huggingface/transformers/276bf5f0d95b31fc0ed72ef6e2e1b771f2265351a4d322667fd8c73d8473d3fc.3d524bdc756dfbb2ba6c3c3a18e4e2afcc84034db29556b337605e9f8c39c2c2
[INFO|modeling_utils.py:1271] 2021-08-04 13:48:21,026 >> loading weights file https://huggingface.co/TurkuNLP/bert-base-finnish-cased-v1/resolve/main/pytorch_model.bin from cache at /home/moisioa3/.cache/huggingface/transformers/276bf5f0d95b31fc0ed72ef6e2e1b771f2265351a4d322667fd8c73d8473d3fc.3d524bdc756dfbb2ba6c3c3a18e4e2afcc84034db29556b337605e9f8c39c2c2
[WARNING|modeling_utils.py:1501] 2021-08-04 13:48:23,973 >> Some weights of the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1518] 2021-08-04 13:48:23,974 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Running tokenizer on every text in dataset:   0%|          | 0/8956 [00:00<?, ?ba/s]08/04/2021 13:48:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-6c3132bbeb5bcdb4.arrow
Running tokenizer on every text in dataset:   0%|          | 1/8956 [00:00<28:53,  5.17ba/s]Running tokenizer on every text in dataset:   0%|          | 3/8956 [00:00<15:33,  9.59ba/s]Running tokenizer on every text in dataset:   0%|          | 5/8956 [00:00<15:20,  9.72ba/s]Running tokenizer on every text in dataset:   0%|          | 7/8956 [00:00<12:51, 11.60ba/s]Running tokenizer on every text in dataset:   0%|          | 9/8956 [00:00<11:32, 12.91ba/s]Running tokenizer on every text in dataset:   0%|          | 11/8956 [00:00<10:53, 13.70ba/s]Running tokenizer on every text in dataset:   0%|          | 13/8956 [00:01<10:28, 14.23ba/s]Running tokenizer on every text in dataset:   0%|          | 15/8956 [00:01<11:18, 13.17ba/s]Running tokenizer on every text in dataset:   0%|          | 17/8956 [00:01<10:42, 13.92ba/s]Running tokenizer on every text in dataset:   0%|          | 19/8956 [00:01<10:11, 14.61ba/s]Running tokenizer on every text in dataset:   0%|          | 21/8956 [00:01<09:49, 15.16ba/s]Running tokenizer on every text in dataset:   0%|          | 24/8956 [00:01<10:13, 14.55ba/s]Running tokenizer on every text in dataset:   0%|          | 26/8956 [00:01<10:03, 14.80ba/s]Running tokenizer on every text in dataset:   0%|          | 28/8956 [00:02<09:40, 15.39ba/s]Running tokenizer on every text in dataset:   0%|          | 30/8956 [00:02<09:33, 15.57ba/s]Running tokenizer on every text in dataset:   0%|          | 32/8956 [00:02<09:41, 15.34ba/s]Running tokenizer on every text in dataset:   0%|          | 34/8956 [00:02<10:55, 13.62ba/s]Running tokenizer on every text in dataset:   0%|          | 36/8956 [00:02<10:20, 14.38ba/s]Running tokenizer on every text in dataset:   0%|          | 38/8956 [00:02<09:45, 15.22ba/s]Running tokenizer on every text in dataset:   0%|          | 40/8956 [00:02<09:19, 15.92ba/s]Running tokenizer on every text in dataset:   0%|          | 42/8956 [00:02<09:04, 16.38ba/s]Running tokenizer on every text in dataset:   0%|          | 44/8956 [00:03<10:42, 13.87ba/s]Running tokenizer on every text in dataset:   1%|          | 46/8956 [00:03<10:12, 14.54ba/s]Running tokenizer on every text in dataset:   1%|          | 48/8956 [00:03<09:46, 15.18ba/s]Running tokenizer on every text in dataset:   1%|          | 50/8956 [00:03<09:24, 15.78ba/s]Running tokenizer on every text in dataset:   1%|          | 52/8956 [00:03<10:23, 14.27ba/s]Running tokenizer on every text in dataset:   1%|          | 54/8956 [00:03<09:56, 14.93ba/s]Running tokenizer on every text in dataset:   1%|          | 56/8956 [00:03<09:27, 15.69ba/s]Running tokenizer on every text in dataset:   1%|          | 58/8956 [00:04<09:33, 15.50ba/s]Running tokenizer on every text in dataset:   1%|          | 60/8956 [00:04<09:25, 15.74ba/s]Running tokenizer on every text in dataset:   1%|          | 62/8956 [00:04<10:15, 14.45ba/s]Running tokenizer on every text in dataset:   1%|          | 64/8956 [00:04<09:49, 15.09ba/s]Running tokenizer on every text in dataset:   1%|          | 66/8956 [00:04<09:48, 15.10ba/s]Running tokenizer on every text in dataset:   1%|          | 68/8956 [00:04<09:54, 14.96ba/s]Running tokenizer on every text in dataset:   1%|          | 70/8956 [00:04<09:31, 15.56ba/s]Running tokenizer on every text in dataset:   1%|          | 72/8956 [00:05<10:48, 13.69ba/s]Running tokenizer on every text in dataset:   1%|          | 74/8956 [00:05<10:05, 14.66ba/s]Running tokenizer on every text in dataset:   1%|          | 76/8956 [00:05<09:45, 15.16ba/s][WARNING|tokenization_utils_base.py:3250] 2021-08-04 13:48:29,369 >> Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors
Running tokenizer on every text in dataset:   1%|          | 78/8956 [00:05<09:30, 15.55ba/s]Running tokenizer on every text in dataset:   1%|          | 80/8956 [00:05<09:29, 15.59ba/s]Running tokenizer on every text in dataset:   1%|          | 82/8956 [00:05<10:35, 13.97ba/s]Running tokenizer on every text in dataset:   1%|          | 84/8956 [00:05<10:19, 14.32ba/s]Running tokenizer on every text in dataset:   1%|          | 86/8956 [00:05<10:00, 14.76ba/s]Running tokenizer on every text in dataset:   1%|          | 88/8956 [00:06<09:49, 15.05ba/s]Running tokenizer on every text in dataset:   1%|          | 90/8956 [00:06<10:42, 13.80ba/s]Running tokenizer on every text in dataset:   1%|          | 92/8956 [00:06<10:16, 14.37ba/s]Running tokenizer on every text in dataset:   1%|          | 94/8956 [00:06<10:00, 14.77ba/s]Running tokenizer on every text in dataset:   1%|          | 96/8956 [00:06<09:56, 14.84ba/s]Running tokenizer on every text in dataset:   1%|          | 98/8956 [00:06<09:41, 15.24ba/s]Running tokenizer on every text in dataset:   1%|          | 100/8956 [00:06<11:02, 13.38ba/s]Running tokenizer on every text in dataset:   1%|          | 102/8956 [00:07<10:33, 13.97ba/s]Running tokenizer on every text in dataset:   1%|          | 104/8956 [00:07<10:07, 14.56ba/s]Running tokenizer on every text in dataset:   1%|          | 106/8956 [00:07<09:45, 15.11ba/s]Running tokenizer on every text in dataset:   1%|          | 108/8956 [00:07<09:05, 16.23ba/s]Running tokenizer on every text in dataset:   1%|          | 110/8956 [00:07<09:19, 15.80ba/s]Running tokenizer on every text in dataset:   1%|▏         | 113/8956 [00:07<08:09, 18.07ba/s]Running tokenizer on every text in dataset:   1%|▏         | 115/8956 [00:07<08:12, 17.95ba/s]Running tokenizer on every text in dataset:   1%|▏         | 117/8956 [00:07<08:34, 17.18ba/s]Running tokenizer on every text in dataset:   1%|▏         | 119/8956 [00:08<09:52, 14.91ba/s]Running tokenizer on every text in dataset:   1%|▏         | 121/8956 [00:08<09:50, 14.95ba/s]Running tokenizer on every text in dataset:   1%|▏         | 123/8956 [00:08<09:43, 15.13ba/s]Running tokenizer on every text in dataset:   1%|▏         | 125/8956 [00:08<09:34, 15.38ba/s]Running tokenizer on every text in dataset:   1%|▏         | 127/8956 [00:08<09:25, 15.61ba/s]Running tokenizer on every text in dataset:   1%|▏         | 129/8956 [00:08<10:14, 14.37ba/s]Running tokenizer on every text in dataset:   1%|▏         | 131/8956 [00:08<09:37, 15.28ba/s]Running tokenizer on every text in dataset:   1%|▏         | 134/8956 [00:09<08:23, 17.51ba/s]Running tokenizer on every text in dataset:   2%|▏         | 137/8956 [00:09<08:29, 17.29ba/s]Running tokenizer on every text in dataset:   2%|▏         | 140/8956 [00:09<07:57, 18.48ba/s]Running tokenizer on every text in dataset:   2%|▏         | 143/8956 [00:09<07:40, 19.12ba/s]Running tokenizer on every text in dataset:   2%|▏         | 146/8956 [00:09<07:26, 19.73ba/s]Running tokenizer on every text in dataset:   2%|▏         | 148/8956 [00:09<08:08, 18.03ba/s]Running tokenizer on every text in dataset:   2%|▏         | 151/8956 [00:09<07:48, 18.78ba/s]Running tokenizer on every text in dataset:   2%|▏         | 154/8956 [00:10<07:36, 19.29ba/s]Running tokenizer on every text in dataset:   2%|▏         | 156/8956 [00:10<08:20, 17.59ba/s]Running tokenizer on every text in dataset:   2%|▏         | 158/8956 [00:10<08:07, 18.05ba/s]Running tokenizer on every text in dataset:   2%|▏         | 161/8956 [00:10<07:44, 18.93ba/s]Running tokenizer on every text in dataset:   2%|▏         | 163/8956 [00:10<07:39, 19.12ba/s]Running tokenizer on every text in dataset:   2%|▏         | 165/8956 [00:10<07:34, 19.33ba/s]Running tokenizer on every text in dataset:   2%|▏         | 167/8956 [00:10<08:33, 17.11ba/s]Running tokenizer on every text in dataset:   2%|▏         | 169/8956 [00:10<08:20, 17.57ba/s]Running tokenizer on every text in dataset:   2%|▏         | 171/8956 [00:11<08:06, 18.07ba/s]Running tokenizer on every text in dataset:   2%|▏         | 174/8956 [00:11<07:42, 19.00ba/s]Running tokenizer on every text in dataset:   2%|▏         | 176/8956 [00:11<08:58, 16.32ba/s]Running tokenizer on every text in dataset:   2%|▏         | 179/8956 [00:11<08:19, 17.58ba/s]Running tokenizer on every text in dataset:   2%|▏         | 182/8956 [00:11<07:56, 18.40ba/s]Running tokenizer on every text in dataset:   2%|▏         | 185/8956 [00:11<08:23, 17.43ba/s]Running tokenizer on every text in dataset:   2%|▏         | 188/8956 [00:11<08:00, 18.24ba/s]Running tokenizer on every text in dataset:   2%|▏         | 191/8956 [00:12<07:46, 18.80ba/s]Running tokenizer on every text in dataset:   2%|▏         | 193/8956 [00:12<07:40, 19.02ba/s]Running tokenizer on every text in dataset:   2%|▏         | 195/8956 [00:12<08:33, 17.04ba/s]Running tokenizer on every text in dataset:   2%|▏         | 197/8956 [00:12<08:20, 17.50ba/s]Running tokenizer on every text in dataset:   2%|▏         | 199/8956 [00:12<08:16, 17.65ba/s]Running tokenizer on every text in dataset:   2%|▏         | 201/8956 [00:12<08:11, 17.81ba/s]Running tokenizer on every text in dataset:   2%|▏         | 203/8956 [00:12<08:00, 18.23ba/s]Running tokenizer on every text in dataset:   2%|▏         | 205/8956 [00:12<08:46, 16.62ba/s]Running tokenizer on every text in dataset:   2%|▏         | 207/8956 [00:13<08:49, 16.53ba/s]Running tokenizer on every text in dataset:   2%|▏         | 209/8956 [00:13<08:32, 17.07ba/s]Running tokenizer on every text in dataset:   2%|▏         | 211/8956 [00:13<08:15, 17.63ba/s]Running tokenizer on every text in dataset:   2%|▏         | 213/8956 [00:13<09:22, 15.54ba/s]Running tokenizer on every text in dataset:   2%|▏         | 215/8956 [00:13<09:06, 16.00ba/s]Running tokenizer on every text in dataset:   2%|▏         | 217/8956 [00:13<09:05, 16.03ba/s]Running tokenizer on every text in dataset:   2%|▏         | 219/8956 [00:13<09:05, 16.00ba/s]Running tokenizer on every text in dataset:   2%|▏         | 221/8956 [00:13<09:00, 16.18ba/s]Running tokenizer on every text in dataset:   2%|▏         | 223/8956 [00:14<09:45, 14.91ba/s]Running tokenizer on every text in dataset:   3%|▎         | 225/8956 [00:14<09:36, 15.15ba/s]Running tokenizer on every text in dataset:   3%|▎         | 227/8956 [00:14<09:19, 15.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 229/8956 [00:14<09:13, 15.76ba/s]Running tokenizer on every text in dataset:   3%|▎         | 231/8956 [00:14<09:02, 16.09ba/s]Running tokenizer on every text in dataset:   3%|▎         | 233/8956 [00:14<09:57, 14.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 235/8956 [00:14<09:15, 15.70ba/s]Running tokenizer on every text in dataset:   3%|▎         | 238/8956 [00:15<08:31, 17.06ba/s]Running tokenizer on every text in dataset:   3%|▎         | 240/8956 [00:15<08:19, 17.44ba/s]Running tokenizer on every text in dataset:   3%|▎         | 242/8956 [00:15<09:02, 16.06ba/s]Running tokenizer on every text in dataset:   3%|▎         | 244/8956 [00:15<08:37, 16.83ba/s]Running tokenizer on every text in dataset:   3%|▎         | 246/8956 [00:15<08:28, 17.11ba/s]Running tokenizer on every text in dataset:   3%|▎         | 248/8956 [00:15<08:11, 17.70ba/s]Running tokenizer on every text in dataset:   3%|▎         | 250/8956 [00:15<08:02, 18.03ba/s]Running tokenizer on every text in dataset:   3%|▎         | 252/8956 [00:15<09:12, 15.76ba/s]Running tokenizer on every text in dataset:   3%|▎         | 255/8956 [00:16<08:35, 16.89ba/s]Running tokenizer on every text in dataset:   3%|▎         | 257/8956 [00:16<08:31, 17.02ba/s]Running tokenizer on every text in dataset:   3%|▎         | 259/8956 [00:16<08:28, 17.12ba/s]Running tokenizer on every text in dataset:   3%|▎         | 261/8956 [00:16<09:17, 15.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 263/8956 [00:16<08:47, 16.47ba/s]Running tokenizer on every text in dataset:   3%|▎         | 266/8956 [00:16<08:08, 17.78ba/s]Running tokenizer on every text in dataset:   3%|▎         | 268/8956 [00:16<07:55, 18.27ba/s]Running tokenizer on every text in dataset:   3%|▎         | 270/8956 [00:16<08:52, 16.32ba/s]Running tokenizer on every text in dataset:   3%|▎         | 272/8956 [00:17<08:35, 16.86ba/s]Running tokenizer on every text in dataset:   3%|▎         | 274/8956 [00:17<08:28, 17.07ba/s]Running tokenizer on every text in dataset:   3%|▎         | 276/8956 [00:17<08:20, 17.33ba/s]Running tokenizer on every text in dataset:   3%|▎         | 278/8956 [00:17<08:23, 17.22ba/s]Running tokenizer on every text in dataset:   3%|▎         | 280/8956 [00:17<09:21, 15.44ba/s]Running tokenizer on every text in dataset:   3%|▎         | 283/8956 [00:17<08:14, 17.52ba/s]Running tokenizer on every text in dataset:   3%|▎         | 285/8956 [00:17<08:19, 17.36ba/s]Running tokenizer on every text in dataset:   3%|▎         | 287/8956 [00:17<08:07, 17.80ba/s]Running tokenizer on every text in dataset:   3%|▎         | 289/8956 [00:18<09:23, 15.38ba/s]Running tokenizer on every text in dataset:   3%|▎         | 291/8956 [00:18<08:53, 16.25ba/s]Running tokenizer on every text in dataset:   3%|▎         | 293/8956 [00:18<08:34, 16.83ba/s]Running tokenizer on every text in dataset:   3%|▎         | 295/8956 [00:18<08:11, 17.64ba/s]Running tokenizer on every text in dataset:   3%|▎         | 298/8956 [00:18<07:47, 18.52ba/s]Running tokenizer on every text in dataset:   3%|▎         | 300/8956 [00:18<08:44, 16.52ba/s]Running tokenizer on every text in dataset:   3%|▎         | 302/8956 [00:18<08:26, 17.09ba/s]Running tokenizer on every text in dataset:   3%|▎         | 304/8956 [00:18<08:18, 17.35ba/s]Running tokenizer on every text in dataset:   3%|▎         | 307/8956 [00:19<07:57, 18.13ba/s]Running tokenizer on every text in dataset:   3%|▎         | 309/8956 [00:19<08:49, 16.34ba/s]Running tokenizer on every text in dataset:   3%|▎         | 312/8956 [00:19<08:18, 17.36ba/s]Running tokenizer on every text in dataset:   4%|▎         | 314/8956 [00:19<08:07, 17.72ba/s]Running tokenizer on every text in dataset:   4%|▎         | 316/8956 [00:19<08:05, 17.81ba/s]Running tokenizer on every text in dataset:   4%|▎         | 318/8956 [00:19<08:56, 16.10ba/s]Running tokenizer on every text in dataset:   4%|▎         | 320/8956 [00:19<08:32, 16.85ba/s]Running tokenizer on every text in dataset:   4%|▎         | 322/8956 [00:19<08:11, 17.57ba/s]Running tokenizer on every text in dataset:   4%|▎         | 324/8956 [00:20<08:13, 17.48ba/s]Running tokenizer on every text in dataset:   4%|▎         | 326/8956 [00:20<08:06, 17.74ba/s]Running tokenizer on every text in dataset:   4%|▎         | 328/8956 [00:20<08:58, 16.02ba/s]Running tokenizer on every text in dataset:   4%|▎         | 330/8956 [00:20<08:47, 16.35ba/s]Running tokenizer on every text in dataset:   4%|▎         | 332/8956 [00:20<08:27, 16.99ba/s]Running tokenizer on every text in dataset:   4%|▎         | 334/8956 [00:20<08:16, 17.36ba/s]Running tokenizer on every text in dataset:   4%|▍         | 336/8956 [00:20<07:59, 17.97ba/s]Running tokenizer on every text in dataset:   4%|▍         | 338/8956 [00:20<08:58, 16.01ba/s]Running tokenizer on every text in dataset:   4%|▍         | 340/8956 [00:21<08:33, 16.78ba/s]Running tokenizer on every text in dataset:   4%|▍         | 342/8956 [00:21<08:21, 17.19ba/s]Running tokenizer on every text in dataset:   4%|▍         | 344/8956 [00:21<08:09, 17.59ba/s]Running tokenizer on every text in dataset:   4%|▍         | 346/8956 [00:21<09:16, 15.47ba/s]Running tokenizer on every text in dataset:   4%|▍         | 348/8956 [00:21<08:47, 16.30ba/s]Running tokenizer on every text in dataset:   4%|▍         | 350/8956 [00:21<08:25, 17.03ba/s]Running tokenizer on every text in dataset:   4%|▍         | 352/8956 [00:21<08:13, 17.45ba/s]Running tokenizer on every text in dataset:   4%|▍         | 354/8956 [00:21<08:15, 17.35ba/s]Running tokenizer on every text in dataset:   4%|▍         | 356/8956 [00:21<09:03, 15.81ba/s]Running tokenizer on every text in dataset:   4%|▍         | 358/8956 [00:22<08:52, 16.16ba/s]Running tokenizer on every text in dataset:   4%|▍         | 360/8956 [00:22<08:25, 17.00ba/s]Running tokenizer on every text in dataset:   4%|▍         | 362/8956 [00:22<08:09, 17.57ba/s]Running tokenizer on every text in dataset:   4%|▍         | 364/8956 [00:22<07:55, 18.09ba/s]Running tokenizer on every text in dataset:   4%|▍         | 366/8956 [00:22<08:47, 16.27ba/s]Running tokenizer on every text in dataset:   4%|▍         | 368/8956 [00:22<08:24, 17.01ba/s]Running tokenizer on every text in dataset:   4%|▍         | 370/8956 [00:22<08:05, 17.68ba/s]Running tokenizer on every text in dataset:   4%|▍         | 372/8956 [00:22<07:59, 17.90ba/s]Running tokenizer on every text in dataset:   4%|▍         | 374/8956 [00:22<07:49, 18.29ba/s]Running tokenizer on every text in dataset:   4%|▍         | 376/8956 [00:23<08:45, 16.32ba/s]Running tokenizer on every text in dataset:   4%|▍         | 378/8956 [00:23<08:28, 16.88ba/s]Running tokenizer on every text in dataset:   4%|▍         | 380/8956 [00:23<08:15, 17.30ba/s]Running tokenizer on every text in dataset:   4%|▍         | 383/8956 [00:23<07:47, 18.35ba/s]Running tokenizer on every text in dataset:   4%|▍         | 385/8956 [00:23<08:36, 16.60ba/s]Running tokenizer on every text in dataset:   4%|▍         | 387/8956 [00:23<08:14, 17.31ba/s]Running tokenizer on every text in dataset:   4%|▍         | 390/8956 [00:23<07:48, 18.28ba/s]Running tokenizer on every text in dataset:   4%|▍         | 392/8956 [00:24<07:48, 18.27ba/s]Running tokenizer on every text in dataset:   4%|▍         | 394/8956 [00:24<08:50, 16.14ba/s]Running tokenizer on every text in dataset:   4%|▍         | 397/8956 [00:24<08:08, 17.53ba/s]Running tokenizer on every text in dataset:   4%|▍         | 400/8956 [00:24<07:47, 18.30ba/s]Running tokenizer on every text in dataset:   4%|▍         | 402/8956 [00:24<07:50, 18.18ba/s]Running tokenizer on every text in dataset:   5%|▍         | 404/8956 [00:24<08:50, 16.13ba/s]Running tokenizer on every text in dataset:   5%|▍         | 407/8956 [00:24<08:12, 17.37ba/s]Running tokenizer on every text in dataset:   5%|▍         | 409/8956 [00:25<08:11, 17.40ba/s]Running tokenizer on every text in dataset:   5%|▍         | 411/8956 [00:25<08:05, 17.59ba/s]Running tokenizer on every text in dataset:   5%|▍         | 413/8956 [00:25<08:56, 15.94ba/s]Running tokenizer on every text in dataset:   5%|▍         | 416/8956 [00:25<08:09, 17.46ba/s]Running tokenizer on every text in dataset:   5%|▍         | 418/8956 [00:25<08:02, 17.69ba/s]Running tokenizer on every text in dataset:   5%|▍         | 420/8956 [00:25<08:03, 17.67ba/s]Running tokenizer on every text in dataset:   5%|▍         | 422/8956 [00:25<08:58, 15.84ba/s]Running tokenizer on every text in dataset:   5%|▍         | 425/8956 [00:25<07:56, 17.90ba/s]Running tokenizer on every text in dataset:   5%|▍         | 429/8956 [00:26<06:21, 22.38ba/s]Running tokenizer on every text in dataset:   5%|▍         | 432/8956 [00:26<06:23, 22.21ba/s]Running tokenizer on every text in dataset:   5%|▍         | 436/8956 [00:26<05:33, 25.55ba/s]Running tokenizer on every text in dataset:   5%|▍         | 440/8956 [00:26<04:59, 28.44ba/s]Running tokenizer on every text in dataset:   5%|▍         | 443/8956 [00:26<05:48, 24.42ba/s]Running tokenizer on every text in dataset:   5%|▍         | 446/8956 [00:26<06:23, 22.21ba/s]Running tokenizer on every text in dataset:   5%|▌         | 449/8956 [00:26<06:51, 20.69ba/s]Running tokenizer on every text in dataset:   5%|▌         | 452/8956 [00:27<07:38, 18.53ba/s]Running tokenizer on every text in dataset:   5%|▌         | 454/8956 [00:27<07:31, 18.82ba/s]Running tokenizer on every text in dataset:   5%|▌         | 456/8956 [00:27<07:28, 18.97ba/s]Running tokenizer on every text in dataset:   5%|▌         | 458/8956 [00:27<07:34, 18.70ba/s]Running tokenizer on every text in dataset:   5%|▌         | 460/8956 [00:27<08:25, 16.82ba/s]Running tokenizer on every text in dataset:   5%|▌         | 463/8956 [00:27<07:46, 18.22ba/s]Running tokenizer on every text in dataset:   5%|▌         | 465/8956 [00:27<07:50, 18.04ba/s]Running tokenizer on every text in dataset:   5%|▌         | 467/8956 [00:27<07:43, 18.30ba/s]Running tokenizer on every text in dataset:   5%|▌         | 469/8956 [00:28<07:37, 18.57ba/s]Running tokenizer on every text in dataset:   5%|▌         | 471/8956 [00:28<08:26, 16.75ba/s]Running tokenizer on every text in dataset:   5%|▌         | 473/8956 [00:28<08:09, 17.31ba/s]Running tokenizer on every text in dataset:   5%|▌         | 475/8956 [00:28<08:09, 17.32ba/s]Running tokenizer on every text in dataset:   5%|▌         | 477/8956 [00:28<08:01, 17.61ba/s]Running tokenizer on every text in dataset:   5%|▌         | 479/8956 [00:28<08:59, 15.73ba/s]Running tokenizer on every text in dataset:   5%|▌         | 482/8956 [00:28<08:25, 16.75ba/s]Running tokenizer on every text in dataset:   5%|▌         | 484/8956 [00:28<08:24, 16.80ba/s]Running tokenizer on every text in dataset:   5%|▌         | 486/8956 [00:29<08:11, 17.24ba/s]Running tokenizer on every text in dataset:   5%|▌         | 488/8956 [00:29<08:08, 17.34ba/s]Running tokenizer on every text in dataset:   5%|▌         | 490/8956 [00:29<09:13, 15.31ba/s]Running tokenizer on every text in dataset:   6%|▌         | 493/8956 [00:29<08:20, 16.92ba/s]Running tokenizer on every text in dataset:   6%|▌         | 496/8956 [00:29<07:56, 17.76ba/s]Running tokenizer on every text in dataset:   6%|▌         | 498/8956 [00:29<08:57, 15.74ba/s]Running tokenizer on every text in dataset:   6%|▌         | 500/8956 [00:29<08:44, 16.11ba/s]Running tokenizer on every text in dataset:   6%|▌         | 502/8956 [00:30<08:45, 16.09ba/s]Running tokenizer on every text in dataset:   6%|▌         | 504/8956 [00:30<08:29, 16.60ba/s]Running tokenizer on every text in dataset:   6%|▌         | 506/8956 [00:30<08:05, 17.40ba/s]Running tokenizer on every text in dataset:   6%|▌         | 508/8956 [00:30<08:43, 16.15ba/s]Running tokenizer on every text in dataset:   6%|▌         | 511/8956 [00:30<08:05, 17.41ba/s]Running tokenizer on every text in dataset:   6%|▌         | 513/8956 [00:30<07:54, 17.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 516/8956 [00:30<07:34, 18.58ba/s]Running tokenizer on every text in dataset:   6%|▌         | 518/8956 [00:31<08:22, 16.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 521/8956 [00:31<07:44, 18.16ba/s]Running tokenizer on every text in dataset:   6%|▌         | 523/8956 [00:31<07:47, 18.04ba/s]Running tokenizer on every text in dataset:   6%|▌         | 525/8956 [00:31<07:41, 18.28ba/s]Running tokenizer on every text in dataset:   6%|▌         | 527/8956 [00:31<08:56, 15.70ba/s]Running tokenizer on every text in dataset:   6%|▌         | 530/8956 [00:31<07:57, 17.65ba/s]Running tokenizer on every text in dataset:   6%|▌         | 532/8956 [00:31<07:46, 18.07ba/s]Running tokenizer on every text in dataset:   6%|▌         | 534/8956 [00:31<07:58, 17.61ba/s]Running tokenizer on every text in dataset:   6%|▌         | 536/8956 [00:32<09:02, 15.52ba/s]Running tokenizer on every text in dataset:   6%|▌         | 538/8956 [00:32<09:02, 15.51ba/s]Running tokenizer on every text in dataset:   6%|▌         | 540/8956 [00:32<08:52, 15.80ba/s]Running tokenizer on every text in dataset:   6%|▌         | 542/8956 [00:32<08:35, 16.32ba/s]Running tokenizer on every text in dataset:   6%|▌         | 544/8956 [00:32<08:14, 17.00ba/s]Running tokenizer on every text in dataset:   6%|▌         | 546/8956 [00:32<08:51, 15.81ba/s]Running tokenizer on every text in dataset:   6%|▌         | 548/8956 [00:32<08:20, 16.79ba/s]Running tokenizer on every text in dataset:   6%|▌         | 550/8956 [00:32<08:06, 17.30ba/s]Running tokenizer on every text in dataset:   6%|▌         | 552/8956 [00:33<08:17, 16.90ba/s]Running tokenizer on every text in dataset:   6%|▌         | 554/8956 [00:33<08:19, 16.83ba/s]Running tokenizer on every text in dataset:   6%|▌         | 556/8956 [00:33<09:19, 15.01ba/s]Running tokenizer on every text in dataset:   6%|▌         | 558/8956 [00:33<08:54, 15.72ba/s]Running tokenizer on every text in dataset:   6%|▋         | 560/8956 [00:33<08:25, 16.60ba/s]Running tokenizer on every text in dataset:   6%|▋         | 562/8956 [00:33<08:29, 16.46ba/s]Running tokenizer on every text in dataset:   6%|▋         | 564/8956 [00:33<08:18, 16.82ba/s]Running tokenizer on every text in dataset:   6%|▋         | 566/8956 [00:33<09:11, 15.22ba/s]Running tokenizer on every text in dataset:   6%|▋         | 568/8956 [00:34<08:41, 16.08ba/s]Running tokenizer on every text in dataset:   6%|▋         | 570/8956 [00:34<08:26, 16.56ba/s]Running tokenizer on every text in dataset:   6%|▋         | 572/8956 [00:34<08:19, 16.79ba/s]Running tokenizer on every text in dataset:   6%|▋         | 574/8956 [00:34<09:21, 14.93ba/s]Running tokenizer on every text in dataset:   6%|▋         | 577/8956 [00:34<08:27, 16.52ba/s]Running tokenizer on every text in dataset:   6%|▋         | 579/8956 [00:34<08:11, 17.05ba/s]Running tokenizer on every text in dataset:   6%|▋         | 582/8956 [00:34<07:48, 17.89ba/s]Running tokenizer on every text in dataset:   7%|▋         | 584/8956 [00:34<08:38, 16.15ba/s]Running tokenizer on every text in dataset:   7%|▋         | 586/8956 [00:35<08:14, 16.94ba/s]Running tokenizer on every text in dataset:   7%|▋         | 588/8956 [00:35<07:55, 17.59ba/s]Running tokenizer on every text in dataset:   7%|▋         | 590/8956 [00:35<07:55, 17.59ba/s]Running tokenizer on every text in dataset:   7%|▋         | 592/8956 [00:35<07:44, 18.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 594/8956 [00:35<08:42, 16.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 596/8956 [00:35<08:24, 16.56ba/s]Running tokenizer on every text in dataset:   7%|▋         | 598/8956 [00:35<08:12, 16.96ba/s]Running tokenizer on every text in dataset:   7%|▋         | 600/8956 [00:35<08:00, 17.38ba/s]Running tokenizer on every text in dataset:   7%|▋         | 602/8956 [00:36<07:50, 17.77ba/s]Running tokenizer on every text in dataset:   7%|▋         | 604/8956 [00:36<08:40, 16.04ba/s]Running tokenizer on every text in dataset:   7%|▋         | 606/8956 [00:36<08:20, 16.68ba/s]Running tokenizer on every text in dataset:   7%|▋         | 608/8956 [00:36<08:03, 17.28ba/s]Running tokenizer on every text in dataset:   7%|▋         | 610/8956 [00:36<08:08, 17.08ba/s]Running tokenizer on every text in dataset:   7%|▋         | 612/8956 [00:36<09:13, 15.08ba/s]Running tokenizer on every text in dataset:   7%|▋         | 614/8956 [00:36<08:57, 15.52ba/s]Running tokenizer on every text in dataset:   7%|▋         | 616/8956 [00:36<08:34, 16.20ba/s]Running tokenizer on every text in dataset:   7%|▋         | 618/8956 [00:37<08:18, 16.72ba/s]Running tokenizer on every text in dataset:   7%|▋         | 620/8956 [00:37<08:11, 16.97ba/s]Running tokenizer on every text in dataset:   7%|▋         | 622/8956 [00:37<09:09, 15.17ba/s]Running tokenizer on every text in dataset:   7%|▋         | 624/8956 [00:37<08:40, 16.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 626/8956 [00:37<08:27, 16.41ba/s]Running tokenizer on every text in dataset:   7%|▋         | 628/8956 [00:37<08:19, 16.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 630/8956 [00:37<08:02, 17.26ba/s]Running tokenizer on every text in dataset:   7%|▋         | 632/8956 [00:37<08:51, 15.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 634/8956 [00:38<08:38, 16.06ba/s]Running tokenizer on every text in dataset:   7%|▋         | 636/8956 [00:38<08:24, 16.48ba/s]Running tokenizer on every text in dataset:   7%|▋         | 638/8956 [00:38<08:00, 17.32ba/s]Running tokenizer on every text in dataset:   7%|▋         | 640/8956 [00:38<07:47, 17.78ba/s]Running tokenizer on every text in dataset:   7%|▋         | 642/8956 [00:38<08:49, 15.71ba/s]Running tokenizer on every text in dataset:   7%|▋         | 644/8956 [00:38<08:33, 16.19ba/s]Running tokenizer on every text in dataset:   7%|▋         | 646/8956 [00:38<08:22, 16.53ba/s]Running tokenizer on every text in dataset:   7%|▋         | 648/8956 [00:38<08:08, 17.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 650/8956 [00:38<09:01, 15.33ba/s]Running tokenizer on every text in dataset:   7%|▋         | 652/8956 [00:39<08:29, 16.30ba/s]Running tokenizer on every text in dataset:   7%|▋         | 654/8956 [00:39<08:02, 17.21ba/s]Running tokenizer on every text in dataset:   7%|▋         | 656/8956 [00:39<07:50, 17.65ba/s]Running tokenizer on every text in dataset:   7%|▋         | 658/8956 [00:39<07:40, 18.00ba/s]Running tokenizer on every text in dataset:   7%|▋         | 660/8956 [00:39<08:27, 16.35ba/s]Running tokenizer on every text in dataset:   7%|▋         | 663/8956 [00:39<07:48, 17.71ba/s]Running tokenizer on every text in dataset:   7%|▋         | 666/8956 [00:39<07:27, 18.52ba/s]Running tokenizer on every text in dataset:   7%|▋         | 668/8956 [00:39<07:27, 18.51ba/s]Running tokenizer on every text in dataset:   7%|▋         | 670/8956 [00:40<08:24, 16.43ba/s]Running tokenizer on every text in dataset:   8%|▊         | 672/8956 [00:40<08:06, 17.04ba/s]Running tokenizer on every text in dataset:   8%|▊         | 674/8956 [00:40<07:51, 17.58ba/s]Running tokenizer on every text in dataset:   8%|▊         | 676/8956 [00:40<07:35, 18.19ba/s]Running tokenizer on every text in dataset:   8%|▊         | 678/8956 [00:40<07:24, 18.61ba/s]Running tokenizer on every text in dataset:   8%|▊         | 680/8956 [00:40<08:19, 16.56ba/s]Running tokenizer on every text in dataset:   8%|▊         | 683/8956 [00:40<07:44, 17.82ba/s]Running tokenizer on every text in dataset:   8%|▊         | 686/8956 [00:40<07:24, 18.61ba/s]slurmstepd: error: *** JOB 62045199 ON gpu27 CANCELLED AT 2021-08-04T13:49:05 ***
Running tokenizer on every text in dataset:   8%|▊         | 688/8956 [00:41<08:41, 15.86ba/s]Job has already finished for job 62045199
