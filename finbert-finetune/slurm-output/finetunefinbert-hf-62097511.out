
Currently Loaded Modules:
  1) CUDA/9.0.176   2) cuDNN/7-CUDA-9.0.176   3) miniconda/4.9.2

 

08/09/2021 09:43:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/09/2021 09:43:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
greater_is_better=None,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp-2/runs/Aug09_09-43-18_dgx5.int.triton.aalto.fi,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
output_dir=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp-2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=bert-base-finnish-cased-v1-finetuned-web-dsp-2,
push_to_hub_organization=None,
push_to_hub_token=None,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=/scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp-2,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/09/2021 09:43:19 - WARNING - datasets.builder - Using custom data configuration default-65a6a01de02db827
08/09/2021 09:43:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/09/2021 09:43:19 - INFO - datasets.info - Loading Dataset info from /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5
08/09/2021 09:43:19 - WARNING - datasets.builder - Reusing dataset text (/home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)
08/09/2021 09:43:19 - INFO - datasets.info - Loading Dataset info from /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 67.02it/s]
[INFO|configuration_utils.py:543] 2021-08-09 09:43:19,519 >> loading configuration file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/config.json
[INFO|configuration_utils.py:581] 2021-08-09 09:43:19,520 >> Model config BertConfig {
  "_name_or_path": "TurkuNLP/bert-base-finnish-cased-v1",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.10.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 50105
}

[INFO|tokenization_utils_base.py:1664] 2021-08-09 09:43:19,534 >> Didn't find file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1728] 2021-08-09 09:43:19,535 >> loading file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/vocab.txt
[INFO|tokenization_utils_base.py:1728] 2021-08-09 09:43:19,535 >> loading file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/tokenizer.json
[INFO|tokenization_utils_base.py:1728] 2021-08-09 09:43:19,535 >> loading file None
[INFO|tokenization_utils_base.py:1728] 2021-08-09 09:43:19,536 >> loading file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/special_tokens_map.json
[INFO|tokenization_utils_base.py:1728] 2021-08-09 09:43:19,536 >> loading file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/tokenizer_config.json
[INFO|modeling_utils.py:1269] 2021-08-09 09:43:19,728 >> loading weights file /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500/pytorch_model.bin
[INFO|modeling_utils.py:1510] 2021-08-09 09:43:23,579 >> All model checkpoint weights were used when initializing BertForMaskedLM.

[INFO|modeling_utils.py:1518] 2021-08-09 09:43:23,580 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /scratch/work/moisioa3/conv_lm/finbert-finetune/TurkuNLP/bert-base-finnish-cased-v1-finetuned-web-dsp/checkpoint-20500.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.
Running tokenizer on every text in dataset:   0%|          | 0/8956 [00:00<?, ?ba/s]08/09/2021 09:43:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/moisioa3/.cache/huggingface/datasets/text/default-65a6a01de02db827/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-1da155afb4453d53.arrow
Running tokenizer on every text in dataset:   0%|          | 1/8956 [00:00<1:14:41,  2.00ba/s]Running tokenizer on every text in dataset:   0%|          | 3/8956 [00:00<28:19,  5.27ba/s]  Running tokenizer on every text in dataset:   0%|          | 5/8956 [00:00<20:53,  7.14ba/s]Running tokenizer on every text in dataset:   0%|          | 7/8956 [00:00<16:31,  9.03ba/s]Running tokenizer on every text in dataset:   0%|          | 9/8956 [00:01<13:50, 10.77ba/s]Running tokenizer on every text in dataset:   0%|          | 11/8956 [00:01<12:30, 11.92ba/s]Running tokenizer on every text in dataset:   0%|          | 13/8956 [00:01<11:28, 12.98ba/s]Running tokenizer on every text in dataset:   0%|          | 15/8956 [00:01<11:08, 13.38ba/s]Running tokenizer on every text in dataset:   0%|          | 17/8956 [00:01<10:19, 14.42ba/s]Running tokenizer on every text in dataset:   0%|          | 19/8956 [00:01<09:54, 15.03ba/s]Running tokenizer on every text in dataset:   0%|          | 21/8956 [00:01<09:19, 15.98ba/s]Running tokenizer on every text in dataset:   0%|          | 24/8956 [00:02<08:57, 16.63ba/s]Running tokenizer on every text in dataset:   0%|          | 26/8956 [00:02<08:53, 16.74ba/s]Running tokenizer on every text in dataset:   0%|          | 28/8956 [00:02<08:37, 17.25ba/s]Running tokenizer on every text in dataset:   0%|          | 30/8956 [00:02<08:32, 17.42ba/s]Running tokenizer on every text in dataset:   0%|          | 32/8956 [00:02<08:46, 16.95ba/s]Running tokenizer on every text in dataset:   0%|          | 34/8956 [00:02<09:28, 15.70ba/s]Running tokenizer on every text in dataset:   0%|          | 36/8956 [00:02<09:05, 16.35ba/s]Running tokenizer on every text in dataset:   0%|          | 38/8956 [00:02<08:41, 17.09ba/s]Running tokenizer on every text in dataset:   0%|          | 40/8956 [00:02<08:33, 17.37ba/s]Running tokenizer on every text in dataset:   0%|          | 42/8956 [00:03<08:19, 17.85ba/s]Running tokenizer on every text in dataset:   0%|          | 44/8956 [00:03<08:58, 16.54ba/s]Running tokenizer on every text in dataset:   1%|          | 46/8956 [00:03<08:44, 16.99ba/s]Running tokenizer on every text in dataset:   1%|          | 48/8956 [00:03<08:45, 16.96ba/s]Running tokenizer on every text in dataset:   1%|          | 50/8956 [00:03<08:27, 17.54ba/s]Running tokenizer on every text in dataset:   1%|          | 52/8956 [00:03<08:54, 16.65ba/s]Running tokenizer on every text in dataset:   1%|          | 54/8956 [00:03<09:04, 16.34ba/s]Running tokenizer on every text in dataset:   1%|          | 56/8956 [00:03<08:36, 17.22ba/s]Running tokenizer on every text in dataset:   1%|          | 58/8956 [00:04<08:52, 16.72ba/s]Running tokenizer on every text in dataset:   1%|          | 60/8956 [00:04<08:42, 17.03ba/s]Running tokenizer on every text in dataset:   1%|          | 62/8956 [00:04<09:39, 15.35ba/s]Running tokenizer on every text in dataset:   1%|          | 64/8956 [00:04<09:08, 16.20ba/s]Running tokenizer on every text in dataset:   1%|          | 66/8956 [00:04<08:59, 16.48ba/s]Running tokenizer on every text in dataset:   1%|          | 68/8956 [00:04<09:08, 16.21ba/s]Running tokenizer on every text in dataset:   1%|          | 70/8956 [00:04<08:48, 16.82ba/s]Running tokenizer on every text in dataset:   1%|          | 72/8956 [00:04<09:07, 16.24ba/s]Running tokenizer on every text in dataset:   1%|          | 74/8956 [00:05<08:45, 16.92ba/s]Running tokenizer on every text in dataset:   1%|          | 76/8956 [00:05<08:45, 16.91ba/s][WARNING|tokenization_utils_base.py:3250] 2021-08-09 09:43:28,827 >> Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors
Running tokenizer on every text in dataset:   1%|          | 78/8956 [00:05<08:36, 17.19ba/s]Running tokenizer on every text in dataset:   1%|          | 80/8956 [00:05<08:34, 17.26ba/s]Running tokenizer on every text in dataset:   1%|          | 82/8956 [00:05<09:16, 15.94ba/s]Running tokenizer on every text in dataset:   1%|          | 84/8956 [00:05<09:12, 16.06ba/s]Running tokenizer on every text in dataset:   1%|          | 86/8956 [00:05<09:12, 16.04ba/s]Running tokenizer on every text in dataset:   1%|          | 88/8956 [00:05<09:10, 16.12ba/s]Running tokenizer on every text in dataset:   1%|          | 90/8956 [00:06<10:00, 14.75ba/s]Running tokenizer on every text in dataset:   1%|          | 92/8956 [00:06<09:39, 15.30ba/s]Running tokenizer on every text in dataset:   1%|          | 94/8956 [00:06<09:28, 15.59ba/s]Running tokenizer on every text in dataset:   1%|          | 96/8956 [00:06<09:22, 15.74ba/s]Running tokenizer on every text in dataset:   1%|          | 98/8956 [00:06<09:09, 16.12ba/s]Running tokenizer on every text in dataset:   1%|          | 100/8956 [00:06<09:28, 15.57ba/s]Running tokenizer on every text in dataset:   1%|          | 102/8956 [00:06<09:26, 15.64ba/s]Running tokenizer on every text in dataset:   1%|          | 104/8956 [00:06<09:08, 16.15ba/s]Running tokenizer on every text in dataset:   1%|          | 106/8956 [00:07<08:53, 16.58ba/s]Running tokenizer on every text in dataset:   1%|          | 108/8956 [00:07<08:51, 16.66ba/s]Running tokenizer on every text in dataset:   1%|          | 110/8956 [00:07<08:30, 17.34ba/s]Running tokenizer on every text in dataset:   1%|▏         | 113/8956 [00:07<07:27, 19.75ba/s]Running tokenizer on every text in dataset:   1%|▏         | 115/8956 [00:07<07:33, 19.49ba/s]Running tokenizer on every text in dataset:   1%|▏         | 117/8956 [00:07<08:05, 18.22ba/s]Running tokenizer on every text in dataset:   1%|▏         | 119/8956 [00:07<08:45, 16.82ba/s]Running tokenizer on every text in dataset:   1%|▏         | 121/8956 [00:07<08:52, 16.60ba/s]Running tokenizer on every text in dataset:   1%|▏         | 123/8956 [00:07<08:48, 16.72ba/s]Running tokenizer on every text in dataset:   1%|▏         | 125/8956 [00:08<08:41, 16.93ba/s]Running tokenizer on every text in dataset:   1%|▏         | 127/8956 [00:08<08:36, 17.09ba/s]Running tokenizer on every text in dataset:   1%|▏         | 129/8956 [00:08<09:00, 16.34ba/s]Running tokenizer on every text in dataset:   1%|▏         | 131/8956 [00:08<08:50, 16.64ba/s]Running tokenizer on every text in dataset:   1%|▏         | 134/8956 [00:08<07:44, 19.01ba/s]Running tokenizer on every text in dataset:   2%|▏         | 137/8956 [00:08<07:07, 20.61ba/s]Running tokenizer on every text in dataset:   2%|▏         | 140/8956 [00:08<07:17, 20.15ba/s]Running tokenizer on every text in dataset:   2%|▏         | 143/8956 [00:08<07:03, 20.82ba/s]Running tokenizer on every text in dataset:   2%|▏         | 146/8956 [00:09<06:47, 21.61ba/s]Running tokenizer on every text in dataset:   2%|▏         | 149/8956 [00:09<07:03, 20.79ba/s]Running tokenizer on every text in dataset:   2%|▏         | 152/8956 [00:09<06:52, 21.35ba/s]Running tokenizer on every text in dataset:   2%|▏         | 155/8956 [00:09<06:43, 21.81ba/s]Running tokenizer on every text in dataset:   2%|▏         | 158/8956 [00:09<07:28, 19.61ba/s]Running tokenizer on every text in dataset:   2%|▏         | 161/8956 [00:09<07:10, 20.45ba/s]Running tokenizer on every text in dataset:   2%|▏         | 164/8956 [00:09<07:01, 20.87ba/s]Running tokenizer on every text in dataset:   2%|▏         | 167/8956 [00:10<07:19, 20.00ba/s]Running tokenizer on every text in dataset:   2%|▏         | 170/8956 [00:10<07:08, 20.49ba/s]Running tokenizer on every text in dataset:   2%|▏         | 173/8956 [00:10<06:55, 21.12ba/s]Running tokenizer on every text in dataset:   2%|▏         | 176/8956 [00:10<07:09, 20.44ba/s]Running tokenizer on every text in dataset:   2%|▏         | 179/8956 [00:10<07:02, 20.76ba/s]Running tokenizer on every text in dataset:   2%|▏         | 182/8956 [00:10<06:57, 21.03ba/s]Running tokenizer on every text in dataset:   2%|▏         | 185/8956 [00:11<07:09, 20.44ba/s]Running tokenizer on every text in dataset:   2%|▏         | 188/8956 [00:11<07:02, 20.74ba/s]Running tokenizer on every text in dataset:   2%|▏         | 191/8956 [00:11<06:55, 21.10ba/s]Running tokenizer on every text in dataset:   2%|▏         | 194/8956 [00:11<06:55, 21.10ba/s]Running tokenizer on every text in dataset:   2%|▏         | 197/8956 [00:11<07:18, 19.97ba/s]Running tokenizer on every text in dataset:   2%|▏         | 200/8956 [00:11<07:21, 19.81ba/s]Running tokenizer on every text in dataset:   2%|▏         | 203/8956 [00:11<07:12, 20.25ba/s]Running tokenizer on every text in dataset:   2%|▏         | 206/8956 [00:12<07:46, 18.76ba/s]Running tokenizer on every text in dataset:   2%|▏         | 208/8956 [00:12<07:49, 18.65ba/s]Running tokenizer on every text in dataset:   2%|▏         | 211/8956 [00:12<07:37, 19.11ba/s]Running tokenizer on every text in dataset:   2%|▏         | 214/8956 [00:12<07:49, 18.63ba/s]Running tokenizer on every text in dataset:   2%|▏         | 216/8956 [00:12<07:46, 18.75ba/s]Running tokenizer on every text in dataset:   2%|▏         | 218/8956 [00:12<07:47, 18.69ba/s]Running tokenizer on every text in dataset:   2%|▏         | 220/8956 [00:12<07:40, 18.99ba/s]Running tokenizer on every text in dataset:   2%|▏         | 223/8956 [00:12<07:37, 19.10ba/s]Running tokenizer on every text in dataset:   3%|▎         | 225/8956 [00:13<07:38, 19.03ba/s]Running tokenizer on every text in dataset:   3%|▎         | 228/8956 [00:13<07:25, 19.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 230/8956 [00:13<07:40, 18.94ba/s]Running tokenizer on every text in dataset:   3%|▎         | 233/8956 [00:13<07:38, 19.02ba/s]Running tokenizer on every text in dataset:   3%|▎         | 236/8956 [00:13<07:20, 19.78ba/s]Running tokenizer on every text in dataset:   3%|▎         | 239/8956 [00:13<07:16, 19.99ba/s]Running tokenizer on every text in dataset:   3%|▎         | 242/8956 [00:13<07:30, 19.35ba/s]Running tokenizer on every text in dataset:   3%|▎         | 245/8956 [00:14<07:26, 19.49ba/s]Running tokenizer on every text in dataset:   3%|▎         | 248/8956 [00:14<07:18, 19.87ba/s]Running tokenizer on every text in dataset:   3%|▎         | 250/8956 [00:14<07:24, 19.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 252/8956 [00:14<08:13, 17.65ba/s]Running tokenizer on every text in dataset:   3%|▎         | 255/8956 [00:14<07:47, 18.60ba/s]Running tokenizer on every text in dataset:   3%|▎         | 257/8956 [00:14<07:48, 18.58ba/s]Running tokenizer on every text in dataset:   3%|▎         | 259/8956 [00:14<07:45, 18.68ba/s]Running tokenizer on every text in dataset:   3%|▎         | 261/8956 [00:14<08:04, 17.94ba/s]Running tokenizer on every text in dataset:   3%|▎         | 264/8956 [00:15<07:40, 18.87ba/s]Running tokenizer on every text in dataset:   3%|▎         | 267/8956 [00:15<07:18, 19.81ba/s]Running tokenizer on every text in dataset:   3%|▎         | 270/8956 [00:15<07:10, 20.18ba/s]Running tokenizer on every text in dataset:   3%|▎         | 273/8956 [00:15<07:34, 19.12ba/s]Running tokenizer on every text in dataset:   3%|▎         | 275/8956 [00:15<07:32, 19.19ba/s]Running tokenizer on every text in dataset:   3%|▎         | 277/8956 [00:15<07:33, 19.12ba/s]Running tokenizer on every text in dataset:   3%|▎         | 279/8956 [00:15<07:30, 19.24ba/s]Running tokenizer on every text in dataset:   3%|▎         | 281/8956 [00:16<07:41, 18.78ba/s]Running tokenizer on every text in dataset:   3%|▎         | 284/8956 [00:16<07:13, 19.99ba/s]Running tokenizer on every text in dataset:   3%|▎         | 286/8956 [00:16<07:13, 19.98ba/s]Running tokenizer on every text in dataset:   3%|▎         | 289/8956 [00:16<07:05, 20.36ba/s]Running tokenizer on every text in dataset:   3%|▎         | 292/8956 [00:16<07:29, 19.28ba/s]Running tokenizer on every text in dataset:   3%|▎         | 295/8956 [00:16<07:14, 19.94ba/s]Running tokenizer on every text in dataset:   3%|▎         | 298/8956 [00:16<06:58, 20.66ba/s]Running tokenizer on every text in dataset:   3%|▎         | 301/8956 [00:17<07:24, 19.48ba/s]Running tokenizer on every text in dataset:   3%|▎         | 303/8956 [00:17<07:24, 19.47ba/s]Running tokenizer on every text in dataset:   3%|▎         | 306/8956 [00:17<07:13, 19.97ba/s]Running tokenizer on every text in dataset:   3%|▎         | 309/8956 [00:17<07:51, 18.33ba/s]Running tokenizer on every text in dataset:   3%|▎         | 312/8956 [00:17<07:32, 19.12ba/s]Running tokenizer on every text in dataset:   4%|▎         | 315/8956 [00:17<07:24, 19.46ba/s]Running tokenizer on every text in dataset:   4%|▎         | 317/8956 [00:17<07:35, 18.97ba/s]Running tokenizer on every text in dataset:   4%|▎         | 319/8956 [00:17<07:53, 18.26ba/s]Running tokenizer on every text in dataset:   4%|▎         | 322/8956 [00:18<07:32, 19.09ba/s]Running tokenizer on every text in dataset:   4%|▎         | 324/8956 [00:18<07:34, 18.99ba/s]Running tokenizer on every text in dataset:   4%|▎         | 327/8956 [00:18<07:22, 19.52ba/s]Running tokenizer on every text in dataset:   4%|▎         | 329/8956 [00:18<07:44, 18.59ba/s]Running tokenizer on every text in dataset:   4%|▎         | 331/8956 [00:18<07:37, 18.84ba/s]Running tokenizer on every text in dataset:   4%|▎         | 334/8956 [00:18<07:27, 19.25ba/s]Running tokenizer on every text in dataset:   4%|▍         | 337/8956 [00:18<07:34, 18.96ba/s]Running tokenizer on every text in dataset:   4%|▍         | 339/8956 [00:19<08:46, 16.37ba/s]Running tokenizer on every text in dataset:   4%|▍         | 342/8956 [00:19<08:09, 17.58ba/s]Running tokenizer on every text in dataset:   4%|▍         | 345/8956 [00:19<07:50, 18.30ba/s]Running tokenizer on every text in dataset:   4%|▍         | 347/8956 [00:19<08:11, 17.52ba/s]Running tokenizer on every text in dataset:   4%|▍         | 350/8956 [00:19<07:45, 18.47ba/s]Running tokenizer on every text in dataset:   4%|▍         | 353/8956 [00:19<07:32, 19.01ba/s]Running tokenizer on every text in dataset:   4%|▍         | 355/8956 [00:19<07:38, 18.77ba/s]Running tokenizer on every text in dataset:   4%|▍         | 357/8956 [00:20<07:59, 17.95ba/s]Running tokenizer on every text in dataset:   4%|▍         | 360/8956 [00:20<07:36, 18.82ba/s]Running tokenizer on every text in dataset:   4%|▍         | 363/8956 [00:20<07:19, 19.55ba/s]Running tokenizer on every text in dataset:   4%|▍         | 366/8956 [00:20<07:28, 19.14ba/s]Running tokenizer on every text in dataset:   4%|▍         | 369/8956 [00:20<07:16, 19.68ba/s]Running tokenizer on every text in dataset:   4%|▍         | 372/8956 [00:20<07:09, 19.98ba/s]Running tokenizer on every text in dataset:   4%|▍         | 374/8956 [00:20<07:14, 19.76ba/s]Running tokenizer on every text in dataset:   4%|▍         | 376/8956 [00:21<07:36, 18.80ba/s]Running tokenizer on every text in dataset:   4%|▍         | 379/8956 [00:21<07:25, 19.27ba/s]Running tokenizer on every text in dataset:   4%|▍         | 382/8956 [00:21<07:44, 18.46ba/s]Running tokenizer on every text in dataset:   4%|▍         | 385/8956 [00:21<07:44, 18.46ba/s]Running tokenizer on every text in dataset:   4%|▍         | 388/8956 [00:21<07:21, 19.39ba/s]Running tokenizer on every text in dataset:   4%|▍         | 391/8956 [00:21<07:27, 19.15ba/s]Running tokenizer on every text in dataset:   4%|▍         | 393/8956 [00:21<07:29, 19.06ba/s]Running tokenizer on every text in dataset:   4%|▍         | 395/8956 [00:22<07:44, 18.42ba/s]Running tokenizer on every text in dataset:   4%|▍         | 398/8956 [00:22<07:12, 19.79ba/s]Running tokenizer on every text in dataset:   4%|▍         | 401/8956 [00:22<07:16, 19.58ba/s]Running tokenizer on every text in dataset:   4%|▍         | 403/8956 [00:22<07:19, 19.45ba/s]Running tokenizer on every text in dataset:   5%|▍         | 405/8956 [00:22<07:57, 17.91ba/s]Running tokenizer on every text in dataset:   5%|▍         | 408/8956 [00:22<07:40, 18.54ba/s]Running tokenizer on every text in dataset:   5%|▍         | 410/8956 [00:22<07:43, 18.44ba/s]Running tokenizer on every text in dataset:   5%|▍         | 412/8956 [00:22<07:35, 18.75ba/s]Running tokenizer on every text in dataset:   5%|▍         | 414/8956 [00:23<07:47, 18.26ba/s]Running tokenizer on every text in dataset:   5%|▍         | 417/8956 [00:23<07:14, 19.67ba/s]Running tokenizer on every text in dataset:   5%|▍         | 419/8956 [00:23<07:18, 19.47ba/s]Running tokenizer on every text in dataset:   5%|▍         | 421/8956 [00:23<07:20, 19.36ba/s]Running tokenizer on every text in dataset:   5%|▍         | 423/8956 [00:23<07:32, 18.88ba/s]Running tokenizer on every text in dataset:   5%|▍         | 426/8956 [00:23<06:35, 21.55ba/s]Running tokenizer on every text in dataset:   5%|▍         | 430/8956 [00:23<05:25, 26.16ba/s]Running tokenizer on every text in dataset:   5%|▍         | 433/8956 [00:23<05:16, 26.93ba/s]Running tokenizer on every text in dataset:   5%|▍         | 437/8956 [00:23<04:41, 30.21ba/s]Running tokenizer on every text in dataset:   5%|▍         | 441/8956 [00:24<04:19, 32.80ba/s]Running tokenizer on every text in dataset:   5%|▍         | 445/8956 [00:24<05:22, 26.41ba/s]Running tokenizer on every text in dataset:   5%|▌         | 448/8956 [00:24<05:52, 24.17ba/s]Running tokenizer on every text in dataset:   5%|▌         | 451/8956 [00:24<06:32, 21.66ba/s]Running tokenizer on every text in dataset:   5%|▌         | 454/8956 [00:24<06:32, 21.66ba/s]Running tokenizer on every text in dataset:   5%|▌         | 457/8956 [00:24<07:01, 20.17ba/s]Running tokenizer on every text in dataset:   5%|▌         | 460/8956 [00:25<06:56, 20.40ba/s]Running tokenizer on every text in dataset:   5%|▌         | 463/8956 [00:25<06:59, 20.27ba/s]Running tokenizer on every text in dataset:   5%|▌         | 466/8956 [00:25<07:31, 18.80ba/s]Running tokenizer on every text in dataset:   5%|▌         | 469/8956 [00:25<07:16, 19.45ba/s]Running tokenizer on every text in dataset:   5%|▌         | 471/8956 [00:25<07:31, 18.80ba/s]Running tokenizer on every text in dataset:   5%|▌         | 474/8956 [00:25<07:20, 19.24ba/s]Running tokenizer on every text in dataset:   5%|▌         | 476/8956 [00:25<07:26, 18.98ba/s]Running tokenizer on every text in dataset:   5%|▌         | 478/8956 [00:25<07:24, 19.06ba/s]Running tokenizer on every text in dataset:   5%|▌         | 480/8956 [00:26<07:39, 18.46ba/s]Running tokenizer on every text in dataset:   5%|▌         | 483/8956 [00:26<07:27, 18.92ba/s]Running tokenizer on every text in dataset:   5%|▌         | 485/8956 [00:26<07:31, 18.77ba/s]Running tokenizer on every text in dataset:   5%|▌         | 487/8956 [00:26<07:24, 19.07ba/s]Running tokenizer on every text in dataset:   5%|▌         | 489/8956 [00:26<07:41, 18.36ba/s]Running tokenizer on every text in dataset:   5%|▌         | 492/8956 [00:26<07:12, 19.59ba/s]Running tokenizer on every text in dataset:   6%|▌         | 495/8956 [00:26<06:51, 20.57ba/s]Running tokenizer on every text in dataset:   6%|▌         | 498/8956 [00:26<07:10, 19.64ba/s]Running tokenizer on every text in dataset:   6%|▌         | 500/8956 [00:27<07:40, 18.35ba/s]Running tokenizer on every text in dataset:   6%|▌         | 502/8956 [00:27<07:47, 18.09ba/s]Running tokenizer on every text in dataset:   6%|▌         | 504/8956 [00:27<07:38, 18.43ba/s]Running tokenizer on every text in dataset:   6%|▌         | 506/8956 [00:27<07:36, 18.51ba/s]Running tokenizer on every text in dataset:   6%|▌         | 508/8956 [00:27<07:42, 18.26ba/s]Running tokenizer on every text in dataset:   6%|▌         | 511/8956 [00:27<07:17, 19.30ba/s]Running tokenizer on every text in dataset:   6%|▌         | 514/8956 [00:27<07:18, 19.25ba/s]Running tokenizer on every text in dataset:   6%|▌         | 517/8956 [00:28<07:01, 20.01ba/s]Running tokenizer on every text in dataset:   6%|▌         | 519/8956 [00:28<07:14, 19.43ba/s]Running tokenizer on every text in dataset:   6%|▌         | 522/8956 [00:28<07:04, 19.86ba/s]Running tokenizer on every text in dataset:   6%|▌         | 524/8956 [00:28<07:21, 19.12ba/s]Running tokenizer on every text in dataset:   6%|▌         | 526/8956 [00:28<07:19, 19.17ba/s]Running tokenizer on every text in dataset:   6%|▌         | 528/8956 [00:28<07:46, 18.08ba/s]Running tokenizer on every text in dataset:   6%|▌         | 531/8956 [00:28<07:06, 19.75ba/s]Running tokenizer on every text in dataset:   6%|▌         | 533/8956 [00:28<07:23, 18.98ba/s]Running tokenizer on every text in dataset:   6%|▌         | 535/8956 [00:28<07:29, 18.75ba/s]Running tokenizer on every text in dataset:   6%|▌         | 537/8956 [00:29<08:04, 17.39ba/s]Running tokenizer on every text in dataset:   6%|▌         | 539/8956 [00:29<08:04, 17.36ba/s]Running tokenizer on every text in dataset:   6%|▌         | 541/8956 [00:29<07:58, 17.59ba/s]Running tokenizer on every text in dataset:   6%|▌         | 543/8956 [00:29<07:44, 18.10ba/s]Running tokenizer on every text in dataset:   6%|▌         | 546/8956 [00:29<07:40, 18.24ba/s]Running tokenizer on every text in dataset:   6%|▌         | 549/8956 [00:29<07:18, 19.19ba/s]Running tokenizer on every text in dataset:   6%|▌         | 551/8956 [00:29<07:23, 18.95ba/s]Running tokenizer on every text in dataset:   6%|▌         | 553/8956 [00:29<07:34, 18.48ba/s]Running tokenizer on every text in dataset:   6%|▌         | 555/8956 [00:30<07:29, 18.68ba/s]Running tokenizer on every text in dataset:   6%|▌         | 557/8956 [00:30<07:59, 17.53ba/s]Running tokenizer on every text in dataset:   6%|▋         | 560/8956 [00:30<07:29, 18.66ba/s]Running tokenizer on every text in dataset:   6%|▋         | 562/8956 [00:30<07:36, 18.39ba/s]Running tokenizer on every text in dataset:   6%|▋         | 564/8956 [00:30<07:28, 18.73ba/s]Running tokenizer on every text in dataset:   6%|▋         | 566/8956 [00:30<08:48, 15.87ba/s]Running tokenizer on every text in dataset:   6%|▋         | 569/8956 [00:30<08:07, 17.21ba/s]Running tokenizer on every text in dataset:   6%|▋         | 571/8956 [00:30<07:51, 17.78ba/s]Running tokenizer on every text in dataset:   6%|▋         | 573/8956 [00:31<07:45, 18.01ba/s]Running tokenizer on every text in dataset:   6%|▋         | 575/8956 [00:31<07:55, 17.62ba/s]Running tokenizer on every text in dataset:   6%|▋         | 578/8956 [00:31<07:30, 18.60ba/s]Running tokenizer on every text in dataset:   6%|▋         | 581/8956 [00:31<07:07, 19.58ba/s]Running tokenizer on every text in dataset:   7%|▋         | 583/8956 [00:31<07:16, 19.19ba/s]Running tokenizer on every text in dataset:   7%|▋         | 585/8956 [00:31<07:42, 18.10ba/s]Running tokenizer on every text in dataset:   7%|▋         | 588/8956 [00:31<07:14, 19.24ba/s]Running tokenizer on every text in dataset:   7%|▋         | 590/8956 [00:31<07:16, 19.18ba/s]Running tokenizer on every text in dataset:   7%|▋         | 593/8956 [00:32<07:04, 19.69ba/s]Running tokenizer on every text in dataset:   7%|▋         | 595/8956 [00:32<07:27, 18.68ba/s]Running tokenizer on every text in dataset:   7%|▋         | 598/8956 [00:32<07:17, 19.10ba/s]Running tokenizer on every text in dataset:   7%|▋         | 601/8956 [00:32<07:07, 19.53ba/s]Running tokenizer on every text in dataset:   7%|▋         | 603/8956 [00:32<07:29, 18.60ba/s]Running tokenizer on every text in dataset:   7%|▋         | 606/8956 [00:32<07:16, 19.14ba/s]Running tokenizer on every text in dataset:   7%|▋         | 609/8956 [00:32<07:14, 19.21ba/s]Running tokenizer on every text in dataset:   7%|▋         | 611/8956 [00:33<07:18, 19.02ba/s]Running tokenizer on every text in dataset:   7%|▋         | 613/8956 [00:33<07:48, 17.80ba/s]Running tokenizer on every text in dataset:   7%|▋         | 615/8956 [00:33<07:41, 18.09ba/s]Running tokenizer on every text in dataset:   7%|▋         | 617/8956 [00:33<07:32, 18.43ba/s]Running tokenizer on every text in dataset:   7%|▋         | 619/8956 [00:33<07:24, 18.78ba/s]Running tokenizer on every text in dataset:   7%|▋         | 621/8956 [00:33<07:24, 18.76ba/s]Running tokenizer on every text in dataset:   7%|▋         | 623/8956 [00:33<07:54, 17.55ba/s]Running tokenizer on every text in dataset:   7%|▋         | 626/8956 [00:33<07:33, 18.35ba/s]Running tokenizer on every text in dataset:   7%|▋         | 628/8956 [00:34<07:31, 18.45ba/s]Running tokenizer on every text in dataset:   7%|▋         | 630/8956 [00:34<07:21, 18.85ba/s]Running tokenizer on every text in dataset:   7%|▋         | 632/8956 [00:34<07:37, 18.20ba/s]Running tokenizer on every text in dataset:   7%|▋         | 634/8956 [00:34<07:34, 18.30ba/s]Running tokenizer on every text in dataset:   7%|▋         | 636/8956 [00:34<07:30, 18.49ba/s]Running tokenizer on every text in dataset:   7%|▋         | 639/8956 [00:34<07:08, 19.41ba/s]Running tokenizer on every text in dataset:   7%|▋         | 641/8956 [00:34<07:33, 18.34ba/s]Running tokenizer on every text in dataset:   7%|▋         | 643/8956 [00:34<07:32, 18.38ba/s]Running tokenizer on every text in dataset:   7%|▋         | 645/8956 [00:34<07:27, 18.57ba/s]Running tokenizer on every text in dataset:   7%|▋         | 647/8956 [00:35<07:59, 17.31ba/s]Running tokenizer on every text in dataset:   7%|▋         | 649/8956 [00:35<07:42, 17.98ba/s]Running tokenizer on every text in dataset:   7%|▋         | 651/8956 [00:35<07:55, 17.47ba/s]Running tokenizer on every text in dataset:   7%|▋         | 654/8956 [00:35<08:18, 16.67ba/s]Running tokenizer on every text in dataset:   7%|▋         | 657/8956 [00:35<07:42, 17.94ba/s]Running tokenizer on every text in dataset:   7%|▋         | 659/8956 [00:35<07:33, 18.29ba/s]Running tokenizer on every text in dataset:   7%|▋         | 661/8956 [00:35<08:03, 17.16ba/s]Running tokenizer on every text in dataset:   7%|▋         | 663/8956 [00:35<07:47, 17.76ba/s]Running tokenizer on every text in dataset:   7%|▋         | 666/8956 [00:36<07:11, 19.19ba/s]Running tokenizer on every text in dataset:   7%|▋         | 669/8956 [00:36<07:03, 19.58ba/s]Running tokenizer on every text in dataset:   7%|▋         | 671/8956 [00:36<07:26, 18.57ba/s]Running tokenizer on every text in dataset:   8%|▊         | 674/8956 [00:36<07:06, 19.42ba/s]Running tokenizer on every text in dataset:   8%|▊         | 677/8956 [00:36<06:51, 20.10ba/s]Running tokenizer on every text in dataset:   8%|▊         | 680/8956 [00:36<07:16, 18.94ba/s]Running tokenizer on every text in dataset:   8%|▊         | 683/8956 [00:36<06:57, 19.83ba/s]Running tokenizer on every text in dataset:   8%|▊         | 686/8956 [00:37<06:45, 20.41ba/s]Running tokenizer on every text in dataset:   8%|▊         | 689/8956 [00:37<06:57, 19.82ba/s]Running tokenizer on every text in dataset:   8%|▊         | 691/8956 [00:37<06:56, 19.85ba/s]Running tokenizer on every text in dataset:   8%|▊         | 693/8956 [00:37<07:03, 19.51ba/s]Running tokenizer on every text in dataset:   8%|▊         | 696/8956 [00:37<06:55, 19.90ba/s]Running tokenizer on every text in dataset:   8%|▊         | 698/8956 [00:37<07:23, 18.64ba/s]Running tokenizer on every text in dataset:   8%|▊         | 700/8956 [00:37<07:17, 18.87ba/s]Running tokenizer on every text in dataset:   8%|▊         | 703/8956 [00:37<07:00, 19.62ba/s]Running tokenizer on every text in dataset:   8%|▊         | 705/8956 [00:38<07:01, 19.56ba/s]Running tokenizer on every text in dataset:   8%|▊         | 708/8956 [00:38<06:46, 20.30ba/s]Running tokenizer on every text in dataset:   8%|▊         | 711/8956 [00:38<06:52, 20.00ba/s]Running tokenizer on every text in dataset:   8%|▊         | 713/8956 [00:38<06:52, 19.98ba/s]Running tokenizer on every text in dataset:   8%|▊         | 715/8956 [00:38<07:01, 19.57ba/s]Running tokenizer on every text in dataset:   8%|▊         | 717/8956 [00:38<07:14, 18.94ba/s]Running tokenizer on every text in dataset:   8%|▊         | 720/8956 [00:38<06:52, 19.96ba/s]Running tokenizer on every text in dataset:   8%|▊         | 723/8956 [00:38<06:42, 20.47ba/s]Running tokenizer on every text in dataset:   8%|▊         | 726/8956 [00:39<07:13, 19.00ba/s]Running tokenizer on every text in dataset:   8%|▊         | 728/8956 [00:39<07:38, 17.95ba/s]Running tokenizer on every text in dataset:   8%|▊         | 730/8956 [00:39<07:35, 18.08ba/s]Running tokenizer on every text in dataset:   8%|▊         | 732/8956 [00:39<07:39, 17.88ba/s]Running tokenizer on every text in dataset:   8%|▊         | 734/8956 [00:39<07:28, 18.34ba/s]Running tokenizer on every text in dataset:   8%|▊         | 736/8956 [00:39<07:39, 17.91ba/s]Running tokenizer on every text in dataset:   8%|▊         | 738/8956 [00:39<07:31, 18.19ba/s]Running tokenizer on every text in dataset:   8%|▊         | 740/8956 [00:39<07:27, 18.35ba/s]Running tokenizer on every text in dataset:   8%|▊         | 743/8956 [00:40<07:08, 19.17ba/s]Running tokenizer on every text in dataset:   8%|▊         | 745/8956 [00:40<07:09, 19.13ba/s]Running tokenizer on every text in dataset:   8%|▊         | 747/8956 [00:40<07:33, 18.09ba/s]Running tokenizer on every text in dataset:   8%|▊         | 749/8956 [00:40<07:45, 17.64ba/s]Running tokenizer on every text in dataset:   8%|▊         | 752/8956 [00:40<07:14, 18.88ba/s]Running tokenizer on every text in dataset:   8%|▊         | 754/8956 [00:40<07:12, 18.95ba/s]Running tokenizer on every text in dataset:   8%|▊         | 756/8956 [00:40<07:47, 17.55ba/s]Running tokenizer on every text in dataset:   8%|▊         | 758/8956 [00:40<07:38, 17.87ba/s]Running tokenizer on every text in dataset:   8%|▊         | 761/8956 [00:41<07:10, 19.05ba/s]Running tokenizer on every text in dataset:   9%|▊         | 764/8956 [00:41<06:57, 19.63ba/s]Running tokenizer on every text in dataset:   9%|▊         | 766/8956 [00:41<07:20, 18.58ba/s]Running tokenizer on every text in dataset:   9%|▊         | 768/8956 [00:41<07:15, 18.81ba/s]Running tokenizer on every text in dataset:   9%|▊         | 770/8956 [00:41<07:09, 19.08ba/s]Running tokenizer on every text in dataset:   9%|▊         | 772/8956 [00:41<07:11, 18.98ba/s]Running tokenizer on every text in dataset:   9%|▊         | 774/8956 [00:41<07:30, 18.16ba/s]Running tokenizer on every text in dataset:   9%|▊         | 777/8956 [00:41<07:06, 19.19ba/s]Running tokenizer on every text in dataset:   9%|▊         | 779/8956 [00:42<07:03, 19.33ba/s]Running tokenizer on every text in dataset:   9%|▊         | 781/8956 [00:42<08:05, 16.84ba/s]Running tokenizer on every text in dataset:   9%|▉         | 784/8956 [00:42<07:54, 17.22ba/s]Running tokenizer on every text in dataset:   9%|▉         | 787/8956 [00:42<07:24, 18.37ba/s]Running tokenizer on every text in dataset:   9%|▉         | 789/8956 [00:42<07:34, 17.98ba/s]Running tokenizer on every text in dataset:   9%|▉         | 792/8956 [00:42<07:14, 18.79ba/s]Running tokenizer on every text in dataset:   9%|▉         | 794/8956 [00:42<07:33, 18.01ba/s]Running tokenizer on every text in dataset:   9%|▉         | 797/8956 [00:43<07:22, 18.44ba/s]Running tokenizer on every text in dataset:   9%|▉         | 800/8956 [00:43<07:02, 19.33ba/s]Running tokenizer on every text in dataset:   9%|▉         | 802/8956 [00:43<06:59, 19.45ba/s]Running tokenizer on every text in dataset:   9%|▉         | 804/8956 [00:43<07:10, 18.93ba/s]Running tokenizer on every text in dataset:   9%|▉         | 807/8956 [00:43<06:56, 19.57ba/s]Running tokenizer on every text in dataset:   9%|▉         | 810/8956 [00:43<06:46, 20.05ba/s]Running tokenizer on every text in dataset:   9%|▉         | 812/8956 [00:43<07:12, 18.82ba/s]Running tokenizer on every text in dataset:   9%|▉         | 815/8956 [00:43<07:08, 19.01ba/s]Running tokenizer on every text in dataset:   9%|▉         | 818/8956 [00:44<07:00, 19.37ba/s]Running tokenizer on every text in dataset:   9%|▉         | 820/8956 [00:44<07:03, 19.22ba/s]Running tokenizer on every text in dataset:   9%|▉         | 822/8956 [00:44<07:24, 18.28ba/s]Running tokenizer on every text in dataset:   9%|▉         | 825/8956 [00:44<07:02, 19.27ba/s]Running tokenizer on every text in dataset:   9%|▉         | 827/8956 [00:44<07:07, 19.00ba/s]Running tokenizer on every text in dataset:   9%|▉         | 830/8956 [00:44<06:52, 19.70ba/s]Running tokenizer on every text in dataset:   9%|▉         | 832/8956 [00:44<07:38, 17.73ba/s]Running tokenizer on every text in dataset:   9%|▉         | 834/8956 [00:44<07:29, 18.06ba/s]Running tokenizer on every text in dataset:   9%|▉         | 836/8956 [00:45<07:28, 18.09ba/s]Running tokenizer on every text in dataset:   9%|▉         | 838/8956 [00:45<07:21, 18.38ba/s]Running tokenizer on every text in dataset:   9%|▉         | 840/8956 [00:45<07:12, 18.77ba/s]Running tokenizer on every text in dataset:   9%|▉         | 842/8956 [00:45<07:39, 17.65ba/s]Running tokenizer on every text in dataset:   9%|▉         | 845/8956 [00:45<07:13, 18.72ba/s]Running tokenizer on every text in dataset:   9%|▉         | 848/8956 [00:45<06:57, 19.44ba/s]Running tokenizer on every text in dataset:   9%|▉         | 850/8956 [00:45<07:17, 18.53ba/s]Running tokenizer on every text in dataset:  10%|▉         | 853/8956 [00:45<06:51, 19.72ba/s]Running tokenizer on every text in dataset:  10%|▉         | 856/8956 [00:46<06:28, 20.84ba/s]Running tokenizer on every text in dataset:  10%|▉         | 859/8956 [00:46<06:22, 21.17ba/s]Running tokenizer on every text in dataset:  10%|▉         | 862/8956 [00:46<06:35, 20.44ba/s]Running tokenizer on every text in dataset:  10%|▉         | 865/8956 [00:46<06:21, 21.18ba/s]Running tokenizer on every text in dataset:  10%|▉         | 868/8956 [00:46<06:11, 21.80ba/s]Running tokenizer on every text in dataset:  10%|▉         | 871/8956 [00:46<06:23, 21.08ba/s]Running tokenizer on every text in dataset:  10%|▉         | 874/8956 [00:46<06:13, 21.62ba/s]Running tokenizer on every text in dataset:  10%|▉         | 877/8956 [00:47<06:08, 21.93ba/s]Running tokenizer on every text in dataset:  10%|▉         | 880/8956 [00:47<06:19, 21.27ba/s]Running tokenizer on every text in dataset:  10%|▉         | 883/8956 [00:47<06:11, 21.71ba/s]Running tokenizer on every text in dataset:  10%|▉         | 886/8956 [00:47<06:12, 21.67ba/s]Running tokenizer on every text in dataset:  10%|▉         | 889/8956 [00:47<06:28, 20.76ba/s]Running tokenizer on every text in dataset:  10%|▉         | 892/8956 [00:47<06:24, 20.95ba/s]Running tokenizer on every text in dataset:  10%|▉         | 895/8956 [00:47<06:34, 20.41ba/s]Running tokenizer on every text in dataset:  10%|█         | 898/8956 [00:48<06:58, 19.26ba/s]Running tokenizer on every text in dataset:  10%|█         | 900/8956 [00:48<06:57, 19.30ba/s]Running tokenizer on every text in dataset:  10%|█         | 902/8956 [00:48<06:59, 19.22ba/s]Running tokenizer on every text in dataset:  10%|█         | 904/8956 [00:48<06:55, 19.38ba/s]Running tokenizer on every text in dataset:  10%|█         | 907/8956 [00:48<06:45, 19.87ba/s]Running tokenizer on every text in dataset:  10%|█         | 910/8956 [00:48<06:25, 20.90ba/s]Running tokenizer on every text in dataset:  10%|█         | 913/8956 [00:48<06:31, 20.54ba/s]Running tokenizer on every text in dataset:  10%|█         | 916/8956 [00:49<06:46, 19.78ba/s]Running tokenizer on every text in dataset:  10%|█         | 918/8956 [00:49<07:08, 18.76ba/s]Running tokenizer on every text in dataset:  10%|█         | 920/8956 [00:49<07:03, 18.98ba/s]Running tokenizer on every text in dataset:  10%|█         | 922/8956 [00:49<07:28, 17.93ba/s]Running tokenizer on every text in dataset:  10%|█         | 925/8956 [00:49<07:04, 18.92ba/s]Running tokenizer on every text in dataset:  10%|█         | 927/8956 [00:49<07:19, 18.27ba/s]Running tokenizer on every text in dataset:  10%|█         | 930/8956 [00:49<06:55, 19.32ba/s]Running tokenizer on every text in dataset:  10%|█         | 932/8956 [00:49<07:23, 18.08ba/s]Running tokenizer on every text in dataset:  10%|█         | 935/8956 [00:50<06:52, 19.44ba/s]Running tokenizer on every text in dataset:  10%|█         | 937/8956 [00:50<07:26, 17.96ba/s]Running tokenizer on every text in dataset:  10%|█         | 939/8956 [00:50<07:30, 17.81ba/s]Running tokenizer on every text in dataset:  11%|█         | 941/8956 [00:50<07:25, 18.01ba/s]Running tokenizer on every text in dataset:  11%|█         | 943/8956 [00:50<07:20, 18.21ba/s]Running tokenizer on every text in dataset:  11%|█         | 945/8956 [00:50<07:45, 17.20ba/s]Running tokenizer on every text in dataset:  11%|█         | 947/8956 [00:50<07:38, 17.47ba/s]Running tokenizer on every text in dataset:  11%|█         | 950/8956 [00:50<07:10, 18.61ba/s]Running tokenizer on every text in dataset:  11%|█         | 953/8956 [00:51<06:52, 19.42ba/s]Running tokenizer on every text in dataset:  11%|█         | 955/8956 [00:51<07:27, 17.90ba/s]Running tokenizer on every text in dataset:  11%|█         | 958/8956 [00:51<07:11, 18.54ba/s]Running tokenizer on every text in dataset:  11%|█         | 961/8956 [00:51<06:56, 19.21ba/s]Running tokenizer on every text in dataset:  11%|█         | 964/8956 [00:51<07:13, 18.46ba/s]Running tokenizer on every text in dataset:  11%|█         | 966/8956 [00:51<07:16, 18.29ba/s]Running tokenizer on every text in dataset:  11%|█         | 969/8956 [00:51<07:02, 18.91ba/s]Running tokenizer on every text in dataset:  11%|█         | 972/8956 [00:52<06:50, 19.47ba/s]Running tokenizer on every text in dataset:  11%|█         | 974/8956 [00:52<07:27, 17.84ba/s]Running tokenizer on every text in dataset:  11%|█         | 976/8956 [00:52<07:25, 17.93ba/s]Running tokenizer on every text in dataset:  11%|█         | 978/8956 [00:52<07:16, 18.26ba/s]Running tokenizer on every text in dataset:  11%|█         | 980/8956 [00:52<07:36, 17.47ba/s]Running tokenizer on every text in dataset:  11%|█         | 982/8956 [00:52<07:20, 18.11ba/s]Running tokenizer on every text in dataset:  11%|█         | 984/8956 [00:52<07:42, 17.24ba/s]Running tokenizer on every text in dataset:  11%|█         | 986/8956 [00:52<08:39, 15.35ba/s]Running tokenizer on every text in dataset:  11%|█         | 989/8956 [00:53<07:59, 16.62ba/s]Running tokenizer on every text in dataset:  11%|█         | 991/8956 [00:53<07:45, 17.11ba/s]Running tokenizer on every text in dataset:  11%|█         | 993/8956 [00:53<08:00, 16.57ba/s]Running tokenizer on every text in dataset:  11%|█         | 995/8956 [00:53<08:02, 16.49ba/s]Running tokenizer on every text in dataset:  11%|█         | 997/8956 [00:53<07:40, 17.28ba/s]Running tokenizer on every text in dataset:  11%|█         | 1000/8956 [00:53<07:10, 18.47ba/s]Running tokenizer on every text in dataset:  11%|█         | 1002/8956 [00:53<07:39, 17.31ba/s]Running tokenizer on every text in dataset:  11%|█         | 1004/8956 [00:53<07:30, 17.64ba/s]Running tokenizer on every text in dataset:  11%|█         | 1006/8956 [00:54<07:21, 18.00ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1008/8956 [00:54<07:19, 18.07ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1011/8956 [00:54<07:01, 18.83ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1013/8956 [00:54<07:27, 17.76ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1015/8956 [00:54<07:19, 18.08ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1018/8956 [00:54<07:02, 18.80ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1020/8956 [00:54<07:04, 18.69ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1022/8956 [00:54<07:24, 17.85ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1025/8956 [00:55<07:05, 18.63ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1027/8956 [00:55<07:13, 18.30ba/s]Running tokenizer on every text in dataset:  11%|█▏        | 1029/8956 [00:55<07:20, 17.98ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1031/8956 [00:55<07:43, 17.10ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1034/8956 [00:55<07:17, 18.09ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1037/8956 [00:55<06:55, 19.06ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1040/8956 [00:55<06:59, 18.88ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1042/8956 [00:55<07:08, 18.49ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1044/8956 [00:56<07:34, 17.40ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1046/8956 [00:56<07:21, 17.93ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1049/8956 [00:56<07:02, 18.71ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1051/8956 [00:56<07:57, 16.55ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1054/8956 [00:56<07:14, 18.20ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1057/8956 [00:56<06:45, 19.48ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1059/8956 [00:56<07:44, 17.01ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1061/8956 [00:57<07:40, 17.13ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1063/8956 [00:57<07:26, 17.69ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1066/8956 [00:57<07:07, 18.46ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1068/8956 [00:57<07:05, 18.53ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1070/8956 [00:57<07:23, 17.79ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1073/8956 [00:57<06:59, 18.78ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1075/8956 [00:57<06:53, 19.05ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1077/8956 [00:57<06:48, 19.27ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1079/8956 [00:58<07:08, 18.38ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1081/8956 [00:58<07:04, 18.53ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1083/8956 [00:58<07:11, 18.26ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1085/8956 [00:58<07:02, 18.65ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1087/8956 [00:58<06:54, 18.98ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1089/8956 [00:58<07:40, 17.07ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1091/8956 [00:58<07:24, 17.68ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1093/8956 [00:58<07:27, 17.56ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1095/8956 [00:58<07:20, 17.83ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1097/8956 [00:59<07:55, 16.52ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1099/8956 [00:59<07:56, 16.50ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1101/8956 [00:59<07:47, 16.79ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1103/8956 [00:59<07:30, 17.42ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1105/8956 [00:59<07:19, 17.85ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1107/8956 [00:59<07:32, 17.34ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1110/8956 [00:59<06:58, 18.74ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1113/8956 [00:59<06:44, 19.37ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1115/8956 [01:00<06:45, 19.32ba/s]Running tokenizer on every text in dataset:  12%|█▏        | 1117/8956 [01:00<07:03, 18.51ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1120/8956 [01:00<06:45, 19.33ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1123/8956 [01:00<06:30, 20.07ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1126/8956 [01:00<06:49, 19.11ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1128/8956 [01:00<06:47, 19.22ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1131/8956 [01:00<06:29, 20.08ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1134/8956 [01:00<06:14, 20.88ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1137/8956 [01:01<06:37, 19.68ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1140/8956 [01:01<06:14, 20.89ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1143/8956 [01:01<06:17, 20.70ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1146/8956 [01:01<06:45, 19.24ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1148/8956 [01:01<06:43, 19.35ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1150/8956 [01:01<06:40, 19.47ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1153/8956 [01:01<06:33, 19.84ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1155/8956 [01:02<06:51, 18.95ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1157/8956 [01:02<06:56, 18.72ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1159/8956 [01:02<07:03, 18.42ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1161/8956 [01:02<06:59, 18.58ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1163/8956 [01:02<07:02, 18.43ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1165/8956 [01:02<07:05, 18.32ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1167/8956 [01:02<06:57, 18.67ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1170/8956 [01:02<06:41, 19.41ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1173/8956 [01:03<07:01, 18.48ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1175/8956 [01:03<07:01, 18.47ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1177/8956 [01:03<06:57, 18.62ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1180/8956 [01:03<06:45, 19.19ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1183/8956 [01:03<06:42, 19.31ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1186/8956 [01:03<06:34, 19.70ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1188/8956 [01:03<06:36, 19.61ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1191/8956 [01:03<06:29, 19.93ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1193/8956 [01:04<06:53, 18.76ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1195/8956 [01:04<06:54, 18.73ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1197/8956 [01:04<06:51, 18.87ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1200/8956 [01:04<06:31, 19.80ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1202/8956 [01:04<06:57, 18.59ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1205/8956 [01:04<06:41, 19.32ba/s]Running tokenizer on every text in dataset:  13%|█▎        | 1208/8956 [01:04<06:24, 20.14ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1211/8956 [01:05<06:34, 19.64ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1214/8956 [01:05<06:20, 20.32ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1217/8956 [01:05<06:25, 20.06ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1220/8956 [01:05<06:29, 19.86ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1222/8956 [01:05<06:46, 19.03ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1224/8956 [01:05<06:46, 19.04ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1226/8956 [01:05<06:40, 19.28ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1228/8956 [01:05<06:42, 19.20ba/s]Running tokenizer on every text in dataset:  14%|█▎        | 1230/8956 [01:06<07:08, 18.04ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1232/8956 [01:06<06:59, 18.42ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1234/8956 [01:06<06:52, 18.70ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1237/8956 [01:06<06:30, 19.74ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1239/8956 [01:06<06:31, 19.71ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1241/8956 [01:06<07:31, 17.09ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1244/8956 [01:06<06:44, 19.04ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1247/8956 [01:06<06:13, 20.66ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1250/8956 [01:07<06:17, 20.43ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1253/8956 [01:07<06:36, 19.43ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1256/8956 [01:07<06:14, 20.54ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1259/8956 [01:07<06:19, 20.29ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1262/8956 [01:07<06:06, 20.97ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1265/8956 [01:07<05:56, 21.60ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1268/8956 [01:07<06:09, 20.78ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1271/8956 [01:08<05:59, 21.40ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1274/8956 [01:08<05:48, 22.06ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1277/8956 [01:08<05:41, 22.48ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1280/8956 [01:08<06:13, 20.55ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1283/8956 [01:08<06:00, 21.27ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1286/8956 [01:08<05:50, 21.88ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1289/8956 [01:08<06:03, 21.08ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1292/8956 [01:08<05:55, 21.58ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1295/8956 [01:09<05:49, 21.91ba/s]Running tokenizer on every text in dataset:  14%|█▍        | 1298/8956 [01:09<06:00, 21.22ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1301/8956 [01:09<05:55, 21.51ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1304/8956 [01:09<05:59, 21.31ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1307/8956 [01:09<06:08, 20.78ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1310/8956 [01:09<05:58, 21.31ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1313/8956 [01:09<05:52, 21.71ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1316/8956 [01:10<06:04, 20.93ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1319/8956 [01:10<05:54, 21.54ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1322/8956 [01:10<05:49, 21.84ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1325/8956 [01:10<06:02, 21.06ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1328/8956 [01:10<05:49, 21.81ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1331/8956 [01:10<05:50, 21.76ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1334/8956 [01:10<05:50, 21.77ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1337/8956 [01:11<06:22, 19.92ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1340/8956 [01:11<06:26, 19.72ba/s]Running tokenizer on every text in dataset:  15%|█▍        | 1342/8956 [01:11<06:31, 19.43ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1344/8956 [01:11<06:58, 18.18ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1346/8956 [01:11<06:57, 18.23ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1348/8956 [01:11<06:57, 18.22ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1350/8956 [01:11<07:00, 18.07ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1352/8956 [01:11<06:53, 18.40ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1354/8956 [01:12<07:15, 17.48ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1356/8956 [01:12<07:11, 17.60ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1358/8956 [01:12<07:09, 17.68ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1360/8956 [01:12<07:16, 17.42ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1362/8956 [01:12<07:12, 17.56ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1364/8956 [01:12<07:30, 16.85ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1366/8956 [01:12<07:26, 17.00ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1368/8956 [01:12<07:09, 17.68ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1370/8956 [01:12<07:07, 17.75ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1372/8956 [01:13<07:07, 17.75ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1374/8956 [01:13<07:44, 16.33ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1376/8956 [01:13<07:36, 16.60ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1379/8956 [01:13<07:09, 17.65ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1381/8956 [01:13<07:06, 17.74ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1383/8956 [01:13<07:37, 16.55ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1385/8956 [01:13<07:38, 16.52ba/s]Running tokenizer on every text in dataset:  15%|█▌        | 1387/8956 [01:14<07:31, 16.75ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1389/8956 [01:14<07:31, 16.76ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1391/8956 [01:14<07:20, 17.18ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1393/8956 [01:14<07:34, 16.63ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1395/8956 [01:14<07:30, 16.77ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1397/8956 [01:14<07:26, 16.93ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1399/8956 [01:14<07:29, 16.80ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1401/8956 [01:14<07:52, 15.99ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1403/8956 [01:14<07:38, 16.48ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1405/8956 [01:15<07:31, 16.73ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1407/8956 [01:15<07:17, 17.25ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1409/8956 [01:15<07:10, 17.53ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1411/8956 [01:15<07:33, 16.62ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1413/8956 [01:15<07:27, 16.86ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1415/8956 [01:15<07:34, 16.61ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1418/8956 [01:15<07:02, 17.82ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1420/8956 [01:15<07:19, 17.13ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1423/8956 [01:16<06:52, 18.27ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1426/8956 [01:16<06:40, 18.78ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1429/8956 [01:16<06:32, 19.17ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1431/8956 [01:16<06:57, 18.03ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1433/8956 [01:16<06:50, 18.32ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1435/8956 [01:16<06:43, 18.62ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1437/8956 [01:16<06:40, 18.75ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1439/8956 [01:16<07:04, 17.71ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1442/8956 [01:17<06:35, 19.02ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1445/8956 [01:17<06:23, 19.59ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1448/8956 [01:17<06:17, 19.90ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1450/8956 [01:17<06:33, 19.06ba/s]Running tokenizer on every text in dataset:  16%|█▌        | 1453/8956 [01:17<06:21, 19.66ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1456/8956 [01:17<06:19, 19.79ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1458/8956 [01:17<06:43, 18.58ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1461/8956 [01:18<06:30, 19.19ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1464/8956 [01:18<06:22, 19.59ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1466/8956 [01:18<06:20, 19.66ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1468/8956 [01:18<06:39, 18.73ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1470/8956 [01:18<06:33, 19.04ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1473/8956 [01:18<06:23, 19.51ba/s]Running tokenizer on every text in dataset:  16%|█▋        | 1476/8956 [01:18<06:14, 19.96ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1478/8956 [01:18<06:35, 18.92ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1481/8956 [01:19<06:20, 19.64ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1484/8956 [01:19<06:16, 19.84ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1487/8956 [01:19<06:31, 19.08ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1490/8956 [01:19<06:19, 19.68ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1493/8956 [01:19<06:14, 19.92ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1495/8956 [01:19<06:23, 19.47ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1497/8956 [01:19<06:55, 17.94ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1499/8956 [01:20<07:33, 16.44ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1501/8956 [01:20<07:23, 16.80ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1503/8956 [01:20<07:18, 16.98ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1505/8956 [01:20<07:09, 17.35ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1507/8956 [01:20<08:24, 14.77ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1509/8956 [01:20<07:58, 15.56ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1511/8956 [01:20<07:48, 15.89ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1513/8956 [01:21<07:52, 15.76ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1515/8956 [01:21<08:08, 15.23ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1517/8956 [01:21<07:53, 15.71ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1519/8956 [01:21<07:45, 15.98ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1521/8956 [01:21<07:33, 16.40ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1523/8956 [01:21<07:25, 16.67ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1525/8956 [01:21<07:44, 15.98ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1527/8956 [01:21<07:46, 15.91ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1529/8956 [01:21<07:31, 16.46ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1531/8956 [01:22<07:24, 16.71ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1533/8956 [01:22<07:15, 17.03ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1535/8956 [01:22<07:45, 15.93ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1537/8956 [01:22<07:32, 16.38ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1539/8956 [01:22<07:29, 16.52ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1541/8956 [01:22<07:28, 16.53ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1543/8956 [01:22<07:13, 17.10ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1545/8956 [01:22<07:40, 16.08ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1547/8956 [01:23<07:28, 16.53ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1549/8956 [01:23<07:11, 17.17ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1552/8956 [01:23<06:41, 18.44ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1554/8956 [01:23<06:57, 17.72ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1556/8956 [01:23<06:46, 18.20ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1558/8956 [01:23<06:39, 18.52ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1560/8956 [01:23<06:35, 18.72ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1563/8956 [01:23<06:46, 18.18ba/s]Running tokenizer on every text in dataset:  17%|█▋        | 1565/8956 [01:24<06:41, 18.40ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1568/8956 [01:24<06:29, 18.97ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1571/8956 [01:24<06:21, 19.35ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1573/8956 [01:24<06:47, 18.11ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1575/8956 [01:24<06:37, 18.56ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1577/8956 [01:24<06:36, 18.61ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1579/8956 [01:24<06:32, 18.78ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1582/8956 [01:24<06:44, 18.21ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1584/8956 [01:25<06:43, 18.25ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1586/8956 [01:25<06:46, 18.12ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1588/8956 [01:25<06:42, 18.31ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1590/8956 [01:25<06:38, 18.48ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1592/8956 [01:25<07:11, 17.08ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1594/8956 [01:25<06:57, 17.65ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1596/8956 [01:25<06:55, 17.70ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1599/8956 [01:25<06:35, 18.59ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1601/8956 [01:26<06:56, 17.66ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1603/8956 [01:26<06:53, 17.79ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1606/8956 [01:26<06:21, 19.25ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1609/8956 [01:26<06:05, 20.09ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1611/8956 [01:26<06:16, 19.49ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1614/8956 [01:26<06:07, 19.98ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1617/8956 [01:26<05:58, 20.48ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1620/8956 [01:26<06:13, 19.65ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1623/8956 [01:27<05:59, 20.37ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1626/8956 [01:27<05:51, 20.84ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1629/8956 [01:27<06:08, 19.90ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1632/8956 [01:27<05:56, 20.52ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1635/8956 [01:27<05:58, 20.44ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1638/8956 [01:27<05:56, 20.53ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1641/8956 [01:27<06:05, 20.00ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1644/8956 [01:28<05:59, 20.32ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1647/8956 [01:28<05:58, 20.38ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1650/8956 [01:28<06:19, 19.25ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1653/8956 [01:28<06:05, 19.99ba/s]Running tokenizer on every text in dataset:  18%|█▊        | 1656/8956 [01:28<05:59, 20.30ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1659/8956 [01:28<06:15, 19.42ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1662/8956 [01:29<06:07, 19.85ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1665/8956 [01:29<06:08, 19.78ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1667/8956 [01:29<06:38, 18.27ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1670/8956 [01:29<06:23, 18.99ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1672/8956 [01:29<06:25, 18.91ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1675/8956 [01:29<06:07, 19.80ba/s]Running tokenizer on every text in dataset:  19%|█▊        | 1677/8956 [01:29<06:16, 19.35ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1680/8956 [01:29<06:00, 20.21ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1683/8956 [01:30<06:12, 19.55ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1686/8956 [01:30<06:20, 19.13ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1688/8956 [01:30<06:19, 19.17ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1690/8956 [01:30<06:26, 18.79ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1693/8956 [01:30<06:21, 19.02ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1695/8956 [01:30<06:18, 19.19ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1697/8956 [01:30<06:22, 18.98ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1700/8956 [01:31<06:13, 19.42ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1703/8956 [01:31<06:07, 19.73ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1705/8956 [01:31<06:21, 19.01ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1708/8956 [01:31<06:09, 19.62ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1711/8956 [01:31<05:57, 20.25ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1714/8956 [01:31<05:57, 20.28ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1717/8956 [01:31<06:20, 19.01ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1719/8956 [01:32<06:21, 18.97ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1722/8956 [01:32<06:13, 19.37ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1724/8956 [01:32<06:35, 18.29ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1726/8956 [01:32<06:30, 18.51ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1728/8956 [01:32<06:25, 18.75ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1730/8956 [01:32<06:20, 18.99ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1732/8956 [01:32<06:21, 18.95ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1734/8956 [01:32<06:43, 17.90ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1736/8956 [01:32<06:32, 18.39ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1738/8956 [01:33<06:25, 18.70ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1740/8956 [01:33<06:20, 18.97ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1743/8956 [01:33<06:30, 18.45ba/s]Running tokenizer on every text in dataset:  19%|█▉        | 1746/8956 [01:33<06:12, 19.34ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1748/8956 [01:33<06:13, 19.31ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1751/8956 [01:33<06:09, 19.52ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1753/8956 [01:33<06:34, 18.26ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1755/8956 [01:33<06:37, 18.11ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1757/8956 [01:34<06:35, 18.21ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1759/8956 [01:34<06:30, 18.43ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1761/8956 [01:34<06:22, 18.81ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1763/8956 [01:34<06:52, 17.42ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1765/8956 [01:34<06:41, 17.90ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1768/8956 [01:34<06:25, 18.63ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1770/8956 [01:34<06:31, 18.36ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1772/8956 [01:34<06:57, 17.23ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1774/8956 [01:34<06:48, 17.60ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1776/8956 [01:35<06:48, 17.57ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1778/8956 [01:35<06:42, 17.84ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1780/8956 [01:35<06:35, 18.15ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1782/8956 [01:35<06:57, 17.18ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1784/8956 [01:35<06:43, 17.77ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1786/8956 [01:35<06:34, 18.16ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1788/8956 [01:35<06:30, 18.35ba/s]Running tokenizer on every text in dataset:  20%|█▉        | 1790/8956 [01:35<06:25, 18.61ba/s]Running tokenizer on every text in dataset:  20%|██        | 1792/8956 [01:36<06:57, 17.15ba/s]Running tokenizer on every text in dataset:  20%|██        | 1794/8956 [01:36<06:49, 17.51ba/s]Running tokenizer on every text in dataset:  20%|██        | 1796/8956 [01:36<06:38, 17.98ba/s]Running tokenizer on every text in dataset:  20%|██        | 1798/8956 [01:36<06:35, 18.12ba/s]Running tokenizer on every text in dataset:  20%|██        | 1800/8956 [01:36<06:56, 17.17ba/s]Running tokenizer on every text in dataset:  20%|██        | 1802/8956 [01:36<06:46, 17.58ba/s]Running tokenizer on every text in dataset:  20%|██        | 1804/8956 [01:36<06:33, 18.20ba/s]Running tokenizer on every text in dataset:  20%|██        | 1806/8956 [01:36<06:32, 18.20ba/s]Running tokenizer on every text in dataset:  20%|██        | 1808/8956 [01:36<06:29, 18.33ba/s]Running tokenizer on every text in dataset:  20%|██        | 1810/8956 [01:37<06:53, 17.28ba/s]Running tokenizer on every text in dataset:  20%|██        | 1812/8956 [01:37<06:38, 17.93ba/s]Running tokenizer on every text in dataset:  20%|██        | 1814/8956 [01:37<06:38, 17.92ba/s]Running tokenizer on every text in dataset:  20%|██        | 1816/8956 [01:37<06:40, 17.83ba/s]Running tokenizer on every text in dataset:  20%|██        | 1818/8956 [01:37<06:32, 18.19ba/s]Running tokenizer on every text in dataset:  20%|██        | 1820/8956 [01:37<06:50, 17.37ba/s]Running tokenizer on every text in dataset:  20%|██        | 1822/8956 [01:37<06:48, 17.45ba/s]Running tokenizer on every text in dataset:  20%|██        | 1824/8956 [01:37<06:39, 17.86ba/s]Running tokenizer on every text in dataset:  20%|██        | 1826/8956 [01:37<06:34, 18.08ba/s]Running tokenizer on every text in dataset:  20%|██        | 1828/8956 [01:38<06:29, 18.31ba/s]Running tokenizer on every text in dataset:  20%|██        | 1830/8956 [01:38<06:59, 17.00ba/s]Running tokenizer on every text in dataset:  20%|██        | 1832/8956 [01:38<06:44, 17.63ba/s]Running tokenizer on every text in dataset:  20%|██        | 1834/8956 [01:38<06:40, 17.77ba/s]Running tokenizer on every text in dataset:  21%|██        | 1836/8956 [01:38<06:28, 18.34ba/s]Running tokenizer on every text in dataset:  21%|██        | 1838/8956 [01:38<06:50, 17.35ba/s]Running tokenizer on every text in dataset:  21%|██        | 1841/8956 [01:38<06:28, 18.30ba/s]Running tokenizer on every text in dataset:  21%|██        | 1843/8956 [01:38<06:24, 18.49ba/s]Running tokenizer on every text in dataset:  21%|██        | 1845/8956 [01:38<06:21, 18.65ba/s]Running tokenizer on every text in dataset:  21%|██        | 1847/8956 [01:39<06:22, 18.60ba/s]Running tokenizer on every text in dataset:  21%|██        | 1849/8956 [01:39<06:45, 17.54ba/s]Running tokenizer on every text in dataset:  21%|██        | 1851/8956 [01:39<06:41, 17.70ba/s]Running tokenizer on every text in dataset:  21%|██        | 1853/8956 [01:39<06:37, 17.87ba/s]Running tokenizer on every text in dataset:  21%|██        | 1855/8956 [01:39<06:36, 17.90ba/s]Running tokenizer on every text in dataset:  21%|██        | 1857/8956 [01:39<07:02, 16.81ba/s]Running tokenizer on every text in dataset:  21%|██        | 1859/8956 [01:39<06:59, 16.90ba/s]Running tokenizer on every text in dataset:  21%|██        | 1861/8956 [01:39<06:51, 17.22ba/s]Running tokenizer on every text in dataset:  21%|██        | 1863/8956 [01:40<06:48, 17.37ba/s]Running tokenizer on every text in dataset:  21%|██        | 1865/8956 [01:40<06:36, 17.88ba/s]Running tokenizer on every text in dataset:  21%|██        | 1867/8956 [01:40<06:40, 17.70ba/s]Running tokenizer on every text in dataset:  21%|██        | 1870/8956 [01:40<06:21, 18.56ba/s]Running tokenizer on every text in dataset:  21%|██        | 1872/8956 [01:40<06:28, 18.23ba/s]Running tokenizer on every text in dataset:  21%|██        | 1874/8956 [01:40<06:39, 17.71ba/s]Running tokenizer on every text in dataset:  21%|██        | 1876/8956 [01:40<07:06, 16.60ba/s]Running tokenizer on every text in dataset:  21%|██        | 1878/8956 [01:40<07:04, 16.68ba/s]Running tokenizer on every text in dataset:  21%|██        | 1880/8956 [01:40<07:12, 16.36ba/s]Running tokenizer on every text in dataset:  21%|██        | 1882/8956 [01:41<06:59, 16.86ba/s]Running tokenizer on every text in dataset:  21%|██        | 1884/8956 [01:41<06:51, 17.18ba/s]Running tokenizer on every text in dataset:  21%|██        | 1886/8956 [01:41<07:47, 15.12ba/s]Running tokenizer on every text in dataset:  21%|██        | 1888/8956 [01:41<07:17, 16.14ba/s]Running tokenizer on every text in dataset:  21%|██        | 1891/8956 [01:41<06:40, 17.65ba/s]Running tokenizer on every text in dataset:  21%|██        | 1893/8956 [01:41<06:47, 17.34ba/s]Running tokenizer on every text in dataset:  21%|██        | 1895/8956 [01:41<07:09, 16.44ba/s]Running tokenizer on every text in dataset:  21%|██        | 1897/8956 [01:42<07:00, 16.77ba/s]Running tokenizer on every text in dataset:  21%|██        | 1899/8956 [01:42<06:54, 17.04ba/s]Running tokenizer on every text in dataset:  21%|██        | 1901/8956 [01:42<06:47, 17.30ba/s]Running tokenizer on every text in dataset:  21%|██        | 1903/8956 [01:42<06:40, 17.60ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1905/8956 [01:42<06:56, 16.92ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1907/8956 [01:42<06:38, 17.67ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1909/8956 [01:42<06:45, 17.38ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1911/8956 [01:42<06:39, 17.62ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1913/8956 [01:42<06:27, 18.16ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1915/8956 [01:43<06:56, 16.90ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1917/8956 [01:43<06:47, 17.29ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1920/8956 [01:43<06:26, 18.19ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1922/8956 [01:43<06:18, 18.59ba/s]Running tokenizer on every text in dataset:  21%|██▏       | 1924/8956 [01:43<06:46, 17.29ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1926/8956 [01:43<06:31, 17.93ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1928/8956 [01:43<06:27, 18.14ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1930/8956 [01:43<06:19, 18.49ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1932/8956 [01:43<06:17, 18.58ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1934/8956 [01:44<06:39, 17.59ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1936/8956 [01:44<06:31, 17.94ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1938/8956 [01:44<06:21, 18.42ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1940/8956 [01:44<06:17, 18.57ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1942/8956 [01:44<06:15, 18.68ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1944/8956 [01:44<06:43, 17.38ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1946/8956 [01:44<06:36, 17.66ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1948/8956 [01:44<06:38, 17.57ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1950/8956 [01:44<06:31, 17.88ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1952/8956 [01:45<07:06, 16.44ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1954/8956 [01:45<06:52, 16.99ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1956/8956 [01:45<06:40, 17.49ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1958/8956 [01:45<06:33, 17.81ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1960/8956 [01:45<06:29, 17.96ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1962/8956 [01:45<06:51, 16.98ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1964/8956 [01:45<06:40, 17.45ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1966/8956 [01:45<06:32, 17.82ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1968/8956 [01:45<06:25, 18.14ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1970/8956 [01:46<06:23, 18.23ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1972/8956 [01:46<07:15, 16.02ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1974/8956 [01:46<07:06, 16.37ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1976/8956 [01:46<06:47, 17.12ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1978/8956 [01:46<06:37, 17.54ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1980/8956 [01:46<06:24, 18.12ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1982/8956 [01:46<06:46, 17.16ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1985/8956 [01:46<06:22, 18.23ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1987/8956 [01:47<06:19, 18.34ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1990/8956 [01:47<06:27, 17.99ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1992/8956 [01:47<06:19, 18.33ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1994/8956 [01:47<06:21, 18.24ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1997/8956 [01:47<06:13, 18.65ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 1999/8956 [01:47<06:09, 18.81ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2001/8956 [01:47<06:34, 17.65ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2003/8956 [01:47<06:26, 18.00ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2005/8956 [01:48<06:36, 17.53ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2007/8956 [01:48<06:34, 17.60ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2009/8956 [01:48<06:52, 16.84ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2011/8956 [01:48<06:43, 17.19ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2013/8956 [01:48<06:38, 17.43ba/s]Running tokenizer on every text in dataset:  22%|██▏       | 2015/8956 [01:48<06:25, 18.03ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2017/8956 [01:48<06:24, 18.05ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2019/8956 [01:48<06:55, 16.71ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2022/8956 [01:49<05:48, 19.87ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2025/8956 [01:49<05:46, 20.03ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2028/8956 [01:49<06:19, 18.25ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2030/8956 [01:49<06:11, 18.63ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2032/8956 [01:49<06:07, 18.84ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2034/8956 [01:49<06:11, 18.64ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2036/8956 [01:49<06:06, 18.89ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2038/8956 [01:49<06:23, 18.04ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2040/8956 [01:49<06:22, 18.09ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2042/8956 [01:50<06:17, 18.30ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2044/8956 [01:50<06:16, 18.35ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2046/8956 [01:50<06:20, 18.18ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2048/8956 [01:50<06:38, 17.32ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2050/8956 [01:50<06:32, 17.61ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2052/8956 [01:50<06:30, 17.70ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2054/8956 [01:50<06:20, 18.13ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2056/8956 [01:50<06:19, 18.18ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2058/8956 [01:51<06:43, 17.10ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2060/8956 [01:51<06:29, 17.68ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2062/8956 [01:51<06:20, 18.13ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2064/8956 [01:51<06:20, 18.12ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2066/8956 [01:51<06:47, 16.92ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2068/8956 [01:51<06:41, 17.18ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2070/8956 [01:51<06:28, 17.74ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2072/8956 [01:51<06:20, 18.11ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2074/8956 [01:51<06:17, 18.22ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2076/8956 [01:52<06:51, 16.71ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2078/8956 [01:52<06:46, 16.93ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2080/8956 [01:52<06:39, 17.19ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2082/8956 [01:52<06:28, 17.68ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2084/8956 [01:52<06:20, 18.08ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2086/8956 [01:52<06:56, 16.49ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2088/8956 [01:52<06:37, 17.28ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2090/8956 [01:52<06:22, 17.96ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2092/8956 [01:52<06:24, 17.85ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2094/8956 [01:53<06:20, 18.05ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2096/8956 [01:53<06:41, 17.09ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2098/8956 [01:53<06:28, 17.67ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2100/8956 [01:53<06:14, 18.30ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2102/8956 [01:53<06:10, 18.47ba/s]Running tokenizer on every text in dataset:  23%|██▎       | 2104/8956 [01:53<06:37, 17.25ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2106/8956 [01:53<06:30, 17.52ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2108/8956 [01:53<06:28, 17.62ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2110/8956 [01:53<06:26, 17.72ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2112/8956 [01:54<06:29, 17.59ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2114/8956 [01:54<06:47, 16.79ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2116/8956 [01:54<06:36, 17.24ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2118/8956 [01:54<06:31, 17.48ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2120/8956 [01:54<06:21, 17.93ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2122/8956 [01:54<06:11, 18.38ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2124/8956 [01:54<06:39, 17.08ba/s]Running tokenizer on every text in dataset:  24%|██▎       | 2126/8956 [01:54<06:22, 17.86ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2128/8956 [01:54<06:12, 18.34ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2130/8956 [01:55<06:19, 17.97ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2132/8956 [01:55<06:11, 18.35ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2134/8956 [01:55<06:42, 16.96ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2137/8956 [01:55<06:21, 17.88ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2140/8956 [01:55<05:57, 19.09ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2142/8956 [01:55<06:12, 18.29ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2144/8956 [01:55<06:12, 18.28ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2146/8956 [01:55<06:14, 18.18ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2148/8956 [01:56<06:16, 18.10ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2150/8956 [01:56<06:08, 18.45ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2152/8956 [01:56<06:36, 17.17ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2154/8956 [01:56<06:33, 17.28ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2156/8956 [01:56<06:24, 17.68ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2158/8956 [01:56<06:20, 17.86ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2160/8956 [01:56<06:24, 17.67ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2162/8956 [01:56<06:47, 16.67ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2164/8956 [01:57<06:38, 17.03ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2166/8956 [01:57<06:29, 17.42ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2168/8956 [01:57<06:23, 17.70ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2170/8956 [01:57<06:21, 17.79ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2172/8956 [01:57<06:40, 16.96ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2174/8956 [01:57<06:33, 17.24ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2176/8956 [01:57<06:24, 17.63ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2178/8956 [01:57<06:22, 17.72ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2180/8956 [01:57<07:01, 16.09ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2182/8956 [01:58<06:51, 16.45ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2184/8956 [01:58<06:31, 17.28ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2186/8956 [01:58<06:49, 16.54ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2188/8956 [01:58<06:46, 16.64ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2190/8956 [01:58<07:00, 16.10ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2192/8956 [01:58<06:49, 16.53ba/s]Running tokenizer on every text in dataset:  24%|██▍       | 2194/8956 [01:58<06:41, 16.85ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2196/8956 [01:58<06:44, 16.73ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2199/8956 [01:59<06:43, 16.76ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2201/8956 [01:59<06:35, 17.07ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2204/8956 [01:59<06:01, 18.70ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2206/8956 [01:59<06:06, 18.42ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2208/8956 [01:59<06:05, 18.45ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2210/8956 [01:59<06:26, 17.44ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2212/8956 [01:59<06:20, 17.71ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2214/8956 [01:59<06:20, 17.70ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2216/8956 [02:00<06:18, 17.81ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2218/8956 [02:00<06:37, 16.96ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2220/8956 [02:00<06:29, 17.27ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2222/8956 [02:00<06:23, 17.54ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2224/8956 [02:00<06:22, 17.60ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2226/8956 [02:00<06:21, 17.63ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2228/8956 [02:00<06:35, 17.03ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2230/8956 [02:00<06:19, 17.71ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2232/8956 [02:00<06:24, 17.50ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2234/8956 [02:01<06:19, 17.72ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2236/8956 [02:01<06:13, 17.98ba/s]Running tokenizer on every text in dataset:  25%|██▍       | 2238/8956 [02:01<06:24, 17.45ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2240/8956 [02:01<06:52, 16.30ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2242/8956 [02:01<06:43, 16.66ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2244/8956 [02:01<06:26, 17.37ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2246/8956 [02:01<07:02, 15.86ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2248/8956 [02:01<07:07, 15.70ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2251/8956 [02:02<06:35, 16.94ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2253/8956 [02:02<06:48, 16.40ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2255/8956 [02:02<06:34, 16.99ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2257/8956 [02:02<06:42, 16.65ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2259/8956 [02:02<06:25, 17.36ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2261/8956 [02:02<06:12, 17.95ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2264/8956 [02:02<05:54, 18.88ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2266/8956 [02:02<06:14, 17.88ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2268/8956 [02:03<06:06, 18.24ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2270/8956 [02:03<06:17, 17.70ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2273/8956 [02:03<05:52, 18.97ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2275/8956 [02:03<06:11, 18.01ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2278/8956 [02:03<06:28, 17.18ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2280/8956 [02:03<06:16, 17.72ba/s]Running tokenizer on every text in dataset:  25%|██▌       | 2282/8956 [02:03<06:07, 18.14ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2284/8956 [02:03<06:01, 18.47ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2286/8956 [02:04<06:13, 17.85ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2288/8956 [02:04<06:05, 18.25ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2290/8956 [02:04<05:58, 18.59ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2292/8956 [02:04<05:55, 18.74ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2294/8956 [02:04<06:13, 17.84ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2297/8956 [02:04<05:47, 19.16ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2299/8956 [02:04<05:43, 19.37ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2302/8956 [02:04<05:39, 19.60ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2304/8956 [02:04<06:00, 18.47ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2306/8956 [02:05<05:54, 18.77ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2308/8956 [02:05<05:48, 19.07ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2310/8956 [02:05<05:50, 18.94ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2312/8956 [02:05<05:47, 19.13ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2314/8956 [02:05<06:06, 18.14ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2316/8956 [02:05<06:03, 18.29ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2318/8956 [02:05<05:54, 18.73ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2321/8956 [02:05<05:38, 19.61ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2323/8956 [02:05<06:01, 18.36ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2325/8956 [02:06<06:07, 18.04ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2327/8956 [02:06<06:14, 17.70ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2329/8956 [02:06<06:02, 18.29ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2332/8956 [02:06<06:14, 17.68ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2334/8956 [02:06<06:04, 18.16ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2336/8956 [02:06<06:01, 18.31ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2338/8956 [02:06<05:52, 18.75ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2341/8956 [02:06<05:48, 18.96ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2343/8956 [02:07<06:05, 18.10ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2345/8956 [02:07<06:00, 18.35ba/s]Running tokenizer on every text in dataset:  26%|██▌       | 2348/8956 [02:07<05:43, 19.23ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2351/8956 [02:07<05:52, 18.71ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2353/8956 [02:07<05:51, 18.77ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2355/8956 [02:07<05:51, 18.78ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2357/8956 [02:07<05:49, 18.86ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2359/8956 [02:07<05:51, 18.76ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2361/8956 [02:08<06:08, 17.91ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2363/8956 [02:08<06:01, 18.22ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2365/8956 [02:08<05:58, 18.39ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2367/8956 [02:08<05:52, 18.68ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2370/8956 [02:08<05:51, 18.71ba/s]Running tokenizer on every text in dataset:  26%|██▋       | 2373/8956 [02:08<05:43, 19.15ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2375/8956 [02:08<05:45, 19.06ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2377/8956 [02:08<05:46, 19.00ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2380/8956 [02:09<05:56, 18.42ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2383/8956 [02:09<05:46, 18.99ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2385/8956 [02:09<05:50, 18.77ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2387/8956 [02:09<05:47, 18.93ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2389/8956 [02:09<06:14, 17.55ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2391/8956 [02:09<06:02, 18.09ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2393/8956 [02:09<05:54, 18.52ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2396/8956 [02:09<05:34, 19.59ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2398/8956 [02:10<05:39, 19.33ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2400/8956 [02:10<05:57, 18.32ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2402/8956 [02:10<05:50, 18.69ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2404/8956 [02:10<05:49, 18.75ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2406/8956 [02:10<05:50, 18.68ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2408/8956 [02:10<05:55, 18.42ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2411/8956 [02:10<05:45, 18.93ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2413/8956 [02:10<05:41, 19.15ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2415/8956 [02:10<05:43, 19.06ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2417/8956 [02:11<05:39, 19.27ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2419/8956 [02:11<06:02, 18.02ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2421/8956 [02:11<05:58, 18.21ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2423/8956 [02:11<05:56, 18.35ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2426/8956 [02:11<05:42, 19.06ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2428/8956 [02:11<06:00, 18.11ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2430/8956 [02:11<05:54, 18.43ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2432/8956 [02:11<05:46, 18.83ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2434/8956 [02:11<05:43, 19.01ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2436/8956 [02:12<05:46, 18.82ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2438/8956 [02:12<06:07, 17.75ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2440/8956 [02:12<05:56, 18.28ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2442/8956 [02:12<05:53, 18.45ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2444/8956 [02:12<05:45, 18.86ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2446/8956 [02:12<06:04, 17.86ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2448/8956 [02:12<05:53, 18.39ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2450/8956 [02:12<05:54, 18.36ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2453/8956 [02:12<05:26, 19.89ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2456/8956 [02:13<05:26, 19.89ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2459/8956 [02:13<05:10, 20.89ba/s]Running tokenizer on every text in dataset:  27%|██▋       | 2462/8956 [02:13<05:02, 21.46ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2465/8956 [02:13<05:12, 20.78ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2468/8956 [02:13<05:04, 21.29ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2471/8956 [02:13<04:54, 22.03ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2474/8956 [02:13<04:49, 22.41ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2477/8956 [02:14<04:55, 21.90ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2480/8956 [02:14<04:51, 22.24ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2483/8956 [02:14<04:46, 22.57ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2486/8956 [02:14<05:06, 21.14ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2489/8956 [02:14<05:00, 21.50ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2492/8956 [02:14<04:50, 22.21ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2495/8956 [02:14<05:05, 21.14ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2498/8956 [02:15<04:59, 21.56ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2503/8956 [02:15<04:11, 25.63ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2506/8956 [02:15<04:21, 24.68ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2509/8956 [02:15<04:24, 24.42ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2512/8956 [02:15<04:23, 24.44ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2515/8956 [02:15<04:38, 23.11ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2518/8956 [02:15<04:36, 23.31ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2521/8956 [02:15<04:30, 23.77ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2524/8956 [02:16<04:44, 22.64ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2527/8956 [02:16<04:40, 22.88ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2530/8956 [02:16<04:40, 22.92ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2533/8956 [02:16<04:54, 21.82ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2536/8956 [02:16<04:49, 22.17ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2539/8956 [02:16<04:48, 22.24ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2542/8956 [02:16<04:55, 21.71ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2545/8956 [02:17<04:50, 22.09ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2548/8956 [02:17<04:49, 22.16ba/s]Running tokenizer on every text in dataset:  28%|██▊       | 2551/8956 [02:17<04:57, 21.50ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2554/8956 [02:17<04:50, 22.00ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2557/8956 [02:17<04:48, 22.19ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2560/8956 [02:17<05:01, 21.23ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2563/8956 [02:17<04:54, 21.73ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2566/8956 [02:18<04:46, 22.28ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2569/8956 [02:18<04:43, 22.51ba/s]Running tokenizer on every text in dataset:  29%|██▊       | 2572/8956 [02:18<04:52, 21.81ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2575/8956 [02:18<04:42, 22.55ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2578/8956 [02:18<04:40, 22.70ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2581/8956 [02:18<04:53, 21.73ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2584/8956 [02:18<04:49, 21.99ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2587/8956 [02:18<04:44, 22.42ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2590/8956 [02:19<04:56, 21.45ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2593/8956 [02:19<04:49, 21.99ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2596/8956 [02:19<04:45, 22.25ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2599/8956 [02:19<04:53, 21.66ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2602/8956 [02:19<04:49, 21.92ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2605/8956 [02:19<04:42, 22.51ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2608/8956 [02:19<04:55, 21.51ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2611/8956 [02:20<04:44, 22.28ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2614/8956 [02:20<04:35, 22.99ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2617/8956 [02:20<04:46, 22.12ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2620/8956 [02:20<04:42, 22.44ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2623/8956 [02:20<04:39, 22.67ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2626/8956 [02:20<04:36, 22.88ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2629/8956 [02:20<04:55, 21.44ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2632/8956 [02:21<04:51, 21.67ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2635/8956 [02:21<04:36, 22.83ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2638/8956 [02:21<04:51, 21.68ba/s]Running tokenizer on every text in dataset:  29%|██▉       | 2641/8956 [02:21<04:39, 22.61ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2644/8956 [02:21<04:35, 22.95ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2647/8956 [02:21<04:45, 22.10ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2650/8956 [02:21<04:38, 22.64ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2653/8956 [02:21<04:31, 23.21ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2656/8956 [02:22<05:04, 20.67ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2659/8956 [02:22<04:51, 21.59ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2662/8956 [02:22<04:44, 22.11ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2665/8956 [02:22<04:49, 21.73ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2668/8956 [02:22<04:41, 22.34ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2671/8956 [02:22<04:39, 22.47ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2674/8956 [02:22<04:52, 21.46ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2677/8956 [02:23<04:43, 22.18ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2680/8956 [02:23<04:35, 22.81ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2683/8956 [02:23<04:32, 23.05ba/s]Running tokenizer on every text in dataset:  30%|██▉       | 2686/8956 [02:23<04:57, 21.10ba/s]Running tokenizer on every text in dataset:  30%|███       | 2689/8956 [02:23<04:48, 21.71ba/s]Running tokenizer on every text in dataset:  30%|███       | 2692/8956 [02:23<04:40, 22.29ba/s]Running tokenizer on every text in dataset:  30%|███       | 2695/8956 [02:23<05:37, 18.56ba/s]Running tokenizer on every text in dataset:  30%|███       | 2698/8956 [02:24<05:16, 19.78ba/s]Running tokenizer on every text in dataset:  30%|███       | 2701/8956 [02:24<05:00, 20.85ba/s]Running tokenizer on every text in dataset:  30%|███       | 2704/8956 [02:24<05:46, 18.06ba/s]Running tokenizer on every text in dataset:  30%|███       | 2707/8956 [02:24<05:21, 19.44ba/s]Running tokenizer on every text in dataset:  30%|███       | 2710/8956 [02:24<05:03, 20.57ba/s]Running tokenizer on every text in dataset:  30%|███       | 2713/8956 [02:24<05:02, 20.66ba/s]Running tokenizer on every text in dataset:  30%|███       | 2716/8956 [02:24<04:55, 21.15ba/s]Running tokenizer on every text in dataset:  30%|███       | 2719/8956 [02:25<04:46, 21.75ba/s]Running tokenizer on every text in dataset:  30%|███       | 2722/8956 [02:25<04:56, 21.01ba/s]Running tokenizer on every text in dataset:  30%|███       | 2725/8956 [02:25<04:46, 21.72ba/s]Running tokenizer on every text in dataset:  30%|███       | 2728/8956 [02:25<04:42, 22.03ba/s]Running tokenizer on every text in dataset:  30%|███       | 2731/8956 [02:25<05:09, 20.12ba/s]Running tokenizer on every text in dataset:  31%|███       | 2734/8956 [02:25<05:00, 20.73ba/s]Running tokenizer on every text in dataset:  31%|███       | 2737/8956 [02:25<04:47, 21.61ba/s]Running tokenizer on every text in dataset:  31%|███       | 2740/8956 [02:26<04:48, 21.55ba/s]Running tokenizer on every text in dataset:  31%|███       | 2743/8956 [02:26<04:51, 21.33ba/s]Running tokenizer on every text in dataset:  31%|███       | 2746/8956 [02:26<04:42, 22.01ba/s]Running tokenizer on every text in dataset:  31%|███       | 2749/8956 [02:26<04:35, 22.53ba/s]Running tokenizer on every text in dataset:  31%|███       | 2752/8956 [02:26<04:47, 21.60ba/s]Running tokenizer on every text in dataset:  31%|███       | 2755/8956 [02:26<04:45, 21.69ba/s]Running tokenizer on every text in dataset:  31%|███       | 2758/8956 [02:26<04:39, 22.14ba/s]Running tokenizer on every text in dataset:  31%|███       | 2761/8956 [02:27<05:05, 20.30ba/s]Running tokenizer on every text in dataset:  31%|███       | 2764/8956 [02:27<04:54, 21.03ba/s]Running tokenizer on every text in dataset:  31%|███       | 2767/8956 [02:27<04:45, 21.65ba/s]Running tokenizer on every text in dataset:  31%|███       | 2770/8956 [02:27<04:54, 21.03ba/s]Running tokenizer on every text in dataset:  31%|███       | 2773/8956 [02:27<04:41, 21.94ba/s]Running tokenizer on every text in dataset:  31%|███       | 2776/8956 [02:27<04:35, 22.47ba/s]Running tokenizer on every text in dataset:  31%|███       | 2779/8956 [02:27<04:48, 21.40ba/s]Running tokenizer on every text in dataset:  31%|███       | 2782/8956 [02:27<04:45, 21.64ba/s]Running tokenizer on every text in dataset:  31%|███       | 2785/8956 [02:28<05:18, 19.35ba/s]Running tokenizer on every text in dataset:  31%|███       | 2788/8956 [02:28<05:18, 19.34ba/s]Running tokenizer on every text in dataset:  31%|███       | 2791/8956 [02:28<05:04, 20.26ba/s]Running tokenizer on every text in dataset:  31%|███       | 2794/8956 [02:28<05:10, 19.84ba/s]Running tokenizer on every text in dataset:  31%|███       | 2797/8956 [02:28<04:55, 20.84ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2800/8956 [02:28<05:01, 20.45ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2803/8956 [02:29<04:50, 21.20ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2806/8956 [02:29<05:00, 20.49ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2809/8956 [02:29<05:02, 20.32ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2812/8956 [02:29<04:50, 21.13ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2815/8956 [02:29<04:46, 21.43ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2818/8956 [02:29<04:50, 21.14ba/s]Running tokenizer on every text in dataset:  31%|███▏      | 2821/8956 [02:29<04:41, 21.76ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2824/8956 [02:30<04:39, 21.92ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2827/8956 [02:30<04:51, 21.06ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2830/8956 [02:30<04:45, 21.43ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2833/8956 [02:30<04:40, 21.82ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2836/8956 [02:30<04:48, 21.21ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2839/8956 [02:30<04:40, 21.84ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2842/8956 [02:30<04:32, 22.46ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2845/8956 [02:30<04:42, 21.61ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2848/8956 [02:31<04:35, 22.14ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2851/8956 [02:31<04:34, 22.22ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2854/8956 [02:31<04:33, 22.34ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2857/8956 [02:31<04:44, 21.44ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2860/8956 [02:31<04:35, 22.09ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2863/8956 [02:31<04:28, 22.67ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2866/8956 [02:31<04:43, 21.49ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2869/8956 [02:32<04:36, 21.99ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2872/8956 [02:32<04:34, 22.18ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2875/8956 [02:32<04:44, 21.34ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2878/8956 [02:32<04:38, 21.84ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2881/8956 [02:32<04:32, 22.31ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2884/8956 [02:32<04:45, 21.27ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2887/8956 [02:32<04:40, 21.64ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2890/8956 [02:33<04:33, 22.19ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2893/8956 [02:33<04:43, 21.35ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2896/8956 [02:33<04:40, 21.63ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2899/8956 [02:33<04:32, 22.23ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2902/8956 [02:33<04:39, 21.67ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2905/8956 [02:33<04:31, 22.30ba/s]Running tokenizer on every text in dataset:  32%|███▏      | 2908/8956 [02:33<04:16, 23.58ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2911/8956 [02:33<04:15, 23.62ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2914/8956 [02:34<04:31, 22.21ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2917/8956 [02:34<04:26, 22.68ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2920/8956 [02:34<04:24, 22.79ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2923/8956 [02:34<04:37, 21.71ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2926/8956 [02:34<04:28, 22.43ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2929/8956 [02:34<04:24, 22.81ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2932/8956 [02:34<04:44, 21.17ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2935/8956 [02:35<04:40, 21.48ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2938/8956 [02:35<04:44, 21.16ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2941/8956 [02:35<05:04, 19.74ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2944/8956 [02:35<04:55, 20.33ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2947/8956 [02:35<04:58, 20.15ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2950/8956 [02:35<05:15, 19.02ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2952/8956 [02:35<05:15, 19.05ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2954/8956 [02:36<05:12, 19.22ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2957/8956 [02:36<05:02, 19.84ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2959/8956 [02:36<05:13, 19.11ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2962/8956 [02:36<05:02, 19.81ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2965/8956 [02:36<05:01, 19.85ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2968/8956 [02:36<05:01, 19.88ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2970/8956 [02:36<05:16, 18.90ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2972/8956 [02:36<05:13, 19.08ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2974/8956 [02:37<05:12, 19.12ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2977/8956 [02:37<04:53, 20.37ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2980/8956 [02:37<04:56, 20.15ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2983/8956 [02:37<04:49, 20.62ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2986/8956 [02:37<04:44, 21.02ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2989/8956 [02:37<05:00, 19.87ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2992/8956 [02:37<04:52, 20.36ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2995/8956 [02:38<04:48, 20.69ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 2998/8956 [02:38<05:01, 19.78ba/s]Running tokenizer on every text in dataset:  33%|███▎      | 3000/8956 [02:38<05:02, 19.66ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3003/8956 [02:38<04:59, 19.88ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3005/8956 [02:38<05:04, 19.56ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3007/8956 [02:38<05:23, 18.38ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3010/8956 [02:38<05:14, 18.91ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3013/8956 [02:39<05:10, 19.14ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3015/8956 [02:39<05:14, 18.90ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3017/8956 [02:39<05:31, 17.91ba/s]Running tokenizer on every text in dataset:  34%|███▎      | 3020/8956 [02:39<05:20, 18.54ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3023/8956 [02:39<05:07, 19.30ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3026/8956 [02:39<05:17, 18.66ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3028/8956 [02:39<05:18, 18.59ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3030/8956 [02:39<05:13, 18.88ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3032/8956 [02:40<05:14, 18.82ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3034/8956 [02:40<05:11, 19.00ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3036/8956 [02:40<05:29, 17.96ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3038/8956 [02:40<05:26, 18.12ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3040/8956 [02:40<05:29, 17.95ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3042/8956 [02:40<05:31, 17.84ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3044/8956 [02:40<05:30, 17.87ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3046/8956 [02:40<05:44, 17.15ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3048/8956 [02:41<05:40, 17.35ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3051/8956 [02:41<05:18, 18.52ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3053/8956 [02:41<05:14, 18.74ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3055/8956 [02:41<05:24, 18.20ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3058/8956 [02:41<05:04, 19.34ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3061/8956 [02:41<04:56, 19.86ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3064/8956 [02:41<05:10, 18.95ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3067/8956 [02:41<05:03, 19.41ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3070/8956 [02:42<04:51, 20.21ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3073/8956 [02:42<05:07, 19.14ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3076/8956 [02:42<04:58, 19.70ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3078/8956 [02:42<04:59, 19.60ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3081/8956 [02:42<04:55, 19.87ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3083/8956 [02:42<05:12, 18.77ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3085/8956 [02:42<05:08, 19.03ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3087/8956 [02:43<05:06, 19.13ba/s]Running tokenizer on every text in dataset:  34%|███▍      | 3089/8956 [02:43<05:04, 19.24ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3092/8956 [02:43<05:16, 18.53ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3095/8956 [02:43<05:04, 19.28ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3098/8956 [02:43<04:50, 20.13ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3101/8956 [02:43<04:40, 20.89ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3104/8956 [02:43<04:51, 20.09ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3107/8956 [02:43<04:45, 20.48ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3110/8956 [02:44<04:42, 20.70ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3113/8956 [02:44<04:53, 19.89ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3116/8956 [02:44<04:46, 20.41ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3119/8956 [02:44<04:42, 20.66ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3122/8956 [02:44<04:50, 20.09ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3125/8956 [02:44<04:45, 20.44ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3128/8956 [02:45<04:42, 20.67ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3131/8956 [02:45<04:50, 20.04ba/s]Running tokenizer on every text in dataset:  35%|███▍      | 3134/8956 [02:45<04:44, 20.45ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3137/8956 [02:45<04:43, 20.54ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3140/8956 [02:45<04:53, 19.83ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3144/8956 [02:45<04:15, 22.74ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3148/8956 [02:45<03:49, 25.30ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3151/8956 [02:46<03:53, 24.83ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3154/8956 [02:46<03:53, 24.87ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3157/8956 [02:46<03:43, 25.90ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3160/8956 [02:46<03:47, 25.52ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3163/8956 [02:46<03:47, 25.43ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3166/8956 [02:46<04:16, 22.61ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3169/8956 [02:46<04:45, 20.28ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3172/8956 [02:46<04:50, 19.91ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3175/8956 [02:47<04:31, 21.32ba/s]Running tokenizer on every text in dataset:  35%|███▌      | 3178/8956 [02:47<04:42, 20.43ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3181/8956 [02:47<04:16, 22.51ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3185/8956 [02:47<03:52, 24.86ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3188/8956 [02:47<03:49, 25.15ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3191/8956 [02:47<03:48, 25.19ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3195/8956 [02:47<03:33, 26.96ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3198/8956 [02:47<03:40, 26.12ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3201/8956 [02:48<03:45, 25.48ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3204/8956 [02:48<03:40, 26.04ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3207/8956 [02:48<03:59, 24.01ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3210/8956 [02:48<04:07, 23.21ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3213/8956 [02:48<04:32, 21.08ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3216/8956 [02:48<04:49, 19.86ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3219/8956 [02:48<04:40, 20.42ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3222/8956 [02:49<04:36, 20.75ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3225/8956 [02:49<04:49, 19.77ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3228/8956 [02:49<04:48, 19.84ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3231/8956 [02:49<04:57, 19.24ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3234/8956 [02:49<04:53, 19.49ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3236/8956 [02:49<05:06, 18.66ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3238/8956 [02:49<05:08, 18.52ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3240/8956 [02:50<05:03, 18.85ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3243/8956 [02:50<04:54, 19.38ba/s]Running tokenizer on every text in dataset:  36%|███▌      | 3245/8956 [02:50<05:57, 16.00ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3247/8956 [02:50<05:39, 16.81ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3250/8956 [02:50<05:17, 17.99ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3253/8956 [02:50<05:28, 17.38ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3255/8956 [02:50<05:34, 17.06ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3258/8956 [02:51<05:17, 17.94ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3260/8956 [02:51<05:10, 18.37ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3262/8956 [02:51<05:32, 17.13ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3264/8956 [02:51<05:39, 16.74ba/s]Running tokenizer on every text in dataset:  36%|███▋      | 3266/8956 [02:51<05:26, 17.44ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3269/8956 [02:51<05:12, 18.21ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3271/8956 [02:51<05:06, 18.56ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3273/8956 [02:51<05:21, 17.66ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3275/8956 [02:52<05:11, 18.24ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3277/8956 [02:52<05:05, 18.56ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3279/8956 [02:52<05:00, 18.87ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3281/8956 [02:52<04:57, 19.05ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3283/8956 [02:52<05:11, 18.24ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3285/8956 [02:52<05:13, 18.09ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3288/8956 [02:52<04:54, 19.23ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3291/8956 [02:52<04:46, 19.79ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3293/8956 [02:53<05:03, 18.65ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3295/8956 [02:53<04:58, 18.96ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3298/8956 [02:53<04:50, 19.51ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3300/8956 [02:53<04:50, 19.45ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3302/8956 [02:53<05:11, 18.16ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3305/8956 [02:53<04:50, 19.46ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3308/8956 [02:53<04:43, 19.89ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3311/8956 [02:53<04:55, 19.08ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3314/8956 [02:54<04:44, 19.80ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3318/8956 [02:54<04:13, 22.28ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3321/8956 [02:54<04:29, 20.90ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3324/8956 [02:54<04:25, 21.18ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3327/8956 [02:54<04:31, 20.76ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3330/8956 [02:54<04:48, 19.49ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3333/8956 [02:55<04:46, 19.60ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3336/8956 [02:55<04:41, 19.95ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3339/8956 [02:55<04:53, 19.12ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3342/8956 [02:55<04:44, 19.73ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3345/8956 [02:55<04:34, 20.40ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3348/8956 [02:55<04:31, 20.62ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3351/8956 [02:55<04:41, 19.94ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3354/8956 [02:56<04:37, 20.22ba/s]Running tokenizer on every text in dataset:  37%|███▋      | 3357/8956 [02:56<04:36, 20.22ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3360/8956 [02:56<04:47, 19.49ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3363/8956 [02:56<04:40, 19.91ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3366/8956 [02:56<04:38, 20.10ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3369/8956 [02:56<04:50, 19.25ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3372/8956 [02:56<04:44, 19.61ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3374/8956 [02:57<04:45, 19.53ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3376/8956 [02:57<04:44, 19.62ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3378/8956 [02:57<05:00, 18.56ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3380/8956 [02:57<04:56, 18.79ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3382/8956 [02:57<04:55, 18.87ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3384/8956 [02:57<04:51, 19.11ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3386/8956 [02:57<04:49, 19.21ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3388/8956 [02:57<05:10, 17.91ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3390/8956 [02:57<05:05, 18.19ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3392/8956 [02:58<05:15, 17.63ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3394/8956 [02:58<05:08, 18.02ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3396/8956 [02:58<05:26, 17.05ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3399/8956 [02:58<05:05, 18.20ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3401/8956 [02:58<05:00, 18.49ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3404/8956 [02:58<04:44, 19.49ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3406/8956 [02:58<04:55, 18.78ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3409/8956 [02:58<04:40, 19.78ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3412/8956 [02:59<04:32, 20.34ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3415/8956 [02:59<04:40, 19.75ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3418/8956 [02:59<04:35, 20.11ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3421/8956 [02:59<04:27, 20.67ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3424/8956 [02:59<04:22, 21.05ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3427/8956 [02:59<04:31, 20.37ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3430/8956 [02:59<04:30, 20.46ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3433/8956 [03:00<04:39, 19.79ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3435/8956 [03:00<04:51, 18.92ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3438/8956 [03:00<04:40, 19.69ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3441/8956 [03:00<04:27, 20.62ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3444/8956 [03:00<04:35, 20.00ba/s]Running tokenizer on every text in dataset:  38%|███▊      | 3447/8956 [03:00<04:31, 20.28ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3450/8956 [03:00<04:29, 20.42ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3453/8956 [03:01<04:47, 19.15ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3455/8956 [03:01<04:48, 19.04ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3457/8956 [03:01<04:50, 18.91ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3459/8956 [03:01<04:48, 19.07ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3462/8956 [03:01<04:39, 19.64ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3464/8956 [03:01<04:57, 18.45ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3466/8956 [03:01<04:51, 18.81ba/s]Running tokenizer on every text in dataset:  39%|███▊      | 3469/8956 [03:02<04:41, 19.51ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3472/8956 [03:02<04:51, 18.83ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3475/8956 [03:02<04:42, 19.39ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3478/8956 [03:02<04:32, 20.09ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3481/8956 [03:02<04:27, 20.50ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3484/8956 [03:02<04:31, 20.14ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3487/8956 [03:02<04:23, 20.75ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3490/8956 [03:03<04:12, 21.61ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3493/8956 [03:03<04:25, 20.55ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3496/8956 [03:03<04:21, 20.92ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3499/8956 [03:03<04:18, 21.08ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3502/8956 [03:03<04:28, 20.29ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3505/8956 [03:03<04:19, 20.99ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3508/8956 [03:03<04:14, 21.44ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3511/8956 [03:04<04:22, 20.76ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3514/8956 [03:04<04:17, 21.10ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3517/8956 [03:04<04:12, 21.52ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3520/8956 [03:04<04:24, 20.59ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3523/8956 [03:04<04:21, 20.76ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3526/8956 [03:04<04:27, 20.26ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3529/8956 [03:04<05:04, 17.82ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3531/8956 [03:05<05:00, 18.04ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3534/8956 [03:05<04:48, 18.81ba/s]Running tokenizer on every text in dataset:  39%|███▉      | 3536/8956 [03:05<04:45, 18.96ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3539/8956 [03:05<04:56, 18.28ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3542/8956 [03:05<04:45, 18.97ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3545/8956 [03:05<04:39, 19.39ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3548/8956 [03:05<04:58, 18.12ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3551/8956 [03:06<04:42, 19.12ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3554/8956 [03:06<04:31, 19.91ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3557/8956 [03:06<04:21, 20.63ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3560/8956 [03:06<04:28, 20.08ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3563/8956 [03:06<04:26, 20.20ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3566/8956 [03:06<04:22, 20.55ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3569/8956 [03:07<04:33, 19.72ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3572/8956 [03:07<04:29, 19.97ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3575/8956 [03:07<04:22, 20.52ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3578/8956 [03:07<04:33, 19.63ba/s]Running tokenizer on every text in dataset:  40%|███▉      | 3580/8956 [03:07<04:34, 19.56ba/s]Running tokenizer on every text in dataset:  40%|████      | 3583/8956 [03:07<04:28, 20.01ba/s]Running tokenizer on every text in dataset:  40%|████      | 3586/8956 [03:07<04:49, 18.57ba/s]Running tokenizer on every text in dataset:  40%|████      | 3589/8956 [03:08<04:33, 19.63ba/s]Running tokenizer on every text in dataset:  40%|████      | 3592/8956 [03:08<04:28, 20.01ba/s]Running tokenizer on every text in dataset:  40%|████      | 3595/8956 [03:08<04:21, 20.51ba/s]Running tokenizer on every text in dataset:  40%|████      | 3598/8956 [03:08<04:33, 19.62ba/s]Running tokenizer on every text in dataset:  40%|████      | 3601/8956 [03:08<04:27, 20.01ba/s]Running tokenizer on every text in dataset:  40%|████      | 3604/8956 [03:08<04:20, 20.54ba/s]Running tokenizer on every text in dataset:  40%|████      | 3607/8956 [03:08<04:26, 20.10ba/s]Running tokenizer on every text in dataset:  40%|████      | 3610/8956 [03:09<04:29, 19.85ba/s]Running tokenizer on every text in dataset:  40%|████      | 3613/8956 [03:09<04:21, 20.46ba/s]Running tokenizer on every text in dataset:  40%|████      | 3616/8956 [03:09<04:31, 19.66ba/s]Running tokenizer on every text in dataset:  40%|████      | 3618/8956 [03:09<04:44, 18.76ba/s]Running tokenizer on every text in dataset:  40%|████      | 3621/8956 [03:09<04:36, 19.32ba/s]Running tokenizer on every text in dataset:  40%|████      | 3624/8956 [03:09<04:41, 18.95ba/s]Running tokenizer on every text in dataset:  40%|████      | 3626/8956 [03:09<05:20, 16.65ba/s]Running tokenizer on every text in dataset:  41%|████      | 3629/8956 [03:10<04:52, 18.18ba/s]Running tokenizer on every text in dataset:  41%|████      | 3632/8956 [03:10<04:33, 19.49ba/s]Running tokenizer on every text in dataset:  41%|████      | 3634/8956 [03:10<04:37, 19.17ba/s]Running tokenizer on every text in dataset:  41%|████      | 3637/8956 [03:10<04:27, 19.90ba/s]Running tokenizer on every text in dataset:  41%|████      | 3640/8956 [03:10<04:21, 20.35ba/s]Running tokenizer on every text in dataset:  41%|████      | 3643/8956 [03:10<04:32, 19.49ba/s]Running tokenizer on every text in dataset:  41%|████      | 3646/8956 [03:10<04:27, 19.87ba/s]Running tokenizer on every text in dataset:  41%|████      | 3649/8956 [03:11<04:23, 20.11ba/s]Running tokenizer on every text in dataset:  41%|████      | 3652/8956 [03:11<04:17, 20.60ba/s]Running tokenizer on every text in dataset:  41%|████      | 3655/8956 [03:11<04:23, 20.10ba/s]Running tokenizer on every text in dataset:  41%|████      | 3658/8956 [03:11<04:17, 20.54ba/s]Running tokenizer on every text in dataset:  41%|████      | 3661/8956 [03:11<04:23, 20.06ba/s]Running tokenizer on every text in dataset:  41%|████      | 3664/8956 [03:11<04:27, 19.76ba/s]Running tokenizer on every text in dataset:  41%|████      | 3667/8956 [03:11<04:17, 20.57ba/s]Running tokenizer on every text in dataset:  41%|████      | 3670/8956 [03:12<05:11, 16.96ba/s]Running tokenizer on every text in dataset:  41%|████      | 3672/8956 [03:12<05:07, 17.17ba/s]Running tokenizer on every text in dataset:  41%|████      | 3675/8956 [03:12<04:43, 18.63ba/s]Running tokenizer on every text in dataset:  41%|████      | 3678/8956 [03:12<04:32, 19.39ba/s]Running tokenizer on every text in dataset:  41%|████      | 3680/8956 [03:12<05:02, 17.44ba/s]Running tokenizer on every text in dataset:  41%|████      | 3682/8956 [03:12<05:03, 17.36ba/s]Running tokenizer on every text in dataset:  41%|████      | 3685/8956 [03:13<04:41, 18.71ba/s]Running tokenizer on every text in dataset:  41%|████      | 3688/8956 [03:13<04:26, 19.74ba/s]Running tokenizer on every text in dataset:  41%|████      | 3690/8956 [03:13<04:27, 19.72ba/s]Running tokenizer on every text in dataset:  41%|████      | 3692/8956 [03:13<04:41, 18.69ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3695/8956 [03:13<04:30, 19.44ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3698/8956 [03:13<04:26, 19.71ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3700/8956 [03:13<04:36, 19.00ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3703/8956 [03:13<04:22, 20.00ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3706/8956 [03:14<04:13, 20.71ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3709/8956 [03:14<04:07, 21.23ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3712/8956 [03:14<04:20, 20.12ba/s]Running tokenizer on every text in dataset:  41%|████▏     | 3715/8956 [03:14<04:16, 20.42ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3718/8956 [03:14<04:07, 21.20ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3721/8956 [03:14<04:08, 21.08ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3724/8956 [03:14<03:54, 22.31ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3727/8956 [03:15<03:47, 23.02ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3730/8956 [03:15<03:56, 22.05ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3733/8956 [03:15<03:55, 22.18ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3736/8956 [03:15<03:57, 22.01ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3739/8956 [03:15<04:07, 21.06ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3742/8956 [03:15<03:59, 21.76ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3745/8956 [03:15<03:54, 22.22ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3748/8956 [03:15<04:08, 20.92ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3751/8956 [03:16<04:07, 21.05ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3754/8956 [03:16<04:07, 21.03ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3757/8956 [03:16<04:12, 20.57ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3760/8956 [03:16<03:58, 21.79ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3763/8956 [03:16<03:50, 22.49ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3766/8956 [03:16<03:55, 22.04ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3769/8956 [03:16<04:14, 20.35ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3772/8956 [03:17<04:12, 20.54ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3775/8956 [03:17<04:14, 20.35ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3778/8956 [03:17<04:32, 18.98ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3780/8956 [03:17<04:30, 19.11ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3783/8956 [03:17<04:27, 19.33ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3786/8956 [03:17<04:33, 18.94ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3789/8956 [03:18<04:23, 19.58ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3791/8956 [03:18<04:24, 19.52ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3794/8956 [03:18<04:14, 20.25ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3797/8956 [03:18<04:18, 19.95ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3800/8956 [03:18<04:11, 20.54ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3803/8956 [03:18<04:11, 20.48ba/s]Running tokenizer on every text in dataset:  42%|████▏     | 3806/8956 [03:18<04:18, 19.93ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3809/8956 [03:18<04:07, 20.79ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3812/8956 [03:19<04:10, 20.52ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3815/8956 [03:19<04:31, 18.95ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3818/8956 [03:19<04:17, 19.92ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3821/8956 [03:19<04:10, 20.50ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3824/8956 [03:19<04:17, 19.93ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3827/8956 [03:19<04:08, 20.67ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3830/8956 [03:20<04:10, 20.48ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3833/8956 [03:20<04:21, 19.60ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3835/8956 [03:20<04:20, 19.67ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3838/8956 [03:20<04:17, 19.86ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3841/8956 [03:20<04:11, 20.35ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3844/8956 [03:20<04:24, 19.36ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3847/8956 [03:20<04:19, 19.68ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3849/8956 [03:21<04:18, 19.75ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3852/8956 [03:21<04:29, 18.93ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3854/8956 [03:21<04:26, 19.16ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3856/8956 [03:21<04:23, 19.36ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3859/8956 [03:21<04:15, 19.96ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3862/8956 [03:21<04:25, 19.19ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3865/8956 [03:21<04:19, 19.65ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3867/8956 [03:21<04:20, 19.57ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3870/8956 [03:22<04:15, 19.93ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3872/8956 [03:22<04:24, 19.19ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3875/8956 [03:22<04:16, 19.84ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3877/8956 [03:22<04:17, 19.72ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3879/8956 [03:22<04:16, 19.77ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3881/8956 [03:22<04:33, 18.57ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3884/8956 [03:22<04:21, 19.37ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3887/8956 [03:22<04:17, 19.67ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3890/8956 [03:23<04:22, 19.32ba/s]Running tokenizer on every text in dataset:  43%|████▎     | 3893/8956 [03:23<04:14, 19.89ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3896/8956 [03:23<04:05, 20.60ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3899/8956 [03:23<04:04, 20.70ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3902/8956 [03:23<04:17, 19.61ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3905/8956 [03:23<04:12, 20.01ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3908/8956 [03:24<04:14, 19.85ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3910/8956 [03:24<04:30, 18.67ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3912/8956 [03:24<04:31, 18.58ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3914/8956 [03:24<04:26, 18.92ba/s]Running tokenizer on every text in dataset:  44%|████▎     | 3917/8956 [03:24<04:20, 19.32ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3919/8956 [03:24<04:35, 18.30ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3921/8956 [03:24<04:54, 17.10ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3924/8956 [03:24<04:34, 18.30ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3927/8956 [03:25<04:25, 18.92ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3929/8956 [03:25<05:04, 16.49ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3932/8956 [03:25<04:42, 17.78ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3935/8956 [03:25<04:26, 18.81ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3937/8956 [03:25<04:49, 17.36ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3939/8956 [03:25<04:48, 17.42ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3941/8956 [03:25<04:40, 17.88ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3943/8956 [03:25<04:33, 18.35ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3945/8956 [03:26<04:27, 18.70ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3947/8956 [03:26<04:44, 17.61ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3949/8956 [03:26<04:36, 18.13ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3952/8956 [03:26<04:24, 18.94ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3954/8956 [03:26<04:30, 18.47ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3956/8956 [03:26<04:25, 18.83ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3958/8956 [03:26<04:43, 17.61ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3960/8956 [03:26<04:37, 18.00ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3962/8956 [03:27<04:30, 18.49ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3965/8956 [03:27<04:13, 19.68ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3967/8956 [03:27<04:22, 19.01ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3970/8956 [03:27<04:14, 19.59ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3972/8956 [03:27<04:17, 19.39ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3974/8956 [03:27<04:19, 19.17ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3976/8956 [03:27<04:28, 18.54ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3979/8956 [03:27<04:16, 19.41ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3982/8956 [03:28<04:11, 19.76ba/s]Running tokenizer on every text in dataset:  44%|████▍     | 3985/8956 [03:28<04:23, 18.87ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3987/8956 [03:28<04:36, 17.96ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3989/8956 [03:28<04:31, 18.29ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3991/8956 [03:28<04:28, 18.52ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3993/8956 [03:28<04:26, 18.61ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3995/8956 [03:28<05:22, 15.40ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3997/8956 [03:28<05:06, 16.17ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 3999/8956 [03:29<04:53, 16.89ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4001/8956 [03:29<05:12, 15.87ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4003/8956 [03:29<04:55, 16.77ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4005/8956 [03:29<05:06, 16.16ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4007/8956 [03:29<04:53, 16.85ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4009/8956 [03:29<04:44, 17.36ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4011/8956 [03:29<04:36, 17.86ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4013/8956 [03:29<04:31, 18.17ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4015/8956 [03:29<04:47, 17.20ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4017/8956 [03:30<04:43, 17.41ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4019/8956 [03:30<04:38, 17.74ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4021/8956 [03:30<04:32, 18.12ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4023/8956 [03:30<04:49, 17.04ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4025/8956 [03:30<04:40, 17.60ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4027/8956 [03:30<04:33, 18.05ba/s]Running tokenizer on every text in dataset:  45%|████▍     | 4029/8956 [03:30<04:26, 18.47ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4031/8956 [03:30<04:29, 18.27ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4033/8956 [03:31<04:45, 17.22ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4035/8956 [03:31<04:42, 17.39ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4037/8956 [03:31<04:36, 17.80ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4039/8956 [03:31<04:29, 18.25ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4041/8956 [03:31<04:23, 18.65ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4043/8956 [03:31<04:40, 17.50ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4045/8956 [03:31<04:36, 17.79ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4047/8956 [03:31<04:31, 18.10ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4049/8956 [03:31<04:28, 18.25ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4051/8956 [03:31<04:28, 18.26ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4053/8956 [03:32<04:43, 17.30ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4055/8956 [03:32<04:35, 17.80ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4057/8956 [03:32<04:34, 17.85ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4059/8956 [03:32<04:31, 18.02ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4061/8956 [03:32<04:45, 17.13ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4063/8956 [03:32<04:37, 17.61ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4065/8956 [03:32<04:30, 18.06ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4067/8956 [03:32<04:27, 18.25ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4069/8956 [03:32<04:27, 18.27ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4071/8956 [03:33<04:43, 17.23ba/s]Running tokenizer on every text in dataset:  45%|████▌     | 4073/8956 [03:33<04:40, 17.38ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4075/8956 [03:33<04:36, 17.67ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4077/8956 [03:33<04:31, 17.95ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4079/8956 [03:33<04:32, 17.89ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4081/8956 [03:33<04:50, 16.81ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4083/8956 [03:33<04:42, 17.22ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4085/8956 [03:33<04:33, 17.83ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4087/8956 [03:34<04:33, 17.82ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4089/8956 [03:34<04:28, 18.10ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4091/8956 [03:34<04:45, 17.07ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4093/8956 [03:34<04:40, 17.33ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4095/8956 [03:34<04:38, 17.47ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4097/8956 [03:34<04:32, 17.82ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4099/8956 [03:34<04:45, 17.03ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4102/8956 [03:34<04:41, 17.27ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4105/8956 [03:35<04:25, 18.29ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4108/8956 [03:35<04:15, 18.99ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4110/8956 [03:35<05:11, 15.58ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4113/8956 [03:35<04:47, 16.86ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4115/8956 [03:35<04:36, 17.49ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4117/8956 [03:35<04:30, 17.89ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4119/8956 [03:35<04:49, 16.69ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4121/8956 [03:35<04:39, 17.32ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4123/8956 [03:36<04:29, 17.96ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4125/8956 [03:36<04:23, 18.36ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4127/8956 [03:36<04:16, 18.80ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4129/8956 [03:36<04:30, 17.87ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4132/8956 [03:36<04:16, 18.79ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4134/8956 [03:36<04:20, 18.50ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4136/8956 [03:36<04:19, 18.54ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4138/8956 [03:36<04:30, 17.78ba/s]Running tokenizer on every text in dataset:  46%|████▌     | 4141/8956 [03:37<04:17, 18.68ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4143/8956 [03:37<04:16, 18.77ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4145/8956 [03:37<04:13, 18.95ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4147/8956 [03:37<04:22, 18.35ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4149/8956 [03:37<04:16, 18.74ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4151/8956 [03:37<04:15, 18.78ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4153/8956 [03:37<04:11, 19.07ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4155/8956 [03:37<04:08, 19.32ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4157/8956 [03:37<04:27, 17.97ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4159/8956 [03:38<04:20, 18.43ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4162/8956 [03:38<04:10, 19.12ba/s]Running tokenizer on every text in dataset:  46%|████▋     | 4164/8956 [03:38<04:09, 19.20ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4166/8956 [03:38<04:25, 18.01ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4169/8956 [03:38<04:12, 18.94ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4172/8956 [03:38<04:01, 19.79ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4175/8956 [03:38<04:10, 19.11ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4178/8956 [03:39<04:01, 19.78ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4181/8956 [03:39<03:54, 20.39ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4184/8956 [03:39<03:51, 20.58ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4187/8956 [03:39<04:00, 19.83ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4190/8956 [03:39<03:50, 20.66ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4193/8956 [03:39<03:50, 20.62ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4196/8956 [03:39<04:07, 19.20ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4199/8956 [03:40<04:01, 19.67ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4201/8956 [03:40<04:06, 19.26ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4203/8956 [03:40<04:08, 19.16ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4205/8956 [03:40<04:22, 18.13ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4207/8956 [03:40<04:18, 18.40ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4209/8956 [03:40<04:16, 18.48ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4211/8956 [03:40<04:16, 18.48ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4213/8956 [03:40<04:28, 17.68ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4215/8956 [03:40<04:20, 18.22ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4217/8956 [03:41<04:19, 18.28ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4219/8956 [03:41<04:18, 18.30ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4222/8956 [03:41<04:11, 18.81ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4224/8956 [03:41<04:21, 18.10ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4226/8956 [03:41<04:17, 18.39ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4228/8956 [03:41<04:14, 18.58ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4230/8956 [03:41<04:09, 18.92ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4232/8956 [03:41<04:25, 17.78ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4234/8956 [03:41<04:17, 18.35ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4237/8956 [03:42<04:09, 18.91ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4239/8956 [03:42<04:06, 19.11ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4242/8956 [03:42<04:11, 18.72ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4245/8956 [03:42<03:55, 20.01ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4248/8956 [03:42<03:47, 20.73ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4251/8956 [03:42<03:56, 19.91ba/s]Running tokenizer on every text in dataset:  47%|████▋     | 4254/8956 [03:42<03:50, 20.44ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4257/8956 [03:43<03:45, 20.83ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4260/8956 [03:43<03:39, 21.37ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4263/8956 [03:43<03:53, 20.14ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4266/8956 [03:43<03:50, 20.33ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4269/8956 [03:43<03:47, 20.56ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4272/8956 [03:43<03:58, 19.61ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4275/8956 [03:43<03:54, 19.96ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4278/8956 [03:44<03:50, 20.30ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4281/8956 [03:44<03:57, 19.71ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4284/8956 [03:44<03:51, 20.21ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4287/8956 [03:44<03:45, 20.70ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4290/8956 [03:44<03:53, 19.95ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4293/8956 [03:44<03:50, 20.20ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4296/8956 [03:45<03:45, 20.70ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4299/8956 [03:45<04:02, 19.17ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4302/8956 [03:45<03:51, 20.12ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4305/8956 [03:45<03:45, 20.64ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4308/8956 [03:45<03:53, 19.90ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4311/8956 [03:45<03:46, 20.50ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4314/8956 [03:45<03:41, 20.97ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4317/8956 [03:46<03:39, 21.16ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4320/8956 [03:46<03:53, 19.86ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4323/8956 [03:46<03:55, 19.69ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4326/8956 [03:46<03:52, 19.88ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4329/8956 [03:46<04:01, 19.16ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4332/8956 [03:46<03:54, 19.68ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4335/8956 [03:46<03:51, 19.95ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4338/8956 [03:47<03:57, 19.46ba/s]Running tokenizer on every text in dataset:  48%|████▊     | 4341/8956 [03:47<03:51, 19.98ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4344/8956 [03:47<03:45, 20.47ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4347/8956 [03:47<03:52, 19.81ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4350/8956 [03:47<03:50, 19.95ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4353/8956 [03:47<03:52, 19.78ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4355/8956 [03:47<03:55, 19.54ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4357/8956 [03:48<04:11, 18.31ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4359/8956 [03:48<04:11, 18.31ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4361/8956 [03:48<04:13, 18.11ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4363/8956 [03:48<04:09, 18.39ba/s]Running tokenizer on every text in dataset:  49%|████▊     | 4365/8956 [03:48<04:19, 17.72ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4368/8956 [03:48<04:22, 17.46ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4371/8956 [03:48<04:04, 18.74ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4374/8956 [03:49<03:51, 19.81ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4376/8956 [03:49<03:58, 19.24ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4379/8956 [03:49<03:53, 19.62ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4382/8956 [03:49<03:48, 19.98ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4384/8956 [03:49<03:58, 19.17ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4386/8956 [03:49<03:56, 19.36ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4388/8956 [03:49<03:56, 19.35ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4391/8956 [03:49<03:46, 20.13ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4394/8956 [03:50<03:57, 19.20ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4396/8956 [03:50<04:21, 17.41ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4398/8956 [03:50<04:13, 17.99ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4401/8956 [03:50<04:02, 18.81ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4403/8956 [03:50<04:08, 18.30ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4405/8956 [03:50<04:05, 18.56ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4408/8956 [03:50<03:51, 19.64ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4411/8956 [03:50<03:50, 19.75ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4413/8956 [03:51<04:09, 18.22ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4415/8956 [03:51<04:15, 17.77ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4417/8956 [03:51<04:23, 17.25ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4419/8956 [03:51<04:26, 17.00ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4421/8956 [03:51<04:18, 17.51ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4423/8956 [03:51<04:27, 16.94ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4426/8956 [03:51<04:11, 18.04ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4428/8956 [03:51<04:06, 18.34ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4430/8956 [03:52<04:02, 18.64ba/s]Running tokenizer on every text in dataset:  49%|████▉     | 4432/8956 [03:52<04:12, 17.90ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4435/8956 [03:52<03:57, 19.00ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4438/8956 [03:52<03:45, 20.01ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4441/8956 [03:52<03:52, 19.38ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4443/8956 [03:52<03:53, 19.33ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4446/8956 [03:52<03:44, 20.10ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4449/8956 [03:52<03:38, 20.65ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4452/8956 [03:53<03:46, 19.85ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4455/8956 [03:53<03:37, 20.65ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4458/8956 [03:53<03:33, 21.02ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4461/8956 [03:53<03:42, 20.24ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4464/8956 [03:53<03:41, 20.28ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4467/8956 [03:53<03:35, 20.82ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4470/8956 [03:54<03:47, 19.76ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4473/8956 [03:54<03:41, 20.23ba/s]Running tokenizer on every text in dataset:  50%|████▉     | 4476/8956 [03:54<03:39, 20.43ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4479/8956 [03:54<03:50, 19.40ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4481/8956 [03:54<03:51, 19.33ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4483/8956 [03:54<03:52, 19.26ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4486/8956 [03:54<03:47, 19.67ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4488/8956 [03:54<03:49, 19.49ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4490/8956 [03:55<04:00, 18.58ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4493/8956 [03:55<03:50, 19.36ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4496/8956 [03:55<03:50, 19.33ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4498/8956 [03:55<03:58, 18.67ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4501/8956 [03:55<03:49, 19.45ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4504/8956 [03:55<04:24, 16.81ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4506/8956 [03:55<04:19, 17.13ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4508/8956 [03:56<04:25, 16.72ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4510/8956 [03:56<04:17, 17.27ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4512/8956 [03:56<04:49, 15.38ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4514/8956 [03:56<04:33, 16.21ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4516/8956 [03:56<04:22, 16.90ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4518/8956 [03:56<04:30, 16.42ba/s]Running tokenizer on every text in dataset:  50%|█████     | 4521/8956 [03:56<04:10, 17.68ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4523/8956 [03:56<04:03, 18.18ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4526/8956 [03:57<03:54, 18.88ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4528/8956 [03:57<04:08, 17.83ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4531/8956 [03:57<03:59, 18.50ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4533/8956 [03:57<03:56, 18.72ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4536/8956 [03:57<04:03, 18.12ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4539/8956 [03:57<03:56, 18.70ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4542/8956 [03:57<03:45, 19.53ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4545/8956 [03:58<03:41, 19.88ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4547/8956 [03:58<03:51, 19.07ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4550/8956 [03:58<03:47, 19.35ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4552/8956 [03:58<03:48, 19.24ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4555/8956 [03:58<03:56, 18.58ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4557/8956 [03:58<03:53, 18.87ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4560/8956 [03:58<03:44, 19.58ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4563/8956 [03:59<03:41, 19.86ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4565/8956 [03:59<03:53, 18.79ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4567/8956 [03:59<03:52, 18.91ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4569/8956 [03:59<03:50, 19.05ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4571/8956 [03:59<03:51, 18.98ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4573/8956 [03:59<03:52, 18.81ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4575/8956 [03:59<04:10, 17.47ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4577/8956 [03:59<04:01, 18.11ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4580/8956 [03:59<03:48, 19.17ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4582/8956 [04:00<03:48, 19.16ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4584/8956 [04:00<03:58, 18.32ba/s]Running tokenizer on every text in dataset:  51%|█████     | 4587/8956 [04:00<03:47, 19.19ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4590/8956 [04:00<03:38, 19.98ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4593/8956 [04:00<03:37, 20.06ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4596/8956 [04:00<03:34, 20.31ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4599/8956 [04:00<03:38, 19.93ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4601/8956 [04:01<03:45, 19.31ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4603/8956 [04:01<03:57, 18.32ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4606/8956 [04:01<03:49, 18.98ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4609/8956 [04:01<03:43, 19.44ba/s]Running tokenizer on every text in dataset:  51%|█████▏    | 4611/8956 [04:01<03:47, 19.10ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4613/8956 [04:01<04:09, 17.40ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4615/8956 [04:01<04:11, 17.28ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4617/8956 [04:01<04:11, 17.28ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4619/8956 [04:02<04:09, 17.39ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4621/8956 [04:02<04:02, 17.86ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4623/8956 [04:02<04:13, 17.09ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4626/8956 [04:02<04:00, 17.99ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4628/8956 [04:02<03:56, 18.31ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4630/8956 [04:02<03:50, 18.75ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4632/8956 [04:02<04:40, 15.39ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4634/8956 [04:02<04:28, 16.13ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4636/8956 [04:03<04:19, 16.62ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4638/8956 [04:03<04:35, 15.67ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4640/8956 [04:03<04:19, 16.65ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4642/8956 [04:03<04:21, 16.52ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4644/8956 [04:03<04:08, 17.37ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4646/8956 [04:03<04:04, 17.65ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4648/8956 [04:03<04:04, 17.65ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4650/8956 [04:03<04:16, 16.79ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4652/8956 [04:04<04:08, 17.32ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4654/8956 [04:04<04:06, 17.44ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4656/8956 [04:04<04:01, 17.80ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4658/8956 [04:04<03:55, 18.22ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4660/8956 [04:04<04:10, 17.18ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4663/8956 [04:04<03:51, 18.57ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4666/8956 [04:04<03:41, 19.37ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4669/8956 [04:04<03:45, 19.00ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4672/8956 [04:05<03:40, 19.39ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4675/8956 [04:05<03:33, 20.08ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4678/8956 [04:05<03:25, 20.80ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4681/8956 [04:05<03:38, 19.58ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4684/8956 [04:05<03:31, 20.20ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4687/8956 [04:05<03:30, 20.32ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4690/8956 [04:05<03:38, 19.51ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4693/8956 [04:06<03:32, 20.03ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4696/8956 [04:06<03:30, 20.26ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4699/8956 [04:06<03:37, 19.58ba/s]Running tokenizer on every text in dataset:  52%|█████▏    | 4701/8956 [04:06<03:36, 19.65ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4704/8956 [04:06<03:31, 20.07ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4707/8956 [04:06<03:37, 19.50ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4710/8956 [04:06<03:30, 20.20ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4713/8956 [04:07<03:21, 21.03ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4716/8956 [04:07<03:18, 21.38ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4719/8956 [04:07<03:25, 20.64ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4722/8956 [04:07<03:20, 21.07ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4725/8956 [04:07<03:18, 21.28ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4728/8956 [04:07<03:27, 20.41ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4731/8956 [04:07<03:19, 21.16ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4734/8956 [04:08<03:18, 21.32ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4737/8956 [04:08<03:27, 20.36ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4740/8956 [04:08<03:21, 20.93ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4743/8956 [04:08<03:20, 21.04ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4746/8956 [04:08<03:31, 19.95ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4749/8956 [04:08<03:29, 20.09ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4752/8956 [04:08<03:21, 20.85ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4755/8956 [04:09<03:26, 20.34ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4758/8956 [04:09<03:23, 20.64ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4761/8956 [04:09<03:18, 21.10ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4764/8956 [04:09<03:23, 20.60ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4767/8956 [04:09<03:19, 21.04ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4770/8956 [04:09<03:16, 21.32ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4773/8956 [04:09<03:13, 21.60ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4776/8956 [04:10<03:22, 20.65ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4779/8956 [04:10<03:19, 20.96ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4782/8956 [04:10<03:20, 20.79ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4785/8956 [04:10<03:30, 19.78ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4788/8956 [04:10<03:22, 20.60ba/s]Running tokenizer on every text in dataset:  53%|█████▎    | 4791/8956 [04:10<03:18, 20.99ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4794/8956 [04:10<03:27, 20.09ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4797/8956 [04:11<03:22, 20.57ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4800/8956 [04:11<03:19, 20.83ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4803/8956 [04:11<03:29, 19.79ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4806/8956 [04:11<03:22, 20.50ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4809/8956 [04:11<03:16, 21.06ba/s]Running tokenizer on every text in dataset:  54%|█████▎    | 4812/8956 [04:11<03:20, 20.63ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4815/8956 [04:11<03:16, 21.03ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4818/8956 [04:12<03:18, 20.86ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4821/8956 [04:12<03:23, 20.30ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4824/8956 [04:12<03:16, 20.99ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4827/8956 [04:12<03:17, 20.94ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4830/8956 [04:12<03:10, 21.66ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4833/8956 [04:12<03:10, 21.70ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4836/8956 [04:12<03:01, 22.75ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4839/8956 [04:13<02:59, 22.93ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4842/8956 [04:13<03:06, 22.05ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4845/8956 [04:13<02:59, 22.92ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4848/8956 [04:13<02:51, 23.99ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4851/8956 [04:13<02:54, 23.47ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4854/8956 [04:13<02:48, 24.34ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4858/8956 [04:13<02:31, 27.09ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4861/8956 [04:13<02:52, 23.77ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4864/8956 [04:14<03:05, 22.07ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4867/8956 [04:14<03:18, 20.59ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4870/8956 [04:14<03:36, 18.88ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4873/8956 [04:14<03:34, 19.07ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4875/8956 [04:14<03:32, 19.22ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4878/8956 [04:14<03:40, 18.47ba/s]Running tokenizer on every text in dataset:  54%|█████▍    | 4880/8956 [04:15<03:42, 18.34ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4883/8956 [04:15<03:28, 19.50ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4886/8956 [04:15<03:09, 21.50ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4889/8956 [04:15<03:05, 21.89ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4892/8956 [04:15<02:51, 23.65ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4895/8956 [04:15<02:47, 24.31ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4898/8956 [04:15<02:54, 23.26ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4901/8956 [04:15<02:47, 24.26ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4904/8956 [04:16<02:41, 25.03ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4907/8956 [04:16<02:56, 23.00ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4910/8956 [04:16<03:10, 21.27ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4913/8956 [04:16<03:15, 20.64ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4916/8956 [04:16<03:36, 18.64ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4919/8956 [04:16<03:30, 19.21ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4921/8956 [04:16<03:31, 19.08ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4923/8956 [04:17<03:33, 18.89ba/s]Running tokenizer on every text in dataset:  55%|█████▍    | 4925/8956 [04:17<04:10, 16.12ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4927/8956 [04:17<04:18, 15.61ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4929/8956 [04:17<04:18, 15.61ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4931/8956 [04:17<04:17, 15.63ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4933/8956 [04:17<04:28, 14.96ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4935/8956 [04:17<04:43, 14.16ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4937/8956 [04:18<04:36, 14.53ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4940/8956 [04:18<04:01, 16.65ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4943/8956 [04:18<03:42, 18.02ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4945/8956 [04:18<03:45, 17.79ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4948/8956 [04:18<03:28, 19.19ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4951/8956 [04:18<03:19, 20.06ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4954/8956 [04:18<03:30, 18.97ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4957/8956 [04:19<03:22, 19.71ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4960/8956 [04:19<03:19, 20.08ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4963/8956 [04:19<03:14, 20.58ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4966/8956 [04:19<03:22, 19.67ba/s]Running tokenizer on every text in dataset:  55%|█████▌    | 4969/8956 [04:19<03:15, 20.41ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4972/8956 [04:19<03:15, 20.39ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4975/8956 [04:19<03:30, 18.94ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4978/8956 [04:20<03:22, 19.64ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4981/8956 [04:20<03:20, 19.78ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4983/8956 [04:20<03:34, 18.53ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4985/8956 [04:20<03:34, 18.49ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4987/8956 [04:20<03:37, 18.26ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4989/8956 [04:20<03:43, 17.78ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4991/8956 [04:20<03:45, 17.56ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4993/8956 [04:20<03:56, 16.79ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4995/8956 [04:21<03:47, 17.42ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4997/8956 [04:21<03:43, 17.75ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 4999/8956 [04:21<03:36, 18.25ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5001/8956 [04:21<03:31, 18.72ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5003/8956 [04:21<03:44, 17.62ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5005/8956 [04:21<03:39, 18.01ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5008/8956 [04:21<03:19, 19.77ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5010/8956 [04:21<03:20, 19.67ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5012/8956 [04:21<03:36, 18.20ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5014/8956 [04:22<03:34, 18.36ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5016/8956 [04:22<03:35, 18.25ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5018/8956 [04:22<03:35, 18.31ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5020/8956 [04:22<03:35, 18.29ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5022/8956 [04:22<03:46, 17.34ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5024/8956 [04:22<03:45, 17.46ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5026/8956 [04:22<03:42, 17.67ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5029/8956 [04:22<03:28, 18.85ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5031/8956 [04:23<03:41, 17.76ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5034/8956 [04:23<03:28, 18.86ba/s]Running tokenizer on every text in dataset:  56%|█████▌    | 5037/8956 [04:23<03:18, 19.71ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5040/8956 [04:23<03:26, 18.92ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5043/8956 [04:23<03:20, 19.56ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5046/8956 [04:23<03:16, 19.93ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5049/8956 [04:23<03:23, 19.18ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5052/8956 [04:24<03:16, 19.86ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5055/8956 [04:24<03:12, 20.27ba/s]Running tokenizer on every text in dataset:  56%|█████▋    | 5058/8956 [04:24<03:08, 20.63ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5061/8956 [04:24<03:13, 20.10ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5064/8956 [04:24<03:16, 19.76ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5066/8956 [04:24<03:22, 19.18ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5068/8956 [04:24<03:26, 18.86ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5071/8956 [04:25<03:15, 19.90ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5074/8956 [04:25<03:06, 20.82ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5077/8956 [04:25<03:00, 21.50ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5080/8956 [04:25<03:11, 20.24ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5083/8956 [04:25<03:10, 20.34ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5086/8956 [04:25<03:07, 20.69ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5089/8956 [04:25<03:17, 19.56ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5092/8956 [04:26<03:11, 20.13ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5095/8956 [04:26<03:09, 20.39ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5098/8956 [04:26<03:19, 19.31ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5101/8956 [04:26<03:14, 19.80ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5103/8956 [04:26<03:16, 19.62ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5105/8956 [04:26<03:20, 19.23ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5107/8956 [04:26<03:36, 17.77ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5109/8956 [04:26<03:33, 18.00ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5111/8956 [04:27<03:32, 18.13ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5113/8956 [04:27<03:32, 18.06ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5115/8956 [04:27<03:31, 18.14ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5117/8956 [04:27<03:44, 17.08ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5119/8956 [04:27<03:41, 17.32ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5121/8956 [04:27<03:38, 17.55ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5123/8956 [04:27<03:34, 17.89ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5125/8956 [04:27<03:43, 17.12ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5127/8956 [04:28<03:39, 17.41ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5129/8956 [04:28<03:35, 17.73ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5131/8956 [04:28<03:34, 17.86ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5133/8956 [04:28<03:32, 17.99ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5135/8956 [04:28<03:46, 16.85ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5137/8956 [04:28<03:42, 17.13ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5139/8956 [04:28<03:37, 17.51ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5141/8956 [04:28<03:39, 17.36ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5143/8956 [04:28<03:35, 17.69ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5145/8956 [04:29<03:46, 16.80ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5147/8956 [04:29<03:42, 17.15ba/s]Running tokenizer on every text in dataset:  57%|█████▋    | 5149/8956 [04:29<03:39, 17.38ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5151/8956 [04:29<03:34, 17.70ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5154/8956 [04:29<03:33, 17.78ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5156/8956 [04:29<03:27, 18.32ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5159/8956 [04:29<03:17, 19.20ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5162/8956 [04:29<03:11, 19.84ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5164/8956 [04:30<03:17, 19.18ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5166/8956 [04:30<03:18, 19.11ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5168/8956 [04:30<03:16, 19.28ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5171/8956 [04:30<03:07, 20.20ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5174/8956 [04:30<03:31, 17.84ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5176/8956 [04:30<03:28, 18.09ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5178/8956 [04:30<03:28, 18.08ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5180/8956 [04:30<03:54, 16.08ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5182/8956 [04:31<03:54, 16.12ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5184/8956 [04:31<03:42, 16.93ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5187/8956 [04:31<03:25, 18.35ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5189/8956 [04:31<03:31, 17.82ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5191/8956 [04:31<03:25, 18.28ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5193/8956 [04:31<03:31, 17.79ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5196/8956 [04:31<03:26, 18.24ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5199/8956 [04:32<03:17, 18.99ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5201/8956 [04:32<03:23, 18.41ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5203/8956 [04:32<03:19, 18.80ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5205/8956 [04:32<03:22, 18.52ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5208/8956 [04:32<03:13, 19.41ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5211/8956 [04:32<03:19, 18.81ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5213/8956 [04:32<03:17, 18.92ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5216/8956 [04:32<03:09, 19.74ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5219/8956 [04:33<03:02, 20.43ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5222/8956 [04:33<03:12, 19.36ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5225/8956 [04:33<03:09, 19.64ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5227/8956 [04:33<03:09, 19.65ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5230/8956 [04:33<03:16, 18.92ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5233/8956 [04:33<03:11, 19.42ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5235/8956 [04:33<03:10, 19.54ba/s]Running tokenizer on every text in dataset:  58%|█████▊    | 5238/8956 [04:34<03:05, 19.99ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5241/8956 [04:34<03:13, 19.21ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5244/8956 [04:34<03:09, 19.56ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5246/8956 [04:34<03:09, 19.57ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5249/8956 [04:34<03:13, 19.14ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5252/8956 [04:34<03:07, 19.75ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5255/8956 [04:34<03:05, 19.94ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5257/8956 [04:34<03:05, 19.90ba/s]Running tokenizer on every text in dataset:  59%|█████▊    | 5259/8956 [04:35<03:15, 18.87ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5262/8956 [04:35<03:08, 19.56ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5264/8956 [04:35<03:08, 19.60ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5267/8956 [04:35<03:03, 20.05ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5269/8956 [04:35<03:16, 18.74ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5272/8956 [04:35<03:11, 19.20ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5275/8956 [04:35<03:06, 19.70ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5277/8956 [04:36<03:15, 18.85ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5280/8956 [04:36<03:10, 19.26ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5283/8956 [04:36<03:05, 19.76ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5286/8956 [04:36<02:59, 20.44ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5289/8956 [04:36<03:03, 19.94ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5291/8956 [04:36<03:04, 19.91ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5294/8956 [04:36<02:59, 20.43ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5297/8956 [04:37<03:04, 19.79ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5300/8956 [04:37<02:55, 20.86ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5303/8956 [04:37<02:54, 20.92ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5306/8956 [04:37<02:55, 20.80ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5309/8956 [04:37<02:50, 21.40ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5312/8956 [04:37<02:47, 21.80ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5315/8956 [04:37<02:51, 21.22ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5318/8956 [04:37<02:45, 21.97ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5321/8956 [04:38<02:43, 22.20ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5324/8956 [04:38<02:45, 21.96ba/s]Running tokenizer on every text in dataset:  59%|█████▉    | 5327/8956 [04:38<02:49, 21.43ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5330/8956 [04:38<02:47, 21.69ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5333/8956 [04:38<02:47, 21.60ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5336/8956 [04:38<02:57, 20.38ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5339/8956 [04:38<02:53, 20.83ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5342/8956 [04:39<02:51, 21.02ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5345/8956 [04:39<02:58, 20.21ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5348/8956 [04:39<02:49, 21.30ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5351/8956 [04:39<02:43, 22.04ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5354/8956 [04:39<02:47, 21.56ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5357/8956 [04:39<02:45, 21.79ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5360/8956 [04:39<02:47, 21.44ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5363/8956 [04:40<02:58, 20.11ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5366/8956 [04:40<02:58, 20.11ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5369/8956 [04:40<02:56, 20.29ba/s]Running tokenizer on every text in dataset:  60%|█████▉    | 5372/8956 [04:40<03:10, 18.84ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5374/8956 [04:40<03:12, 18.61ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5376/8956 [04:40<03:11, 18.70ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5378/8956 [04:40<03:08, 18.93ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5381/8956 [04:41<03:01, 19.67ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5383/8956 [04:41<03:13, 18.49ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5385/8956 [04:41<03:11, 18.64ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5387/8956 [04:41<03:10, 18.73ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5390/8956 [04:41<02:59, 19.83ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5392/8956 [04:41<03:07, 19.06ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5394/8956 [04:41<03:05, 19.21ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5397/8956 [04:41<02:52, 20.59ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5400/8956 [04:42<02:49, 20.99ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5403/8956 [04:42<03:01, 19.59ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5406/8956 [04:42<02:58, 19.94ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5409/8956 [04:42<02:54, 20.28ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5412/8956 [04:42<03:02, 19.43ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5415/8956 [04:42<02:57, 19.96ba/s]Running tokenizer on every text in dataset:  60%|██████    | 5418/8956 [04:42<02:50, 20.71ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5421/8956 [04:43<02:58, 19.86ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5424/8956 [04:43<02:54, 20.22ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5427/8956 [04:43<02:52, 20.44ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5430/8956 [04:43<03:01, 19.45ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5433/8956 [04:43<02:57, 19.82ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5435/8956 [04:43<02:59, 19.64ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5437/8956 [04:43<03:00, 19.47ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5439/8956 [04:44<03:09, 18.54ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5441/8956 [04:44<03:05, 18.90ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5443/8956 [04:44<03:04, 19.06ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5445/8956 [04:44<03:30, 16.67ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5447/8956 [04:44<03:23, 17.21ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5449/8956 [04:44<03:34, 16.32ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5451/8956 [04:44<03:48, 15.32ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5453/8956 [04:44<03:38, 16.06ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5455/8956 [04:44<03:30, 16.60ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5457/8956 [04:45<03:25, 17.07ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5459/8956 [04:45<03:34, 16.27ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5461/8956 [04:45<03:26, 16.94ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5463/8956 [04:45<03:22, 17.21ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5465/8956 [04:45<03:20, 17.40ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5467/8956 [04:45<03:31, 16.51ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5469/8956 [04:45<03:26, 16.86ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5471/8956 [04:45<03:23, 17.11ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5473/8956 [04:46<03:19, 17.46ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5475/8956 [04:46<03:15, 17.77ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5477/8956 [04:46<03:31, 16.44ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5479/8956 [04:46<03:29, 16.61ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5481/8956 [04:46<03:19, 17.42ba/s]Running tokenizer on every text in dataset:  61%|██████    | 5484/8956 [04:46<03:02, 19.07ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5486/8956 [04:46<03:12, 18.05ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5489/8956 [04:46<02:57, 19.52ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5492/8956 [04:47<02:51, 20.23ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5495/8956 [04:47<02:48, 20.52ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5498/8956 [04:47<02:54, 19.86ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5501/8956 [04:47<02:47, 20.60ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5504/8956 [04:47<02:41, 21.43ba/s]Running tokenizer on every text in dataset:  61%|██████▏   | 5507/8956 [04:47<02:47, 20.64ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5510/8956 [04:47<02:41, 21.33ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5513/8956 [04:48<02:31, 22.74ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5516/8956 [04:48<02:41, 21.29ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5519/8956 [04:48<02:38, 21.63ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5522/8956 [04:48<02:38, 21.61ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5525/8956 [04:48<02:47, 20.50ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5528/8956 [04:48<02:51, 20.01ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5531/8956 [04:48<02:57, 19.31ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5533/8956 [04:49<03:00, 18.92ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5535/8956 [04:49<03:14, 17.60ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5537/8956 [04:49<03:14, 17.59ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5539/8956 [04:49<03:16, 17.43ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5541/8956 [04:49<03:13, 17.65ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5543/8956 [04:49<03:21, 16.92ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5545/8956 [04:49<03:15, 17.41ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5547/8956 [04:49<03:16, 17.34ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5549/8956 [04:49<03:10, 17.85ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5551/8956 [04:50<03:07, 18.16ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5553/8956 [04:50<03:32, 16.02ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5555/8956 [04:50<03:23, 16.72ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5557/8956 [04:50<03:16, 17.32ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5559/8956 [04:50<03:11, 17.73ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5561/8956 [04:50<03:09, 17.92ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5563/8956 [04:50<03:16, 17.28ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5566/8956 [04:50<03:04, 18.38ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5568/8956 [04:51<03:00, 18.78ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5570/8956 [04:51<02:57, 19.08ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5572/8956 [04:51<03:05, 18.24ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5574/8956 [04:51<03:00, 18.71ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5576/8956 [04:51<03:00, 18.70ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5579/8956 [04:51<02:55, 19.23ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5581/8956 [04:51<03:03, 18.37ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5584/8956 [04:51<02:53, 19.47ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5586/8956 [04:51<02:56, 19.04ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5588/8956 [04:52<02:55, 19.24ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5590/8956 [04:52<02:56, 19.08ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5592/8956 [04:52<03:09, 17.72ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5594/8956 [04:52<03:04, 18.19ba/s]Running tokenizer on every text in dataset:  62%|██████▏   | 5597/8956 [04:52<02:52, 19.49ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5600/8956 [04:52<02:56, 19.04ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5603/8956 [04:52<02:45, 20.27ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5606/8956 [04:53<02:50, 19.65ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5608/8956 [04:53<02:50, 19.60ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5610/8956 [04:53<03:00, 18.53ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5612/8956 [04:53<02:57, 18.81ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5614/8956 [04:53<02:56, 18.89ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5617/8956 [04:53<02:52, 19.39ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5619/8956 [04:53<03:00, 18.48ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5622/8956 [04:53<02:54, 19.08ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5625/8956 [04:54<02:49, 19.68ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5628/8956 [04:54<02:45, 20.14ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5631/8956 [04:54<02:47, 19.82ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5634/8956 [04:54<02:45, 20.04ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5637/8956 [04:54<02:43, 20.36ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5640/8956 [04:54<02:53, 19.15ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5643/8956 [04:54<02:50, 19.47ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5645/8956 [04:55<02:49, 19.54ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5648/8956 [04:55<02:56, 18.79ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5651/8956 [04:55<02:52, 19.21ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5654/8956 [04:55<02:49, 19.47ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5657/8956 [04:55<02:55, 18.85ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5659/8956 [04:55<02:54, 18.85ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5661/8956 [04:55<02:53, 18.94ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5663/8956 [04:55<02:54, 18.89ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5665/8956 [04:56<02:53, 18.92ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5667/8956 [04:56<03:05, 17.69ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5669/8956 [04:56<03:01, 18.12ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5671/8956 [04:56<02:58, 18.45ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5673/8956 [04:56<02:56, 18.63ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5675/8956 [04:56<02:55, 18.67ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5677/8956 [04:56<03:09, 17.32ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5679/8956 [04:56<03:02, 17.96ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5681/8956 [04:56<02:58, 18.38ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5683/8956 [04:57<02:56, 18.57ba/s]Running tokenizer on every text in dataset:  63%|██████▎   | 5686/8956 [04:57<03:00, 18.11ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5689/8956 [04:57<02:52, 18.92ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5691/8956 [04:57<02:53, 18.87ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5694/8956 [04:57<02:48, 19.38ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5696/8956 [04:57<02:56, 18.46ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5699/8956 [04:57<03:09, 17.17ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5702/8956 [04:58<02:57, 18.37ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5705/8956 [04:58<02:58, 18.25ba/s]Running tokenizer on every text in dataset:  64%|██████▎   | 5708/8956 [04:58<03:04, 17.61ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5711/8956 [04:58<02:54, 18.64ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5714/8956 [04:58<02:55, 18.45ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5717/8956 [04:58<02:51, 18.83ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5720/8956 [04:59<02:47, 19.30ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5722/8956 [04:59<02:47, 19.35ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5724/8956 [04:59<02:55, 18.43ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5726/8956 [04:59<02:56, 18.32ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5728/8956 [04:59<02:56, 18.32ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5730/8956 [04:59<02:53, 18.59ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5732/8956 [04:59<02:51, 18.76ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5734/8956 [04:59<03:04, 17.44ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5736/8956 [04:59<03:01, 17.75ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5738/8956 [05:00<02:57, 18.13ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5740/8956 [05:00<02:55, 18.35ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5742/8956 [05:00<02:59, 17.90ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5744/8956 [05:00<03:08, 17.04ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5746/8956 [05:00<03:03, 17.47ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5748/8956 [05:00<02:59, 17.88ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5750/8956 [05:00<02:54, 18.39ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5752/8956 [05:00<03:06, 17.18ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5754/8956 [05:01<03:05, 17.28ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5756/8956 [05:01<03:04, 17.35ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5758/8956 [05:01<03:01, 17.57ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5760/8956 [05:01<02:58, 17.94ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5762/8956 [05:01<03:12, 16.61ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5764/8956 [05:01<03:08, 16.96ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5766/8956 [05:01<03:03, 17.37ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5768/8956 [05:01<02:56, 18.03ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5771/8956 [05:01<02:59, 17.79ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5773/8956 [05:02<02:56, 18.00ba/s]Running tokenizer on every text in dataset:  64%|██████▍   | 5775/8956 [05:02<02:54, 18.26ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5777/8956 [05:02<02:55, 18.09ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5779/8956 [05:02<02:55, 18.13ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5781/8956 [05:02<03:03, 17.31ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5783/8956 [05:02<02:59, 17.66ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5785/8956 [05:02<02:57, 17.85ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5787/8956 [05:02<02:52, 18.34ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5789/8956 [05:02<02:50, 18.55ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5791/8956 [05:03<02:59, 17.59ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5793/8956 [05:03<02:54, 18.17ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5796/8956 [05:03<02:42, 19.49ba/s]Running tokenizer on every text in dataset:  65%|██████▍   | 5798/8956 [05:03<02:48, 18.78ba/s]slurmstepd: error: *** JOB 62097511 ON dgx5 CANCELLED AT 2021-08-09T09:48:27 ***
Job has already finished for job 62097511
